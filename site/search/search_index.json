{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to Polly! Polly is an AI-enabled platform for storage, reproducible analysis and integration of high-throughput biomedical data on the cloud. Polly creates a unique, centralized ecosystem that enables a diverse team of biologists, bioinformaticians and scientific leaders to share and collaborate on workspaces, data and insights. Polly combines a proprietary platform with optional, on-demand high-value bioinformatics services for biological target identification and validation, pathway elucidation, cellular phenotype analysis, and biomarker identification. This starter guide provides a broad overview of the platform to get you started on an exciting journey of research and discovery with your data. Accessing the platform Sign up for the platform To sign in to the Polly platform, you must have an account. Account credentials can be generated for both temporary, limited access as well as enterprise accounts. Reach out to our team at polly@elucidata.io for your login credentials. Note: We recommend using Google Chrome for an optimized experience. Login to the platform You can sign in Polly with the provided credentials: Username and Password, connected to your account, and then click Sign in . When logging in for the first time, you will be prompted to reset your password. If you do not sign out when you leave Polly, you will be automatically authenticated for your next visit. Login through Polly CLI To be able to start using the features of Polly CLI, the first step is to log in to Polly using the Terminal / Command Prompt. Use the following command to log in. polly login Polly Username and Password need to be put in when prompted. Once logged in, you do not get automatically logged out and will have to log out manually with the command to log out. Navigating through the platform The Polly Landing page provides access to 3 distinct interconnected modules on Polly. Manage Data Find comprehensive information about your workspaces, including files, notebooks, apps, analyses, and reports. This page also includes information about the creation date, workspace owner, and collaborators. Use Apps Polly hosts a suite of apps and production-ready workflows to analyze and process a variety of data with ease. You can build and host your custom workflows from individual components or choose from a suite of ready-to-use workflows on Polly. Write & Maintain Code Polly enables coding in a range of computational environments with configurable machine sizes and dockerized environments. Discover Insights Polly enables access to proprietary, expert-curated, disease-specific data lakes to accelerate your drug discovery programs. Mine data lakes for public and proprietary data that are harmonized across sources on Polly Discover. Getting help on the platform We firmly believe that our greatest achievement is to enable customers and their success on the platform. The customer success team can be reached anytime on polly@elucidata.io for queries on the product, data analysis and feature requests. We also offer a detailed on-boarding plan to get your team up and running on Polly as well as library of resources, videos and publications. Live Chat Our platform uses Intercom as a standard messenger launcher. A text bubble pops up, inquiring if you need help. Polly embeds this support to establish a dedicated customer success manager. Technical support is available from 9 AM to 8 PM EST and CET. Email You can always email the team at polly@elucidata.io for further support, to send feedback, or to suggest features.","title":"Home"},{"location":"index.html#welcome-to-polly","text":"Polly is an AI-enabled platform for storage, reproducible analysis and integration of high-throughput biomedical data on the cloud. Polly creates a unique, centralized ecosystem that enables a diverse team of biologists, bioinformaticians and scientific leaders to share and collaborate on workspaces, data and insights. Polly combines a proprietary platform with optional, on-demand high-value bioinformatics services for biological target identification and validation, pathway elucidation, cellular phenotype analysis, and biomarker identification. This starter guide provides a broad overview of the platform to get you started on an exciting journey of research and discovery with your data.","title":"Welcome to Polly!"},{"location":"index.html#accessing-the-platform","text":"Sign up for the platform To sign in to the Polly platform, you must have an account. Account credentials can be generated for both temporary, limited access as well as enterprise accounts. Reach out to our team at polly@elucidata.io for your login credentials. Note: We recommend using Google Chrome for an optimized experience. Login to the platform You can sign in Polly with the provided credentials: Username and Password, connected to your account, and then click Sign in . When logging in for the first time, you will be prompted to reset your password. If you do not sign out when you leave Polly, you will be automatically authenticated for your next visit. Login through Polly CLI To be able to start using the features of Polly CLI, the first step is to log in to Polly using the Terminal / Command Prompt. Use the following command to log in. polly login Polly Username and Password need to be put in when prompted. Once logged in, you do not get automatically logged out and will have to log out manually with the command to log out.","title":"Accessing the platform"},{"location":"index.html#navigating-through-the-platform","text":"The Polly Landing page provides access to 3 distinct interconnected modules on Polly. Manage Data Find comprehensive information about your workspaces, including files, notebooks, apps, analyses, and reports. This page also includes information about the creation date, workspace owner, and collaborators. Use Apps Polly hosts a suite of apps and production-ready workflows to analyze and process a variety of data with ease. You can build and host your custom workflows from individual components or choose from a suite of ready-to-use workflows on Polly. Write & Maintain Code Polly enables coding in a range of computational environments with configurable machine sizes and dockerized environments. Discover Insights Polly enables access to proprietary, expert-curated, disease-specific data lakes to accelerate your drug discovery programs. Mine data lakes for public and proprietary data that are harmonized across sources on Polly Discover.","title":"Navigating through the platform"},{"location":"index.html#getting-help-on-the-platform","text":"We firmly believe that our greatest achievement is to enable customers and their success on the platform. The customer success team can be reached anytime on polly@elucidata.io for queries on the product, data analysis and feature requests. We also offer a detailed on-boarding plan to get your team up and running on Polly as well as library of resources, videos and publications.","title":"Getting help on the platform"},{"location":"index.html#live-chat","text":"Our platform uses Intercom as a standard messenger launcher. A text bubble pops up, inquiring if you need help. Polly embeds this support to establish a dedicated customer success manager. Technical support is available from 9 AM to 8 PM EST and CET.","title":"Live Chat"},{"location":"index.html#email","text":"You can always email the team at polly@elucidata.io for further support, to send feedback, or to suggest features.","title":"Email"},{"location":"Data Lake.html","text":"Introduction With Polly you can access public databases which have been curated and stored in the form of data lakes or make data lakes with your own data. These data lakes can be explored and analyzed either through Polly Notebooks or the several Data Lake Applications available. Polly Discover consists of the following major components: Data Lake Curation Data lakes are reservoirs of information that contain multi-omics data, annotations from publicly available databases, publications, etc. Reservoirs are further segregated into two parts: public data repositories which are curated by Elucidata using public data sources, and private data repositories, where you can add your proprietary data. Data Lake Exploration To explore a data lake, Polly provides tools such as applications and Polly Notebooks. These tools enable you to find relevant data by searching for keywords associated with the file name, file metadata, or the contents of a file. On-the-fly Analysis Once you have narrowed down relevant omics datasets, you can analyze the dataset(s) on the fly using various statistical analyses while displaying intuitive visualizations. Available public data repositories Public data repositories on Polly consist of processed and curated datasets from various sources. They can be readily used for searching for new datasets or running an analysis on one or more datasets. AML: Microarray and RNA Sequencing datasets for Acute Myeloid Leukemia. GBM: Microarray and RNA Sequencing datasets for Gliblastoma Multiforme. IBD: Microarray and RNA Sequencing datasets for Inflammatory Bowel Disease. GEO: Microarray and RNA Sequencing datasets from Gene Expression Omnbius. Single cell Atlas: Single cell RNA Sequencing datasets from Gene Expression Omnibus GTEX: Normal tissue RNA Sequencing datasets from Genotype-Tissue Expression project TCGA: Tumor RNA Sequencing datasets from The Cancer Genome Atlas. COVID-19: Transcriptional datasets for SARS viruses, viral infections, and therapeutics for novel coronavirus. Additionally, the public data repositories also consist of publicly available databases that have been curated for annotations. These publicly available databases are currently part of these repositories. HMDB: Pathway information from Human Metabolome Database. KEGG: Pathway information from Kyoto Encyclopedia of Genes and Genomes. Reactome: Pathway information from Reactome. GWAS: Phenotypic data from Genome-Wide Association Studies Catalogue. Dataset Filtering Dataset Filtering Dashboard For smaller data lakes, we provide a dataset filtering interface. It allows you to explore and filter the relevant datasets present in the data lake. Filters and Columns The filtering interface provides 4 parameters that you can use to filter the datasets within the selected repository. The parameters are: Disease: This option will give you an overview of all the diseased type datasets present in the repository. You can choose to work on any of the disease options listed or the normal datasets. In order to do the selection, mark the checkboxes present besides the disease of your interest. Organism: It provides the list of the organisms associated with the datasets of the datalake. You can mark a selection to filter the datasets of only the desired organism. Tissue: This section will give you the distribution of tissue across the repository. Click on Load More to look at the entire list, or use the search option to find the tissue type you are looking for. Select the tissue type required to filter the datasets specific to it. Data Type: The dataset variety would be listed in this option. Choose the data type for your study by selecting the checkbox beside it. When the selections are marked, you can find the filtered datasets on the right panel. Note: You can select multiple entries at the same time. To clear your filters at any point in time, click on the clear option present beside all the parameters. Dataset Selection The right panel displays the dataset present in the repository. It incorporates the Dataset ID: Unique identifier associated with the dataset. Description: It encompasses the title of the paper. Organism: Organism associated with the dataset Datatype: Datatype of the dataset e.g. Transcriptomic, Metabolomics, Single Cell etc Disease: Disease studied with the selected dataset Tissue: Type of tissue the dataset is from Source: Provides the link to the publication Once you have narrowed down relevant omics datasets, you can mark a selection on the checkbox present beside the desired dataset. On-the-fly Analysis You can analyze the selected dataset on the fly using various applications on Polly. They enable you to perform various statistical analyses, displaying intuitive visualizations, and allowing you to create a hitlist while analyzing multiple datasets simultaneously. In order to select the tool of your analysis, click on Select the Application option at the bottom of the screen after selection of any dataset from the list and choose the analysis platform of your interest and click on open. Select the workspace where you would like to store the analysis and click on Launch to open the selected application/notebook. Data Lake Applications Data Lake Applications are built on top of data lakes to query and explore relevant datasets. The following data lake applications are a part of the current platform: Polly Discover Application: It is a platform for visualization, analytics, and exploration for bulk transcriptomics data curated from GEO. It offers users an interactive dashboard for analysis and visualization of transcriptomics data. Currently, the platform handles gene expression microarray and RNA-seq data and supports three species human, mouse, and rat. Single Cell Visualziation: It is a comprehensive visualization platform for single-cell transcriptomics data. The app is helpful in visualizing cells and the association of different genes with the metadata. Cellxgene: It is an interactive data explorer for single-cell transcriptomics datasets. DepMap CCLE: Exploration application for cell line dependency and gene expression data from DepMap and CCLE. GTEx Application GTEx Application is is a platform for visualization, analytics, and exploration of transcriptomics data from GTEx. Dual Mode Data Visualization(Metabolomics App: This app allows you to perform downstream analysis on untargeted unlabeled metabolomics data along with insightful visualizations. It provides a variety of normalization methods, scaling options, and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights. Discover Notebook: This app allows you to perform downstream analysis on untargeted unlabeled metabolomics data along with insightful visualizations. It provides a variety of normalization methods, scaling options, and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights. Polly Notebooks Docker Machine Configuration Discover Notebook Single-cell Single Cell Downstream Memory-optimized 32GB, Polly 2x-large Discover Notebook Transcriptomics RNA-Seq Downstream RNA-Seq Downstream Discover Notebook Proteomics RNA-Seq Downstream Polly medium 4GB Discover Notebook Metabolomics Metabolomics Polly medium 4GB Polly Discover Application Opening the app Upon opening the Discover portal on Polly, choose a data repository that you would like to explore. The page should look something like this. After selecting a repository, you\u2019ll be able to view a filtering interface which provides parameters that you can use to filter the datasets within the selected repository. Once you select a dataset , you can access the integrated tools attached with the repository. For transcriptomics data, you can use discover application for further analysis of transcriptomics data. The app shows overview page which contains a brief description of the application, it's scope and the usage as shown below. Exploring the data lake Search for relevant datasets by navigating to the Dataset Search tab in the navigation pane to the left. Keyword search can be applied to the following fields: Data Set ID Data Set Source Description Diseases Is Public Organisms Platform Tissue Year The search will return all datasets that are associated with your search. The result should look like the image below. The table shown above shows very few columns by default. In order to view the other columns in the table, you can select the fields from Available Columns and click on Show! button. Download Selected Dataset button will let you download the dataset that you have selected on your local system. Export results to CSV button will let you download the search result table in the form of a .csv file. Once you have narrowed down the relevant datasets, you can analyze one or more datasets on the fly within the app. Analyzing a single dataset You can analyze a single dataset by selecting the checkbox to the left of the entry in the table. Once you\u2019ve selected the checkbox, click on the Analyze Data button below the table description. After clicking the Analyze Data button, the app will read the selected dataset and take you to the Dataset Analysis tab. Here, you can perform the following analyses: Principal Component Analysis (PCA) Principal Component Analysis: Also known as PCA plot, it is used to see the overall differences between cohorts of interest, if a strong separation is found along X axis (PC1) then that means strong biological differences between cohorts of interest. One can also increase the number of genes considered in the PCA plot, as one increases the number of genes, it is bound to decrease the PC1 component. Boxplot Visualization Boxplot can be really useful in understanding the distribution of expression within a dataset. For any downstream analysis such as differential expression or pathway analysis, the distribution has to be normal since they use tests which assume this distribution. Plots A box and whisker plot (a boxplot) is a graph that presents information from a five-number summary namely lower extreme, lower quartile, median, upper quartile, and upper extreme. In this plot, the median is marked by a vertical line inside the box; the ends of the box are upper and lower quartiles; the two lines outside the box extend to the highest and lowest observations. It is useful for knowing the nature of distribution (i.e., skewed) and potential unusual observations. Heatmap A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. This heatmap shows the cohort wise mean expression of a particular gene. The samples are aggregated on the basis of a given cohort and the mean is calculated based on the cohort information. Differential Expression Differential expression analysis means taking the normalised read count data and performing statistical analysis to discover quantitative changes in expression levels between experimental groups. For example, we use statistical testing to decide whether, for a given gene, an observed difference in read counts is significant, that is, whether it is greater than what would be expected just due to natural random variation. X2K Analysis X2K infers upstream regulatory networks from signatures of differentially expressed genes. By combining transcription factor enrichment analysis, protein-protein interaction network expansion, with kinase enrichment analysis, X2K produces inferred networks of transcription factors, proteins, and kinases predicted to regulate the expression of the inputted gene list. Gene Ontology Plot Gene Ontology Annotation Plot is a simple but useful tool for visualizing, comparing and plotting GO (Gene Ontology) annotation results. Enrichr Enrichr, includes new gene-set libraries, an alternative approach to rank enriched terms, and various interactive visualization approaches to display enrichment results using the JavaScript library, Data Driven Documents (D3). GSEA Gene Set Enrichment Analysis (GSEA) is a computational method that determines whether an a priori defined set of genes shows statistically significant, concordant differences between two biological states (e.g. phenotypes). Specific Pathway Visualization using Pathview Pathview maps, integrates and renders a wide variety of biological data on relevant pathway graphs. GTEx Opening the app GTEx respository can be accessed using the GTEx card on Discover. After selecting the repository, you\u2019ll be able to see a dashboard with different tissues. Select a dataset and use GTEx application to explore the dataset. The app will open and you should see the overview page which contains a brief overview of the application, scope and caveats as shown below. Analyzing a dataset As the application starts, it will load the requested dataset. Once it is loaded, it can be explored. Principal Component Analysis (PCA) Principal Component Analysis: Also known as PCA plot, it is used to see the overall differences between cohorts of interest, if a strong separation is found along X axis (PC1) then that means strong biological differences between cohorts of interest. It provides an aspect to check the quality control of different samples. The tab provides a metadata table to check different characterstics of samples. Furthermore various parameters of PCA can be adjusted. A publication quality and an interactive version of the PCA plot is available to explore. Bar Plot Barplot provides exploration of different genes either standalone or as a part of different pathways. The distribution can be grouped by different metadata cohorts such as tissue type or tissue-subtype. Using the Gene Expression toggle, different genes can be queried for different samples. Upon selecting the Pathway Visualization option, pathway specific genes can be selected. At a given time more than one pathways can be selected GTEx Expression Map GTEx Expression Map can be used to explore the distribution of selected genes in different GTEx tissues. After exploring the selected tissue and finding list of genes of interest, it presents visualization methods like GTEx Expression Violin and GTEx Expression Heatmap to study the distribution of genes across different tissues. A single gene can be selected to plot violin for it's expression across different tissues. Multipe genes can be used to make a heatmap for different tissues. Single Cell Visualization Opening the app Upon opening the Discover application on Polly, choose a relevant data repository which hosts single cell data. After selecting a repository, you\u2019ll be able to view a filtering interface which provides parameters that you can use to filter the datasets within the selected repository. Once you select a dataset you can access the integrated tools attached with the repository. You can use Single Cell Visualization application for further analysis of single cell data. The app will open and you should see the overview page which contains a brief overview of the application, scope and caveats as shown below. Exploring the data lake Search for relevant datasets by navigating to the Dataset Search tab in the navigation panel to the left. Keyword search can be applied to the following fields: DatasetID Platform Title Description Disease Pubmed ID Organism Cell Types Tissue The search will return all datasets that are associated with your search. The result should look like as shown below. The table shown above shows very few columns by default. In order to view the other columns in the table, you can select the fields from Available Columns and click on Show button. Once you have narrowed down the relevant datasets, you can analyze one dataset on the fly within the app. Analyzing a dataset You can analyze a single dataset by selecting the checkbox to the left of the entry in the table. Once you\u2019ve selected the checkbox, click on the Load button below the table description. After clicking the Load button, the app will read the selected dataset. Once the loading finishes, you can check the further tabs to explore the dataset: Dataset Summary This tab provides quick summary of the selected dataset. The tab reveals the no. of celltype/clusters, genes and cells, available metadata and quality control metrics for the selected dataset. The Value boxes at the top provide information about the no. of genes, cells and celltypes/clusters. Below it lies the metadata summary table which contains the different metadata fields and their categories. The table is searchable and clicking on a particular metadata shows it's distribution. For instance if you want to see the distribution of cell types in a study, you can search the keyword 'cell_type' in name search box. Upon click on it a table describing the distribution of cell types will popup. Quality Control(QC) Metrics helps in understanding the processing of the dataset. The application provides the opportunity to understand quality control using QC distribution and QC scatter plots. Using QC distribution one can understand the distribution of a single quality control metric in a particular metadata. For instance, if we want to check the distribution of gene counts in different cell type, we can select 'gene_counts' as QC metric and 'cell_type' as cluster. Using QC scatter plot one can understand the assosciation between distribution of two different quality control metrics. It can be useful in understanding the distribution of gene counts and UMI counts. Cell Visualization Cell visualization provides exploration of cells using dimensionality reduction methods. The tab presents the dimensionality reduction methods such as tSNE, UMAP, PCA and others to visualize the distribution of the cells. The Visualize cells panel on the right, shows the distribution of the cells in an interactive way. Using Feature selection panel, a metadata or a gene feature can be selected for plotting. The Customize visualization panel, offers the scope of customizing visualization features such as highlight non-zero cells , point size and method used based on personal preferences. Marker Marker tab provides exploration of distribution of markers. It presents visualization methods like dot plot and violin plot to study the distribution of genes across different metadata. The Marker selection panel, provides the user with options to choose different genes and metadata for plotting marker distribution. The Marker dot plot panel, is the area for exploring the average expression and distribution of marker using dot plot in an interactive fashion. The Marker Violin plot panel is the area for exploring the marker distribution using violin plot in an interactive fashion. Using Customize Violin slider, the range of values used for plotting violin can be adjusted. It is useful for observing a section of data such as the non-zero values of the expression. Cell-Type Aggregation After having insights about different markers in a single cell study, it is imperative to have a look at the expression of the selected markers across different cell types in different studies. Single cell visualization provides pan-dataset exloratory analysis in the form of Cell Type Aggregation . It provides the scope to query the respective repository for median expression of a selected gene across top 20 cell types based on expression. This provides the scope to study expression of a gene in a particular cell type in a given biological context. Potential use-case example: What is the expression of SARS-CoV2 virus entry specific host protein 'ACE2' across different cell types in different studies? The Cell Type Aggregation tab provides the input in the form of text field where query can be made for a gene, using our internal discover services, the input gene is queried across all the datasets in which it is expressed in the repository. To check for ACE2 expression, simply enter ACE2 in the search field. After entering the gene, clicking on Search gene will generate a bar plot showing the median expression and the distribution of gene expression across different studies. Access through Polly Notebook interface The Polly Notebook Dockers on Polly have an internal python package called \u2018discoverPy\u2019 pre-installed, which can be used to search for datasets in the various data repositories. Structure of a data repository A data repository is a collection of different files having different file types. To ensure easy access at a granular level to all datasets a data repository is organized in the following manner. Under this schema, each repository can be considered as a collection of indices which can be used for querying. The discoverPy package can access all indices of a data repository using API endpoints. Click here for a detailed documentation about Polly Notebooks. Usage Initialize a discover object This discover object is used to interact with a dataset repository. from discoverpy import Discover discover = Discover() discover List all available data repositories along with their indices. discover.get_repositories() Set a repository for fetching the different endpoints. Choose a repository from the list of repositories and use it's corresponding it to set the discover object to point to that repository. For single cell repositories use mode='single_cell'. For bulk data repositories use mode='bulk' (default) For geo repository repo_id is 16 . discover.set_repo('16') For sc_data_lake repository repo_id is 17 . discover.set_repo(\"17\", mode=\"single_cell\") After you\u2019ve added the indices for a repository, you can view the discover object discover Note that the \u2018annotation_repo\u2019 index is added automatically for each repository. Querying at the dataset level To search for datasets, the \u2018_files\u2019 index can be searched using the metadata fields present in it. Get fields present in the index discover.dataset_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. Some generic fields are present for each file. __bucket__: S3 bucket name __filetype__: Type of file such as pdf, gct etc. __key__: S3 key of the file __location__: Location of the file within data repository discover.dataset_repo.get_top_n_examples(n = 30) Search for a dataset by keyword in a particular field. Searching for \u201cmll\u201d in the field \u201cdescription\u201d here. dataset_query_df = discover.dataset_repo.query_dataset_by_field(\"description\",\"mll\") dataset_query_df Querying at the sample level GCT File Format The datasets in the public repositories are saved as a .gct file. This is a file format in which data can be stored along with the sample metadata. The data values in the actual matrix along with features (genes) are indexed in the \u2018_gct_data\u2019 index of the repository and the sample metadata is index in the \u2018_gct_metadata\u2019 of the index of the repository. H5AD File Format The single cell datasets in the public repositories are saved as a .h5ad file. This is a file format in which data can be stored along with the sample metadata. Get fields present in the index discover.sample_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. discover.sample_repo.get_top_n_examples(n =30) Search for samples by keyword in a particular field. Searching for \u201cM1\u201d in the field \u201cfab_classification_ch1\u201d here. fab_df = discover.sample_repo.query_samples_by_field(\"fab_classification_ch1\", \"M1\", n = 100) fab_df Search for samples by keywords in all fields. This can be used if the field to search for is not known beforehand. fab_all_fields = discover.sample_repo.query_samples_by_all_fields(\"M1\", n = 100) fab_all_fields Querying at the feature level The matrix of a .gct/.h5ad file contains the actual values for the different features(genes/metabolites). The \u2018_gct_index\u2019 or \u2018_h5ad_index\u2019 index of a repository can be queried for features. Get fields present in the index discover.feature_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. discover.feature_repo.get_top_n_examples(n=20) The \u2018__index__\u2019 column contains the feature name Get values for a particular feature across all samples in all datasets of the repository. Getting values for \u201cNRAS\u201d gene here. nras_df =discover.feature_repo.get_feature_values(\"NRAS\", n = 1000) nras_df To get features from all single cell datasets, use the variant get_feature_values_sc. See the following example. hhex_df =discover.feature_repo.get_feature_values_sc(\"HHEX\", n = 1000) hhex_df Access annotation repositories The various gene annotation databases can also be accessed through discoverpy. These can be used to get information about a particular gene or a set of genes. Get all annotation databases discover.annotation_repo.get_annotation_databases() Get annotations for a list of genes from a particular database. Getting Reactome pathways for the genes. discover.annotation_repo.get_feature_annotation('reactome', ['ACTA1', 'AHCTF1', 'AKAP13', 'ATP2C1', 'CDK7']) Advanced queries You can also perform more complex queries on multiple fields combining them with boolean logic. Some examples are shown here. Get microarray stem cell datasets which did not involve a knockdown experiment discover.dataset_repo.query_dataset_by_field_combination(and_fields={\"platform\":\"Microarray\", \"tissue\":\"stem cells\"}, not_fields={\"description\":\"knockdown\"}, n = 50) Get samples containing CD34 cells or mononuclear cells do not include de novo samples discover.sample_repo.query_samples_by_field_combination(or_fields = {\"cell_type_ch1\":\"CD34\",\"cell_type_ch1\":\"mononuclear\"}, not_fields = {\"treatment_protocol_ch1\":\"de novo\"}, n = 300) Downloading datasets You can use the get_file(key, repo_id, file_name) function to download a dataset from a datalake repository. The function has following 3 parameters: __key__: S3 key of the file __repo_id__: Repository id __file_name__: Name of the file with file extentions such as gct, h5ad etc. discover.get_file('AML_data_lake/data/Microarray/GSE76320/GCT/GSE76320_GPL8321_curated.gct', '1', 'GSE76320_GPL8321_curated.gct') Videos","title":"Data Lake"},{"location":"Data Lake.html#introduction","text":"With Polly you can access public databases which have been curated and stored in the form of data lakes or make data lakes with your own data. These data lakes can be explored and analyzed either through Polly Notebooks or the several Data Lake Applications available. Polly Discover consists of the following major components: Data Lake Curation Data lakes are reservoirs of information that contain multi-omics data, annotations from publicly available databases, publications, etc. Reservoirs are further segregated into two parts: public data repositories which are curated by Elucidata using public data sources, and private data repositories, where you can add your proprietary data. Data Lake Exploration To explore a data lake, Polly provides tools such as applications and Polly Notebooks. These tools enable you to find relevant data by searching for keywords associated with the file name, file metadata, or the contents of a file. On-the-fly Analysis Once you have narrowed down relevant omics datasets, you can analyze the dataset(s) on the fly using various statistical analyses while displaying intuitive visualizations.","title":"Introduction"},{"location":"Data Lake.html#available-public-data-repositories","text":"Public data repositories on Polly consist of processed and curated datasets from various sources. They can be readily used for searching for new datasets or running an analysis on one or more datasets. AML: Microarray and RNA Sequencing datasets for Acute Myeloid Leukemia. GBM: Microarray and RNA Sequencing datasets for Gliblastoma Multiforme. IBD: Microarray and RNA Sequencing datasets for Inflammatory Bowel Disease. GEO: Microarray and RNA Sequencing datasets from Gene Expression Omnbius. Single cell Atlas: Single cell RNA Sequencing datasets from Gene Expression Omnibus GTEX: Normal tissue RNA Sequencing datasets from Genotype-Tissue Expression project TCGA: Tumor RNA Sequencing datasets from The Cancer Genome Atlas. COVID-19: Transcriptional datasets for SARS viruses, viral infections, and therapeutics for novel coronavirus. Additionally, the public data repositories also consist of publicly available databases that have been curated for annotations. These publicly available databases are currently part of these repositories. HMDB: Pathway information from Human Metabolome Database. KEGG: Pathway information from Kyoto Encyclopedia of Genes and Genomes. Reactome: Pathway information from Reactome. GWAS: Phenotypic data from Genome-Wide Association Studies Catalogue.","title":"Available public data repositories"},{"location":"Data Lake.html#dataset-filtering","text":"","title":"Dataset Filtering"},{"location":"Data Lake.html#dataset-filtering-dashboard","text":"For smaller data lakes, we provide a dataset filtering interface. It allows you to explore and filter the relevant datasets present in the data lake.","title":"Dataset Filtering Dashboard"},{"location":"Data Lake.html#filters-and-columns","text":"The filtering interface provides 4 parameters that you can use to filter the datasets within the selected repository. The parameters are: Disease: This option will give you an overview of all the diseased type datasets present in the repository. You can choose to work on any of the disease options listed or the normal datasets. In order to do the selection, mark the checkboxes present besides the disease of your interest. Organism: It provides the list of the organisms associated with the datasets of the datalake. You can mark a selection to filter the datasets of only the desired organism. Tissue: This section will give you the distribution of tissue across the repository. Click on Load More to look at the entire list, or use the search option to find the tissue type you are looking for. Select the tissue type required to filter the datasets specific to it. Data Type: The dataset variety would be listed in this option. Choose the data type for your study by selecting the checkbox beside it. When the selections are marked, you can find the filtered datasets on the right panel. Note: You can select multiple entries at the same time. To clear your filters at any point in time, click on the clear option present beside all the parameters.","title":"Filters and Columns"},{"location":"Data Lake.html#dataset-selection","text":"The right panel displays the dataset present in the repository. It incorporates the Dataset ID: Unique identifier associated with the dataset. Description: It encompasses the title of the paper. Organism: Organism associated with the dataset Datatype: Datatype of the dataset e.g. Transcriptomic, Metabolomics, Single Cell etc Disease: Disease studied with the selected dataset Tissue: Type of tissue the dataset is from Source: Provides the link to the publication Once you have narrowed down relevant omics datasets, you can mark a selection on the checkbox present beside the desired dataset.","title":"Dataset Selection"},{"location":"Data Lake.html#on-the-fly-analysis","text":"You can analyze the selected dataset on the fly using various applications on Polly. They enable you to perform various statistical analyses, displaying intuitive visualizations, and allowing you to create a hitlist while analyzing multiple datasets simultaneously. In order to select the tool of your analysis, click on Select the Application option at the bottom of the screen after selection of any dataset from the list and choose the analysis platform of your interest and click on open. Select the workspace where you would like to store the analysis and click on Launch to open the selected application/notebook.","title":"On-the-fly Analysis"},{"location":"Data Lake.html#data-lake-applications","text":"Data Lake Applications are built on top of data lakes to query and explore relevant datasets. The following data lake applications are a part of the current platform: Polly Discover Application: It is a platform for visualization, analytics, and exploration for bulk transcriptomics data curated from GEO. It offers users an interactive dashboard for analysis and visualization of transcriptomics data. Currently, the platform handles gene expression microarray and RNA-seq data and supports three species human, mouse, and rat. Single Cell Visualziation: It is a comprehensive visualization platform for single-cell transcriptomics data. The app is helpful in visualizing cells and the association of different genes with the metadata. Cellxgene: It is an interactive data explorer for single-cell transcriptomics datasets. DepMap CCLE: Exploration application for cell line dependency and gene expression data from DepMap and CCLE. GTEx Application GTEx Application is is a platform for visualization, analytics, and exploration of transcriptomics data from GTEx. Dual Mode Data Visualization(Metabolomics App: This app allows you to perform downstream analysis on untargeted unlabeled metabolomics data along with insightful visualizations. It provides a variety of normalization methods, scaling options, and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights. Discover Notebook: This app allows you to perform downstream analysis on untargeted unlabeled metabolomics data along with insightful visualizations. It provides a variety of normalization methods, scaling options, and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights. Polly Notebooks Docker Machine Configuration Discover Notebook Single-cell Single Cell Downstream Memory-optimized 32GB, Polly 2x-large Discover Notebook Transcriptomics RNA-Seq Downstream RNA-Seq Downstream Discover Notebook Proteomics RNA-Seq Downstream Polly medium 4GB Discover Notebook Metabolomics Metabolomics Polly medium 4GB","title":"Data Lake Applications"},{"location":"Data Lake.html#polly-discover-application","text":"Opening the app Upon opening the Discover portal on Polly, choose a data repository that you would like to explore. The page should look something like this. After selecting a repository, you\u2019ll be able to view a filtering interface which provides parameters that you can use to filter the datasets within the selected repository. Once you select a dataset , you can access the integrated tools attached with the repository. For transcriptomics data, you can use discover application for further analysis of transcriptomics data. The app shows overview page which contains a brief description of the application, it's scope and the usage as shown below. Exploring the data lake Search for relevant datasets by navigating to the Dataset Search tab in the navigation pane to the left. Keyword search can be applied to the following fields: Data Set ID Data Set Source Description Diseases Is Public Organisms Platform Tissue Year The search will return all datasets that are associated with your search. The result should look like the image below. The table shown above shows very few columns by default. In order to view the other columns in the table, you can select the fields from Available Columns and click on Show! button. Download Selected Dataset button will let you download the dataset that you have selected on your local system. Export results to CSV button will let you download the search result table in the form of a .csv file. Once you have narrowed down the relevant datasets, you can analyze one or more datasets on the fly within the app. Analyzing a single dataset You can analyze a single dataset by selecting the checkbox to the left of the entry in the table. Once you\u2019ve selected the checkbox, click on the Analyze Data button below the table description. After clicking the Analyze Data button, the app will read the selected dataset and take you to the Dataset Analysis tab. Here, you can perform the following analyses: Principal Component Analysis (PCA) Principal Component Analysis: Also known as PCA plot, it is used to see the overall differences between cohorts of interest, if a strong separation is found along X axis (PC1) then that means strong biological differences between cohorts of interest. One can also increase the number of genes considered in the PCA plot, as one increases the number of genes, it is bound to decrease the PC1 component. Boxplot Visualization Boxplot can be really useful in understanding the distribution of expression within a dataset. For any downstream analysis such as differential expression or pathway analysis, the distribution has to be normal since they use tests which assume this distribution. Plots A box and whisker plot (a boxplot) is a graph that presents information from a five-number summary namely lower extreme, lower quartile, median, upper quartile, and upper extreme. In this plot, the median is marked by a vertical line inside the box; the ends of the box are upper and lower quartiles; the two lines outside the box extend to the highest and lowest observations. It is useful for knowing the nature of distribution (i.e., skewed) and potential unusual observations. Heatmap A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. This heatmap shows the cohort wise mean expression of a particular gene. The samples are aggregated on the basis of a given cohort and the mean is calculated based on the cohort information. Differential Expression Differential expression analysis means taking the normalised read count data and performing statistical analysis to discover quantitative changes in expression levels between experimental groups. For example, we use statistical testing to decide whether, for a given gene, an observed difference in read counts is significant, that is, whether it is greater than what would be expected just due to natural random variation. X2K Analysis X2K infers upstream regulatory networks from signatures of differentially expressed genes. By combining transcription factor enrichment analysis, protein-protein interaction network expansion, with kinase enrichment analysis, X2K produces inferred networks of transcription factors, proteins, and kinases predicted to regulate the expression of the inputted gene list. Gene Ontology Plot Gene Ontology Annotation Plot is a simple but useful tool for visualizing, comparing and plotting GO (Gene Ontology) annotation results. Enrichr Enrichr, includes new gene-set libraries, an alternative approach to rank enriched terms, and various interactive visualization approaches to display enrichment results using the JavaScript library, Data Driven Documents (D3). GSEA Gene Set Enrichment Analysis (GSEA) is a computational method that determines whether an a priori defined set of genes shows statistically significant, concordant differences between two biological states (e.g. phenotypes). Specific Pathway Visualization using Pathview Pathview maps, integrates and renders a wide variety of biological data on relevant pathway graphs.","title":"Polly Discover Application"},{"location":"Data Lake.html#gtex","text":"Opening the app GTEx respository can be accessed using the GTEx card on Discover. After selecting the repository, you\u2019ll be able to see a dashboard with different tissues. Select a dataset and use GTEx application to explore the dataset. The app will open and you should see the overview page which contains a brief overview of the application, scope and caveats as shown below. Analyzing a dataset As the application starts, it will load the requested dataset. Once it is loaded, it can be explored. Principal Component Analysis (PCA) Principal Component Analysis: Also known as PCA plot, it is used to see the overall differences between cohorts of interest, if a strong separation is found along X axis (PC1) then that means strong biological differences between cohorts of interest. It provides an aspect to check the quality control of different samples. The tab provides a metadata table to check different characterstics of samples. Furthermore various parameters of PCA can be adjusted. A publication quality and an interactive version of the PCA plot is available to explore. Bar Plot Barplot provides exploration of different genes either standalone or as a part of different pathways. The distribution can be grouped by different metadata cohorts such as tissue type or tissue-subtype. Using the Gene Expression toggle, different genes can be queried for different samples. Upon selecting the Pathway Visualization option, pathway specific genes can be selected. At a given time more than one pathways can be selected GTEx Expression Map GTEx Expression Map can be used to explore the distribution of selected genes in different GTEx tissues. After exploring the selected tissue and finding list of genes of interest, it presents visualization methods like GTEx Expression Violin and GTEx Expression Heatmap to study the distribution of genes across different tissues. A single gene can be selected to plot violin for it's expression across different tissues. Multipe genes can be used to make a heatmap for different tissues.","title":"GTEx"},{"location":"Data Lake.html#single-cell-visualization","text":"Opening the app Upon opening the Discover application on Polly, choose a relevant data repository which hosts single cell data. After selecting a repository, you\u2019ll be able to view a filtering interface which provides parameters that you can use to filter the datasets within the selected repository. Once you select a dataset you can access the integrated tools attached with the repository. You can use Single Cell Visualization application for further analysis of single cell data. The app will open and you should see the overview page which contains a brief overview of the application, scope and caveats as shown below. Exploring the data lake Search for relevant datasets by navigating to the Dataset Search tab in the navigation panel to the left. Keyword search can be applied to the following fields: DatasetID Platform Title Description Disease Pubmed ID Organism Cell Types Tissue The search will return all datasets that are associated with your search. The result should look like as shown below. The table shown above shows very few columns by default. In order to view the other columns in the table, you can select the fields from Available Columns and click on Show button. Once you have narrowed down the relevant datasets, you can analyze one dataset on the fly within the app. Analyzing a dataset You can analyze a single dataset by selecting the checkbox to the left of the entry in the table. Once you\u2019ve selected the checkbox, click on the Load button below the table description. After clicking the Load button, the app will read the selected dataset. Once the loading finishes, you can check the further tabs to explore the dataset: Dataset Summary This tab provides quick summary of the selected dataset. The tab reveals the no. of celltype/clusters, genes and cells, available metadata and quality control metrics for the selected dataset. The Value boxes at the top provide information about the no. of genes, cells and celltypes/clusters. Below it lies the metadata summary table which contains the different metadata fields and their categories. The table is searchable and clicking on a particular metadata shows it's distribution. For instance if you want to see the distribution of cell types in a study, you can search the keyword 'cell_type' in name search box. Upon click on it a table describing the distribution of cell types will popup. Quality Control(QC) Metrics helps in understanding the processing of the dataset. The application provides the opportunity to understand quality control using QC distribution and QC scatter plots. Using QC distribution one can understand the distribution of a single quality control metric in a particular metadata. For instance, if we want to check the distribution of gene counts in different cell type, we can select 'gene_counts' as QC metric and 'cell_type' as cluster. Using QC scatter plot one can understand the assosciation between distribution of two different quality control metrics. It can be useful in understanding the distribution of gene counts and UMI counts. Cell Visualization Cell visualization provides exploration of cells using dimensionality reduction methods. The tab presents the dimensionality reduction methods such as tSNE, UMAP, PCA and others to visualize the distribution of the cells. The Visualize cells panel on the right, shows the distribution of the cells in an interactive way. Using Feature selection panel, a metadata or a gene feature can be selected for plotting. The Customize visualization panel, offers the scope of customizing visualization features such as highlight non-zero cells , point size and method used based on personal preferences. Marker Marker tab provides exploration of distribution of markers. It presents visualization methods like dot plot and violin plot to study the distribution of genes across different metadata. The Marker selection panel, provides the user with options to choose different genes and metadata for plotting marker distribution. The Marker dot plot panel, is the area for exploring the average expression and distribution of marker using dot plot in an interactive fashion. The Marker Violin plot panel is the area for exploring the marker distribution using violin plot in an interactive fashion. Using Customize Violin slider, the range of values used for plotting violin can be adjusted. It is useful for observing a section of data such as the non-zero values of the expression. Cell-Type Aggregation After having insights about different markers in a single cell study, it is imperative to have a look at the expression of the selected markers across different cell types in different studies. Single cell visualization provides pan-dataset exloratory analysis in the form of Cell Type Aggregation . It provides the scope to query the respective repository for median expression of a selected gene across top 20 cell types based on expression. This provides the scope to study expression of a gene in a particular cell type in a given biological context. Potential use-case example: What is the expression of SARS-CoV2 virus entry specific host protein 'ACE2' across different cell types in different studies? The Cell Type Aggregation tab provides the input in the form of text field where query can be made for a gene, using our internal discover services, the input gene is queried across all the datasets in which it is expressed in the repository. To check for ACE2 expression, simply enter ACE2 in the search field. After entering the gene, clicking on Search gene will generate a bar plot showing the median expression and the distribution of gene expression across different studies.","title":"Single Cell Visualization"},{"location":"Data Lake.html#access-through-polly-notebook-interface","text":"The Polly Notebook Dockers on Polly have an internal python package called \u2018discoverPy\u2019 pre-installed, which can be used to search for datasets in the various data repositories.","title":"Access through Polly Notebook interface"},{"location":"Data Lake.html#structure-of-a-data-repository","text":"A data repository is a collection of different files having different file types. To ensure easy access at a granular level to all datasets a data repository is organized in the following manner. Under this schema, each repository can be considered as a collection of indices which can be used for querying. The discoverPy package can access all indices of a data repository using API endpoints. Click here for a detailed documentation about Polly Notebooks.","title":"Structure of a data repository"},{"location":"Data Lake.html#usage","text":"Initialize a discover object This discover object is used to interact with a dataset repository. from discoverpy import Discover discover = Discover() discover List all available data repositories along with their indices. discover.get_repositories() Set a repository for fetching the different endpoints. Choose a repository from the list of repositories and use it's corresponding it to set the discover object to point to that repository. For single cell repositories use mode='single_cell'. For bulk data repositories use mode='bulk' (default) For geo repository repo_id is 16 . discover.set_repo('16') For sc_data_lake repository repo_id is 17 . discover.set_repo(\"17\", mode=\"single_cell\") After you\u2019ve added the indices for a repository, you can view the discover object discover Note that the \u2018annotation_repo\u2019 index is added automatically for each repository.","title":"Usage"},{"location":"Data Lake.html#querying-at-the-dataset-level","text":"To search for datasets, the \u2018_files\u2019 index can be searched using the metadata fields present in it. Get fields present in the index discover.dataset_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. Some generic fields are present for each file. __bucket__: S3 bucket name __filetype__: Type of file such as pdf, gct etc. __key__: S3 key of the file __location__: Location of the file within data repository discover.dataset_repo.get_top_n_examples(n = 30) Search for a dataset by keyword in a particular field. Searching for \u201cmll\u201d in the field \u201cdescription\u201d here. dataset_query_df = discover.dataset_repo.query_dataset_by_field(\"description\",\"mll\") dataset_query_df","title":"Querying at the dataset level"},{"location":"Data Lake.html#querying-at-the-sample-level","text":"GCT File Format The datasets in the public repositories are saved as a .gct file. This is a file format in which data can be stored along with the sample metadata. The data values in the actual matrix along with features (genes) are indexed in the \u2018_gct_data\u2019 index of the repository and the sample metadata is index in the \u2018_gct_metadata\u2019 of the index of the repository. H5AD File Format The single cell datasets in the public repositories are saved as a .h5ad file. This is a file format in which data can be stored along with the sample metadata. Get fields present in the index discover.sample_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. discover.sample_repo.get_top_n_examples(n =30) Search for samples by keyword in a particular field. Searching for \u201cM1\u201d in the field \u201cfab_classification_ch1\u201d here. fab_df = discover.sample_repo.query_samples_by_field(\"fab_classification_ch1\", \"M1\", n = 100) fab_df Search for samples by keywords in all fields. This can be used if the field to search for is not known beforehand. fab_all_fields = discover.sample_repo.query_samples_by_all_fields(\"M1\", n = 100) fab_all_fields","title":"Querying at the sample level"},{"location":"Data Lake.html#querying-at-the-feature-level","text":"The matrix of a .gct/.h5ad file contains the actual values for the different features(genes/metabolites). The \u2018_gct_index\u2019 or \u2018_h5ad_index\u2019 index of a repository can be queried for features. Get fields present in the index discover.feature_repo.get_all_fields() To get a sense of what values are present in each field, one can view the top n entries. discover.feature_repo.get_top_n_examples(n=20) The \u2018__index__\u2019 column contains the feature name Get values for a particular feature across all samples in all datasets of the repository. Getting values for \u201cNRAS\u201d gene here. nras_df =discover.feature_repo.get_feature_values(\"NRAS\", n = 1000) nras_df To get features from all single cell datasets, use the variant get_feature_values_sc. See the following example. hhex_df =discover.feature_repo.get_feature_values_sc(\"HHEX\", n = 1000) hhex_df","title":"Querying at the feature level"},{"location":"Data Lake.html#access-annotation-repositories","text":"The various gene annotation databases can also be accessed through discoverpy. These can be used to get information about a particular gene or a set of genes. Get all annotation databases discover.annotation_repo.get_annotation_databases() Get annotations for a list of genes from a particular database. Getting Reactome pathways for the genes. discover.annotation_repo.get_feature_annotation('reactome', ['ACTA1', 'AHCTF1', 'AKAP13', 'ATP2C1', 'CDK7'])","title":"Access annotation repositories"},{"location":"Data Lake.html#advanced-queries","text":"You can also perform more complex queries on multiple fields combining them with boolean logic. Some examples are shown here. Get microarray stem cell datasets which did not involve a knockdown experiment discover.dataset_repo.query_dataset_by_field_combination(and_fields={\"platform\":\"Microarray\", \"tissue\":\"stem cells\"}, not_fields={\"description\":\"knockdown\"}, n = 50) Get samples containing CD34 cells or mononuclear cells do not include de novo samples discover.sample_repo.query_samples_by_field_combination(or_fields = {\"cell_type_ch1\":\"CD34\",\"cell_type_ch1\":\"mononuclear\"}, not_fields = {\"treatment_protocol_ch1\":\"de novo\"}, n = 300)","title":"Advanced queries"},{"location":"Data Lake.html#downloading-datasets","text":"You can use the get_file(key, repo_id, file_name) function to download a dataset from a datalake repository. The function has following 3 parameters: __key__: S3 key of the file __repo_id__: Repository id __file_name__: Name of the file with file extentions such as gct, h5ad etc. discover.get_file('AML_data_lake/data/Microarray/GSE76320/GCT/GSE76320_GPL8321_curated.gct', '1', 'GSE76320_GPL8321_curated.gct')","title":"Downloading datasets"},{"location":"Data Lake.html#videos","text":"","title":"Videos"},{"location":"Release Notes.html","text":"Release Notes February 26th, 2021 New Introduced the functionality that enables the users to host their own application on Polly by using Polly CLI . Enabled feature level querying for GEO Data Lake. Added Genomics docker for variant calling and annotation. Added a new notebook environment for Genomics Variant Analysis. Enabled partial string search for dataset id in the search bar. Added 20,096 new curated transcriptomics and single cell datasets to different Data Lakes. February 12th, 2021 New Introduced the status page for real time updates on Polly\u2019s status, downtime, incidents, and maintenance. Added auto-run feature for selected Studio Presets. Enabled component updating and versioning by component creator. Added 11,580 new curated transcriptomics datasets to GEO and LINCS Data Lakes. Update Updated the UI of visualization dashboard of Data Studio for better visibility. Updated all notebook dockers with the latest version of discoverpy (0.0.10). Added finer error and warning messages to CLI . Removed the 1000 row limit on query results in CLI . January 29th, 2021 New Public sharing of the reports created within any Studio session is now available on Polly. Added 14,727 new curated transcriptomics and metabolomics datasets with 9,513 transcriptomics datasets being added to the LINCS Data Lake. Update Added specific error message to indicate presence of multiple groups with the same compound name in Labeled LC-MS Workflow . Added specific error message in Labeled LC-MS Workflow if isotopologues of the compound are spread over different metagroups in El-MAVEN output. January 15th, 2021 New GTEx Correlation and Enrichment Analysis preset is now available which can be used to identify enriched pathways based on the gene correlations. Added TraceFinder Downstream Analysis preset with additional feature of translating the analytical insights into shareable dashboards. Added 1,836 new curated transcriptomics and proteomics datasets to different Data Lakes. Update Enabled use of retention time information for metabolite identification and updated Untargeted Pipeline library to handle already identified metabolities. January 1st, 2021 Update Updated Untargeted Pipeline to be compatible with El-MAVEN's peakML output. December 18th, 2020 New LINCS(Library of Integrated Network-Based Cellular Signatures) repository with 19,520 curated datasets has been added in Data Lake . Update Added ANOVA Test and updated Limma Test with extra filters for volcano plot and Heatmap for the differentially expressed results in the Dual Mode Data Visulaization . December 4th, 2020 New We now support reactions from Chinese Hamster Ovary (CHO) for integrated pathway analysis in IntOmix . Update Resolved timeout error for opening a folder containing large number of files within a Workspace. Resolved issue with Workspace root directory redirection on selection. November 20th, 2020 New Improved OmixWiki UI for better consumption. Added the ability to clone Notebooks within Workspaces. Update Added granular error messages for Notebook functions and CLI jobs. Resolved the issue with renaming large data files. Resolved the issue with folder breadcrumb in Workspaces. Fixed involuntary logout issue. November 6th, 2020 New Data transfer time limit has been extended to 8 hour enabling transfer of 1TB data through CLI at once. Update Updated user interface of Discover and Data Studio . Added filtering interface to GEO data lake. Added search functionality on Discover interface. Added highlight and cumulative size feature on multiselection in Workspaces . Updated collaborators icon to show number of collaborators. Resolved inconsistent log 2 FC values for multiple comparisons in IntOmix . Resolved sample name descrepancy in concentration plot of QuantFit . Fixed table column resizing error on filtering interface. Resolved a bug in Polly Docker Domain. October 23rd, 2020 New Hosted our first User Group Meeting . Introduced our public platform OmixWiki , showcasing top 100 cited COVID-19 publications with end to end omics analysis. Released the newest version of El-MAVEN v0.12.0 . Update Updated Workspaces user interface. Added filtering interface to COVID-19 data lake. Updated datasets searchability on dataset ID and description. Fixed incorrect memory error in CLI . October 9th, 2020 New Introduced the option to make dockers on Polly public by adding public docker domain. Welcome screen now displays the username. Decreased launch time for applications and notebooks through horizontal pod scaling and buffering. Update Fixed landing on Discover after logging in error. Fixed priority assignment of automated jobs error. Fixed renaming files after upload error. Fixed 404 error in Metabolomics Data Lake. Integrated documentation to every application. September 25th, 2020 New Introduced Labeled LC-MS Analysis preset for natural abundance correction and visualization for single or dual labeled LC-MS data combined with an interactive, customizable and shareable reporting dashboard. Integrated pathway visualization in Labeled LC-MS Workflow . Added dilution factor and protein normalization in the Lipidomics Visualization Dashboard . Update Added warning message to prevent duplicate folder creation in Workspaces. Fixed nested folder creation and notebook renaming error in Workspaces. Fixed 503 error in Metabolomics Data Lake. Fixed a bug associated with notebooks and shiny apps opening to a blank screen. Fixed error occurring in automated jobs. September 11th, 2020 New Introduced Data Studio that brings the tools you need to create, customize, and share your analysis effortlessly with your team across the world. Introduced CCLE Correlation Analysis for identification of features correlated with a gene mutation such as mutations in other genes, expression and sample level metadata. Update Updated the version of scanpy to 1.6.0 in single cell docker. Fixed a bug in notebook giving error with CLI commands. August 28th, 2020 New Introduced a metabolomics docker equipped with packages for analysis of metabolomics data. Added restore functionality to all the Data Lake applications . Added boxplots for lipids in Lipidomics application . Update Updated discoverpy package in all the dockers to the latest version. Fixed CellxGene visualization loading for specific datasets. Fixed duplicate metabolite generation issue within the Dual Mode Data Visualization application . Fixed minor UI issues in Workspaces. Decreased Workspaces loading time. August 14th, 2020 New Introduced Workspaces on Polly, which is a new and improved version of Polly Projects. Added GTEx app to process the filtered datasets from GTEx data lake. Added a filtering interface for GTEx data lake that allows filtering of the data on the basis of fields within the curated dataset. Integrated Discover and Dual Mode Visualization for processing and further analysis of transcriptomic and metabolomic and single cell filtered datasets. Integrated Notebook to process the filtered datasets. Hosted CellxGene for processing and visualization of single cell datasets. Update Enabled logs access functionality through Polly CLI . Added the python package, Discoverpy to all the dockers. Deprecated The Project Management Dashboard has been deprecated and replaced by Workspaces. July 31st, 2020 New Added dot plot for Gene Ontology in the Discover application. Added an extra layer of security in authentication. Update Allowed internal standards and unlabeled data to pass through the Labeled LC-MS Workflow to generate output. Added Phantasus, Boxplot & Whisker plot along with the bar plot in the Discover application. Fixed Polly CLI auto login error in notebooks. Fixed unresponsive notebook with infinite loading. July 17th, 2020 New We have released the newest version of Polly CLI v0.1.18 enabling you to run a CLI job without the need of \"secret\" key if the private docker is on Polly. Update Labeled LC-MS Workflow has N and C as indistinguishable isotopes. Improved the stability of both Shiny and Desktop Applications. Communication within the infrastructure is now through encrypted keys. Shiny apps as well as shiny states are encrypted during transit as well as storage. Added encryption for the disks running the computations. Encrypted buckets containing credentials. July 3rd, 2020 New We have released the newest version of El-MAVEN v0.11.0. Polly now provides its own docker repository for easy management of dockers . Update Added Si as an indistinguishable isotope in Labeled LC-MS Workflow. Introduced pre-processing functionalities along with updated selections and heatmap for visualization in Lipidomics Visualization Dashboard . Deprecated Deprecated El-MAVEN FirstView Integration. June 19th, 2020 New We now support reactions from Drosophila melanogaster for integrated pathway analysis in IntOmix. Introduced pathway enrichment and pathway view feature along with comparative analysis in Dual Mode Data Visualization. DEPMAP CCLE (DEPMAP Cancer cell line expression data and dependency scores for genes) repository has been added in Data Lake . Implemented input file access from the sub-folders of a project for applications. Update The Single Cell Downstream docker is updated with these new packages: rpy2, anndata2ri (Python packages), ExperimentHub (R package). Added a GPU instance for Polly CLI . June 5th, 2020 New Introduced visualization of labels in stacked plot within Labeled LC-MS Workflow. Enabled least privilege access for stringent access policies. Encryption of data in transit and at rest. Update Improved access logs throughout the platform. Enhanced security using a secrets management service. Implemented regular backups and versioning of data. May 22nd, 2020 New Introduced Polly QuantFit node in Compound Discoverer TM that allows peak picking and absolute quantification on raw data obtained from a Thermo Scientific TM Mass Spec instrument. May 8th, 2020 New We now host our desktop application, El-MAVEN on Polly . Phi calculation feature has been added to Labeled LC-MS/MS Workflow. Update Changed the optimized color palette in IntOmix from a red-yellow-green scale to a more intuitive red-green scale. All upregulated metabolites or genes are represented by a shade of red and downregulated metabolites or genes as a shade of green. Changed the non-optimized color palette in IntOmix from a pink-purple scale to a red-green scale to remove ambiguity. April 24th, 2020 New COVID-19 (Transcriptional datasets for SARS viruses, viral infections, and therapeutics for novel coronavirus) repository has been added in Data Lake . .update-button { background-color: #4C61AF; border: 1px solid #364574; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; } .new-button { background-color: #4CAF50; border: 1px solid #367437; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; } .Deprecated-button { background-color: #b30000; border: 1px solid #b30000; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; }","title":"Release Notes"},{"location":"Release Notes.html#release-notes","text":"February 26th, 2021 New Introduced the functionality that enables the users to host their own application on Polly by using Polly CLI . Enabled feature level querying for GEO Data Lake. Added Genomics docker for variant calling and annotation. Added a new notebook environment for Genomics Variant Analysis. Enabled partial string search for dataset id in the search bar. Added 20,096 new curated transcriptomics and single cell datasets to different Data Lakes. February 12th, 2021 New Introduced the status page for real time updates on Polly\u2019s status, downtime, incidents, and maintenance. Added auto-run feature for selected Studio Presets. Enabled component updating and versioning by component creator. Added 11,580 new curated transcriptomics datasets to GEO and LINCS Data Lakes. Update Updated the UI of visualization dashboard of Data Studio for better visibility. Updated all notebook dockers with the latest version of discoverpy (0.0.10). Added finer error and warning messages to CLI . Removed the 1000 row limit on query results in CLI . January 29th, 2021 New Public sharing of the reports created within any Studio session is now available on Polly. Added 14,727 new curated transcriptomics and metabolomics datasets with 9,513 transcriptomics datasets being added to the LINCS Data Lake. Update Added specific error message to indicate presence of multiple groups with the same compound name in Labeled LC-MS Workflow . Added specific error message in Labeled LC-MS Workflow if isotopologues of the compound are spread over different metagroups in El-MAVEN output. January 15th, 2021 New GTEx Correlation and Enrichment Analysis preset is now available which can be used to identify enriched pathways based on the gene correlations. Added TraceFinder Downstream Analysis preset with additional feature of translating the analytical insights into shareable dashboards. Added 1,836 new curated transcriptomics and proteomics datasets to different Data Lakes. Update Enabled use of retention time information for metabolite identification and updated Untargeted Pipeline library to handle already identified metabolities. January 1st, 2021 Update Updated Untargeted Pipeline to be compatible with El-MAVEN's peakML output. December 18th, 2020 New LINCS(Library of Integrated Network-Based Cellular Signatures) repository with 19,520 curated datasets has been added in Data Lake . Update Added ANOVA Test and updated Limma Test with extra filters for volcano plot and Heatmap for the differentially expressed results in the Dual Mode Data Visulaization . December 4th, 2020 New We now support reactions from Chinese Hamster Ovary (CHO) for integrated pathway analysis in IntOmix . Update Resolved timeout error for opening a folder containing large number of files within a Workspace. Resolved issue with Workspace root directory redirection on selection. November 20th, 2020 New Improved OmixWiki UI for better consumption. Added the ability to clone Notebooks within Workspaces. Update Added granular error messages for Notebook functions and CLI jobs. Resolved the issue with renaming large data files. Resolved the issue with folder breadcrumb in Workspaces. Fixed involuntary logout issue. November 6th, 2020 New Data transfer time limit has been extended to 8 hour enabling transfer of 1TB data through CLI at once. Update Updated user interface of Discover and Data Studio . Added filtering interface to GEO data lake. Added search functionality on Discover interface. Added highlight and cumulative size feature on multiselection in Workspaces . Updated collaborators icon to show number of collaborators. Resolved inconsistent log 2 FC values for multiple comparisons in IntOmix . Resolved sample name descrepancy in concentration plot of QuantFit . Fixed table column resizing error on filtering interface. Resolved a bug in Polly Docker Domain. October 23rd, 2020 New Hosted our first User Group Meeting . Introduced our public platform OmixWiki , showcasing top 100 cited COVID-19 publications with end to end omics analysis. Released the newest version of El-MAVEN v0.12.0 . Update Updated Workspaces user interface. Added filtering interface to COVID-19 data lake. Updated datasets searchability on dataset ID and description. Fixed incorrect memory error in CLI . October 9th, 2020 New Introduced the option to make dockers on Polly public by adding public docker domain. Welcome screen now displays the username. Decreased launch time for applications and notebooks through horizontal pod scaling and buffering. Update Fixed landing on Discover after logging in error. Fixed priority assignment of automated jobs error. Fixed renaming files after upload error. Fixed 404 error in Metabolomics Data Lake. Integrated documentation to every application. September 25th, 2020 New Introduced Labeled LC-MS Analysis preset for natural abundance correction and visualization for single or dual labeled LC-MS data combined with an interactive, customizable and shareable reporting dashboard. Integrated pathway visualization in Labeled LC-MS Workflow . Added dilution factor and protein normalization in the Lipidomics Visualization Dashboard . Update Added warning message to prevent duplicate folder creation in Workspaces. Fixed nested folder creation and notebook renaming error in Workspaces. Fixed 503 error in Metabolomics Data Lake. Fixed a bug associated with notebooks and shiny apps opening to a blank screen. Fixed error occurring in automated jobs. September 11th, 2020 New Introduced Data Studio that brings the tools you need to create, customize, and share your analysis effortlessly with your team across the world. Introduced CCLE Correlation Analysis for identification of features correlated with a gene mutation such as mutations in other genes, expression and sample level metadata. Update Updated the version of scanpy to 1.6.0 in single cell docker. Fixed a bug in notebook giving error with CLI commands. August 28th, 2020 New Introduced a metabolomics docker equipped with packages for analysis of metabolomics data. Added restore functionality to all the Data Lake applications . Added boxplots for lipids in Lipidomics application . Update Updated discoverpy package in all the dockers to the latest version. Fixed CellxGene visualization loading for specific datasets. Fixed duplicate metabolite generation issue within the Dual Mode Data Visualization application . Fixed minor UI issues in Workspaces. Decreased Workspaces loading time. August 14th, 2020 New Introduced Workspaces on Polly, which is a new and improved version of Polly Projects. Added GTEx app to process the filtered datasets from GTEx data lake. Added a filtering interface for GTEx data lake that allows filtering of the data on the basis of fields within the curated dataset. Integrated Discover and Dual Mode Visualization for processing and further analysis of transcriptomic and metabolomic and single cell filtered datasets. Integrated Notebook to process the filtered datasets. Hosted CellxGene for processing and visualization of single cell datasets. Update Enabled logs access functionality through Polly CLI . Added the python package, Discoverpy to all the dockers. Deprecated The Project Management Dashboard has been deprecated and replaced by Workspaces. July 31st, 2020 New Added dot plot for Gene Ontology in the Discover application. Added an extra layer of security in authentication. Update Allowed internal standards and unlabeled data to pass through the Labeled LC-MS Workflow to generate output. Added Phantasus, Boxplot & Whisker plot along with the bar plot in the Discover application. Fixed Polly CLI auto login error in notebooks. Fixed unresponsive notebook with infinite loading. July 17th, 2020 New We have released the newest version of Polly CLI v0.1.18 enabling you to run a CLI job without the need of \"secret\" key if the private docker is on Polly. Update Labeled LC-MS Workflow has N and C as indistinguishable isotopes. Improved the stability of both Shiny and Desktop Applications. Communication within the infrastructure is now through encrypted keys. Shiny apps as well as shiny states are encrypted during transit as well as storage. Added encryption for the disks running the computations. Encrypted buckets containing credentials. July 3rd, 2020 New We have released the newest version of El-MAVEN v0.11.0. Polly now provides its own docker repository for easy management of dockers . Update Added Si as an indistinguishable isotope in Labeled LC-MS Workflow. Introduced pre-processing functionalities along with updated selections and heatmap for visualization in Lipidomics Visualization Dashboard . Deprecated Deprecated El-MAVEN FirstView Integration. June 19th, 2020 New We now support reactions from Drosophila melanogaster for integrated pathway analysis in IntOmix. Introduced pathway enrichment and pathway view feature along with comparative analysis in Dual Mode Data Visualization. DEPMAP CCLE (DEPMAP Cancer cell line expression data and dependency scores for genes) repository has been added in Data Lake . Implemented input file access from the sub-folders of a project for applications. Update The Single Cell Downstream docker is updated with these new packages: rpy2, anndata2ri (Python packages), ExperimentHub (R package). Added a GPU instance for Polly CLI . June 5th, 2020 New Introduced visualization of labels in stacked plot within Labeled LC-MS Workflow. Enabled least privilege access for stringent access policies. Encryption of data in transit and at rest. Update Improved access logs throughout the platform. Enhanced security using a secrets management service. Implemented regular backups and versioning of data. May 22nd, 2020 New Introduced Polly QuantFit node in Compound Discoverer TM that allows peak picking and absolute quantification on raw data obtained from a Thermo Scientific TM Mass Spec instrument. May 8th, 2020 New We now host our desktop application, El-MAVEN on Polly . Phi calculation feature has been added to Labeled LC-MS/MS Workflow. Update Changed the optimized color palette in IntOmix from a red-yellow-green scale to a more intuitive red-green scale. All upregulated metabolites or genes are represented by a shade of red and downregulated metabolites or genes as a shade of green. Changed the non-optimized color palette in IntOmix from a pink-purple scale to a red-green scale to remove ambiguity. April 24th, 2020 New COVID-19 (Transcriptional datasets for SARS viruses, viral infections, and therapeutics for novel coronavirus) repository has been added in Data Lake . .update-button { background-color: #4C61AF; border: 1px solid #364574; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; } .new-button { background-color: #4CAF50; border: 1px solid #367437; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; } .Deprecated-button { background-color: #b30000; border: 1px solid #b30000; border-radius: 70px; color: #FFFFFF; padding: 0px 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 12px; margin: 4px 2px; cursor: default; }","title":"Release Notes"},{"location":"Apps/Host Apps.html","text":"Host Apps Polly can host Shiny applications, other web applications, and desktop applications. This means all your applications can reside on a single platform and can be shared with your colleagues easily. The following are the advantages of hosting applications on Polly: Get the Shiny application to run locally and leave the rest to Polly. The applications scale automatically on the Polly infrastructure for multiple users to use simultaneously. Access to the application is controlled by Polly authentication. Only users in your organization with required Polly authorization can access the applications. Have input and output data along with analysis stored at the same location in a Polly Workspace. Share the analyses along with input and output data by simply sharing the Workspace. Additional benefits for Shiny apps Restorability: Polly provides an infrastructure to store a state of UI elements of the Shiny app. If any saved state has been restored, the app would redo the computations required to get back to the saved state automatically. Shiny Applications Machine configurations available The following are the machine configurations available on Polly for hosting shiny apps: vCPUs Memory (GBs) 1 2 2 4 2 6 2 8 4 4 4 8 If you need any other machine configuration, please contact us at polly@elucidata.io . Languages Supported We support various versions of R and Python for Shiny App development. Versions of R available: 3.5.2 3.6.0 3.6.1 3.6.2 Contact us at polly@elucidata.io in case you need a different version and we will add for you. Versions of Python available : 2.7.16 3.5.9 3.6.10 3.7.6 3.8.1 Contact us at polly@elucidata.io in case you need a different version and we will add for you. The Shiny apps run on the base dockers maintained by Polly. Custom dockers with other languages can also be used on Polly. To add custom dockers for Shiny apps, fill this form and we will get back to you in a day with the docker ready on Polly. This gist will help in getting the file dependencies that are required for docker building. Writing a Shiny Application Packrat is used for R dependencies. Install packrat using the following command to avail all the Polly advantages. remotes::install_github(\"GeorgeSabu/packrat\") Install R dependencies required for Shiny Application using packrat by referring this documentation. The mapping between the name of requirement files and the corresponding Python versions (check the Python versions supported by Polly) is mentioned here. Any Python dependency has to be added in the requirement files. 2.7.16 \u2192 requirements2.txt 3.5.9 \u2192 requirements3.5.txt 3.6.10 \u2192 requirements3.6.txt 3.7.6 \u2192 requirements3.7.txt 3.8.1 \u2192 requirements3.txt Install Shiny executing the mentioned command. Note that installation using this command will ensure that you get all the Polly benefits. remotes::install_github(\"GeorgeSabu/shiny\") Write the Shiny application in the usual way. Hosting a Shiny Application on Polly Share the Github or Bitbucket Repository with the Shiny Application with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application Other Web Applications Any web application that can be dockerized can be hosted on Polly. Here are the steps to host Web Applications on Polly: Dockerize the Application and make sure that it runs locally. Share the docker file through Github or Bitbucket with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application Desktop Applications Any desktop application that can be dockerized can be hosted on Polly. Here are the steps to host Web Applications on Polly: Dockerize the Application using the base docker mithoopolly/gui_desktop_base_apps:bionic . To run the docker locally, execute the following command. docker run --privileged -p 8080:8080 --shm-size=1g Share the docker file through Github or Bitbucket with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application Note: To host Windows application on Polly, wine can be used. Here is an example repo of applications that have been hosted on Polly.","title":"Host Apps"},{"location":"Apps/Host Apps.html#host-apps","text":"Polly can host Shiny applications, other web applications, and desktop applications. This means all your applications can reside on a single platform and can be shared with your colleagues easily. The following are the advantages of hosting applications on Polly: Get the Shiny application to run locally and leave the rest to Polly. The applications scale automatically on the Polly infrastructure for multiple users to use simultaneously. Access to the application is controlled by Polly authentication. Only users in your organization with required Polly authorization can access the applications. Have input and output data along with analysis stored at the same location in a Polly Workspace. Share the analyses along with input and output data by simply sharing the Workspace. Additional benefits for Shiny apps Restorability: Polly provides an infrastructure to store a state of UI elements of the Shiny app. If any saved state has been restored, the app would redo the computations required to get back to the saved state automatically.","title":"Host Apps"},{"location":"Apps/Host Apps.html#shiny-applications","text":"","title":"Shiny Applications"},{"location":"Apps/Host Apps.html#machine-configurations-available","text":"The following are the machine configurations available on Polly for hosting shiny apps: vCPUs Memory (GBs) 1 2 2 4 2 6 2 8 4 4 4 8 If you need any other machine configuration, please contact us at polly@elucidata.io .","title":"Machine configurations available"},{"location":"Apps/Host Apps.html#languages-supported","text":"We support various versions of R and Python for Shiny App development. Versions of R available: 3.5.2 3.6.0 3.6.1 3.6.2 Contact us at polly@elucidata.io in case you need a different version and we will add for you. Versions of Python available : 2.7.16 3.5.9 3.6.10 3.7.6 3.8.1 Contact us at polly@elucidata.io in case you need a different version and we will add for you. The Shiny apps run on the base dockers maintained by Polly. Custom dockers with other languages can also be used on Polly. To add custom dockers for Shiny apps, fill this form and we will get back to you in a day with the docker ready on Polly. This gist will help in getting the file dependencies that are required for docker building.","title":"Languages Supported"},{"location":"Apps/Host Apps.html#writing-a-shiny-application","text":"Packrat is used for R dependencies. Install packrat using the following command to avail all the Polly advantages. remotes::install_github(\"GeorgeSabu/packrat\") Install R dependencies required for Shiny Application using packrat by referring this documentation. The mapping between the name of requirement files and the corresponding Python versions (check the Python versions supported by Polly) is mentioned here. Any Python dependency has to be added in the requirement files. 2.7.16 \u2192 requirements2.txt 3.5.9 \u2192 requirements3.5.txt 3.6.10 \u2192 requirements3.6.txt 3.7.6 \u2192 requirements3.7.txt 3.8.1 \u2192 requirements3.txt Install Shiny executing the mentioned command. Note that installation using this command will ensure that you get all the Polly benefits. remotes::install_github(\"GeorgeSabu/shiny\") Write the Shiny application in the usual way.","title":"Writing a Shiny Application"},{"location":"Apps/Host Apps.html#hosting-a-shiny-application-on-polly","text":"Share the Github or Bitbucket Repository with the Shiny Application with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application","title":"Hosting a Shiny Application on Polly"},{"location":"Apps/Host Apps.html#other-web-applications","text":"Any web application that can be dockerized can be hosted on Polly. Here are the steps to host Web Applications on Polly: Dockerize the Application and make sure that it runs locally. Share the docker file through Github or Bitbucket with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application","title":"Other Web Applications"},{"location":"Apps/Host Apps.html#desktop-applications","text":"Any desktop application that can be dockerized can be hosted on Polly. Here are the steps to host Web Applications on Polly: Dockerize the Application using the base docker mithoopolly/gui_desktop_base_apps:bionic . To run the docker locally, execute the following command. docker run --privileged -p 8080:8080 --shm-size=1g Share the docker file through Github or Bitbucket with the organization Elucidatainc . Mail the following details to polly@elucidata.io Name of the Application A brief description of the application (upto 255 characters) Link to the Github or Bitbucket Repo Memory (RAM) and Number CPUs required for the application Note: To host Windows application on Polly, wine can be used. Here is an example repo of applications that have been hosted on Polly.","title":"Desktop Applications"},{"location":"Apps/Introduction.html","text":"Overview The applications on Polly are built to process and visualize experimental data ranging from mass-spec based omics to sequencing-based omics, from dual-mode data visualization to the analysis of CRISPR screening. Though despite the variety and specificity of the apps present on Polly, there are a couple of features common across all apps. These features are built to help you get started with minimal effort, analyze and share results and processed data with your collaborators with ease. Demo Data To help you get started even if you do not have your own data, every app on Polly has demo data uploaded as shown in Figure 1, Figure 2 and Figure 3. Moreover the demo data also serves as a reference point for the input files and their format required for each app. Figure 1. Demo Data for FirstView Figure 2. Demo data for MetScape Figure 3. Demo data for Labeled LC-MS app Upload Data All Polly applications provide you the option to upload data from local storage as shown in Figure 4 and Figure 5. Figure 4. Upload data from local storage for MetScape Figure 5. Upload data from local storage for Labeled LC-MS app Some of the Polly applications also provide the ability to upload input files from Polly workspace by using the option Import from Polly as shown in Figure 6. This simplifies data processing as well as makes Polly the platform where biological data can be stored and processed conveniently. Figure 6. Upload data from workspace for Labeled LC-MS app Restore Analysis All Polly applications contain the restore functionality that allows any analysis to be restored to the last step. Analyses can be restored by navigating to the desired workspace. Click on the specific analysis and select the algorithm you want to restore. Clicking on Restore from the right panel will take you back to the application with the same data used before as shown in Figure 7. Restore helps laboratories and organizations with standardization across labs and improves reproducibility. Figure 7. Restore for Angular applications Some of the Polly applications also provide the ability to save at that exact moment of your choice using the Save the State option. Figure 8. Saving the states in Shiny applications For the applications providing the Save the state option, you have an additional step to choose which state to restore. Multiple states, if saved are depicted as versions which makes it extremely easy to visually identify the state of interest as shown in Figure 8 and Figure 9. Figure 9. Restoring the saved states Download Plots & Output All applications on Polly allow you to download processed data and plots when displayed so that if needed you can verify from a third party about exactly how the data is being processed at a specific step. The tabular data can be downloaded as .csv files whereas the plots can be downloaded as .png, .jpeg or .svg files as shown in Figure 10, FIgure 11 and Figure 12. Figure 10. Download Pathway Dashboard as an image or the processed data as a .csv file Figure 11. Download PCA Plot as an image Figure 12. Download the processed data as a .csv file As with upload, data can be downloaded to local storage for Angular and Shiny apps as well as Polly workspace for Shiny apps. Reports Reports serve as the culmination of each analysis where all plots and insights generated are added in a way that is easy to understand and draw conclusions from when shared with collaborators or even used as a point of reference for something done earlier. Presence of reports for each analysis ordered sequentially promotes reproducibility across the laboratory or the organization and reduces the manual labor involved in knowledge transfer. To ensure that you do not spend a lot of time downloading plots and arranging them in a pre-defined order with the insights generated each time for a study, we have built One Click Report (OCR), functionality that allows you to generate reports in a standardized format. Customization The major advantage with our implementation is customization and ease of use. With our framework, we can implement an OCR for you on the app of your choice within 1 week . You can customize the structure of the report, any text to be included and the plots themselves. Plots not on the app can be added as well as the existing plots modified according to your requirements. Currently, custom OCR is integrated in the following apps: MetScape","title":"Introduction"},{"location":"Apps/Introduction.html#overview","text":"The applications on Polly are built to process and visualize experimental data ranging from mass-spec based omics to sequencing-based omics, from dual-mode data visualization to the analysis of CRISPR screening. Though despite the variety and specificity of the apps present on Polly, there are a couple of features common across all apps. These features are built to help you get started with minimal effort, analyze and share results and processed data with your collaborators with ease.","title":"Overview"},{"location":"Apps/Introduction.html#demo-data","text":"To help you get started even if you do not have your own data, every app on Polly has demo data uploaded as shown in Figure 1, Figure 2 and Figure 3. Moreover the demo data also serves as a reference point for the input files and their format required for each app. Figure 1. Demo Data for FirstView Figure 2. Demo data for MetScape Figure 3. Demo data for Labeled LC-MS app","title":"Demo Data"},{"location":"Apps/Introduction.html#upload-data","text":"All Polly applications provide you the option to upload data from local storage as shown in Figure 4 and Figure 5. Figure 4. Upload data from local storage for MetScape Figure 5. Upload data from local storage for Labeled LC-MS app Some of the Polly applications also provide the ability to upload input files from Polly workspace by using the option Import from Polly as shown in Figure 6. This simplifies data processing as well as makes Polly the platform where biological data can be stored and processed conveniently. Figure 6. Upload data from workspace for Labeled LC-MS app","title":"Upload Data"},{"location":"Apps/Introduction.html#restore-analysis","text":"All Polly applications contain the restore functionality that allows any analysis to be restored to the last step. Analyses can be restored by navigating to the desired workspace. Click on the specific analysis and select the algorithm you want to restore. Clicking on Restore from the right panel will take you back to the application with the same data used before as shown in Figure 7. Restore helps laboratories and organizations with standardization across labs and improves reproducibility. Figure 7. Restore for Angular applications Some of the Polly applications also provide the ability to save at that exact moment of your choice using the Save the State option. Figure 8. Saving the states in Shiny applications For the applications providing the Save the state option, you have an additional step to choose which state to restore. Multiple states, if saved are depicted as versions which makes it extremely easy to visually identify the state of interest as shown in Figure 8 and Figure 9. Figure 9. Restoring the saved states","title":"Restore Analysis"},{"location":"Apps/Introduction.html#download-plots-output","text":"All applications on Polly allow you to download processed data and plots when displayed so that if needed you can verify from a third party about exactly how the data is being processed at a specific step. The tabular data can be downloaded as .csv files whereas the plots can be downloaded as .png, .jpeg or .svg files as shown in Figure 10, FIgure 11 and Figure 12. Figure 10. Download Pathway Dashboard as an image or the processed data as a .csv file Figure 11. Download PCA Plot as an image Figure 12. Download the processed data as a .csv file As with upload, data can be downloaded to local storage for Angular and Shiny apps as well as Polly workspace for Shiny apps.","title":"Download Plots &amp; Output"},{"location":"Apps/Introduction.html#reports","text":"Reports serve as the culmination of each analysis where all plots and insights generated are added in a way that is easy to understand and draw conclusions from when shared with collaborators or even used as a point of reference for something done earlier. Presence of reports for each analysis ordered sequentially promotes reproducibility across the laboratory or the organization and reduces the manual labor involved in knowledge transfer. To ensure that you do not spend a lot of time downloading plots and arranging them in a pre-defined order with the insights generated each time for a study, we have built One Click Report (OCR), functionality that allows you to generate reports in a standardized format. Customization The major advantage with our implementation is customization and ease of use. With our framework, we can implement an OCR for you on the app of your choice within 1 week . You can customize the structure of the report, any text to be included and the plots themselves. Plots not on the app can be added as well as the existing plots modified according to your requirements. Currently, custom OCR is integrated in the following apps: MetScape","title":"Reports"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html","text":"Introduction Overview Oncology is in the midst of a paradigm shift towards personalized medicine with the increased wave of next-generation sequence data from labs and clinics. To the modern cancer biologist, the genomic and transcriptomic features of a patient\u2019s tumor have the potential to function as a key guide for selecting patient treatments. Clinical trials remain costly, however, and the need for reliable biomarkers for cancer drug sensitivity is as great as ever. Enter the Cancer Cell Line Encyclopedia (CCLE), a collection of open access whole genome, whole exome, and RNA-seq datasets encompassing nearly 1000 human cancer cell lines. A project of the Broad Institute , Novartis Institutes for Biomedical Research , and the Genomics Institute of the Novartis Research Foundation , the CCLE correlates genomic data from 947 human cancer cell lines with pharmacological profiles of 24 anticancer drugs. The availability of these datasets allows for large-scale comparative analysis. The CCLE Correlation Analysis App can be used to identify features correlated with a gene mutation such as mutations in other genes, expression, and sample-level metadata. Scope of the App Annotate the column metadata with a gene mutation. Filter samples from the expression data and metadata based on selection. Analyze the correlation between the mutation status of the selected gene against the other features. Performs univariate analysis like limma t-test along with multivariate analysis like PCA analysis. Getting Started User Input The data files can be downloaded from DepMap portal . The DepMap release version and file type to be used are mentioned below. CCLE expression: File \u201cCCLE_expression_v2.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal. CCLE mutation: Mutation File - File \u201cCCLE_mutations.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal. CCLE metadata: File \u201csample_info.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal. Tutorial Select CCLE Correlation Analysis App from the dashboard under the Studio Presets tab. Figure 1. Polly Dashboard Select an existing Workspace from the drop-down and provide the Name of the Session to be redirected to CCLE Correlation Analysis Preset's upload page. Figure 2. App selection Data Curation Upload Files The first component is the Upload component which allows you to upload the input files required for processing through the app. It includes the raw expression, sample metadata and mutation files from CCLE. Figure 3. CCLE Correlation Analaysis Interface To upload the input files, Click on Browse , which will open a sliding menu containing the data files in the selected workspace. Select the desired file and click on Import . Figure 4. Browse Workspace files Once all the files are imported, click on Run Task to execute the component. Figure 5. Execute component It will generate three output files: Figure 6. Generated output Expression matrix: Table for the expression values of genes in cell lines with genes in rows and samples in columns. Mutation matrix: Table depicting the presence and absence of gene mutations in cell lines. 0s represent the wild-type genes and 1s represent mutated genes. CCLE metadata: Table for the metadata information associated with each cell line. Rows represent samples and columns represent various attributes of the samples. Annotate samples with gene mutation This component allows you to annotate the column metadata with the gene mutation. Mutation Gene: Select a gene for annotation using the dropdown menu of Mutation Gene . Across samples, the mutation status of this gene will be correlated with various features such as the expression of other genes and the sample level metadata. Once all the fields are filled, Click on Run Task to execute the component. Note: The expression outputs generated at the previous step are used as an input here. These input files are not required to be added at each step. They automatically get mapped from the previous component. Figure 7. Annotation parameters The generated output: CCLE annotated metadata: New column will be added which has the information about the mutation status of the selected gene across all the samples. If multiple genes are selected, then multiple columns will be added. Figure 8. Annotation output Filtering This component is used to filter samples from the expression data and metadata based on your selection. The component provides an option to choose a column and subsequent value from within that column to subset the data. Figure 9. Filtering parameters Filter samples?: It provides two options - Yes and No . If ' No \u2019 is selected, then no filtering is applied to the data and all the samples are available for usage in the downstream components. If ' Yes ' is selected, then you must select the options for filtering from the beneath dropdowns. Select Column: Names of the columns from the metadata file are enlisted here for you to choose. Column Value: Once a column has been selected in the Select Column dropdown, then all the unique values from the selected column are available for you to pick from. Once all the fields are selected, click on Run Task to execute the component. It will generate three output files: Figure 10. Filtering output Expression data: Subsetted expression data with filtered samples. Mutation data: Subsetted mutation matrix with filtered samples. Metadata: Subsetted metadata with filtered samples. Data Exploration Gene expression Box plot This component generated a gene expression Boxplot. It is a visualization component that uses a five-number summary (minimum, first quartile, median, third quartile, and maximum) to display the distribution of data based on quartiles. Outliers are also displayed as individual points. Figure 11. Boxplot parameters Select gene: Dropdown for picking the gene for which the boxplot is to be made. Select metadata column: Dropdown to select one of the metadata columns. After executing the component, an interactive Boxplot will be generated as an output. Figure 12. Boxplot PCA This component allows you to simplify the complexity of high-dimensional data while retaining the trends and patterns in it. It projects the data onto a lower dimension with an objective to find the best summary of the data using a limited number of principal components that help in understanding the clustering pattern between biologically grouped and ungrouped samples. Figure 13. PCA parameters Cohort Column: Dropdown to select one of the metadata columns. Top N Variants: The top N variable entities will be used for PCA calculation. Define the number in this box. The default number used is 1500. It generates two outputs: PCA Plot: A plot is created where the samples are labeled based on the cohort selected in the metadata column. When you hovers over the points, sample ID and percentage of variance explained by each principal component are displayed along with the cohort. PCA Score: Table of the first 10 PC values and metadata columns. Figure 14. PCA outputs Correlation This component uses Fisher's exact test to test for correlation between the mutation status of the selected gene against the other selected features. Correlation with other gene mutations - A contingency table for each gene with the mutation count across all the samples for the selected gene and the gene against which the test is to be done. Correlation with metadata - The selected metadata column is split so that each newly created column represents one unique value from the column and the values represent the presence(1) or absence(0) of that feature. Contingency table created for each of these columns separately. Correlation with expression - The gene expression values are binarised and samples are labeled Highly expressed where there is at least one standard deviation difference on the higher side. Rest are labeled Not expressed . A contingency table is then created. Figure 15. Correlation parameters Mutation gene: Dropdown for picking the gene for which its mutation status across samples will be correlated with various features such as expression of other genes and the sample level metadata. Metadata column: Dropdown to select one of the metadata columns. Test correlation: Multi-select dropdown with two options - expression data and mutation data. You can select either of them or both. The component performs Fisher's exact test to test for correlation of the mutation status of the selected gene vs the options picked here. Once all the parameters are selected, execute the component by clicking on Run Task . It will generate three output files: Figure 16. Correlation outputs Pvals Metadata: Table for p -values of Fisher's exact test of selected gene mutation against all the unique values from the metadata column selected. Pvals Expression: Table for p -values of Fisher's exact test of selected gene mutation against the binarised expression values of all the genes present in the expression matrix. Pvals Mutation: Table for p -values of Fisher's exact test of selected gene mutation against the mutation status of all the genes in the mutation matrix. Differential Expression This component allows the search for differentially expressed (DE) genes, that is, genes that show differences in expression level between conditions or in other ways are associated with given predictors or responses. Figure 17. Differential expression parameters Cohort Column: Dropdown to select one of the metadata columns. Cohort A: Dropdown to select a cohort from the metadata column selected. Cohort B: Dropdown to select another cohort from the metadata column selected. Normalization: Perform log 2 normalization if data is not normalized. Algorithm: You can select any one of the two algorithms - limma or Unpaired t-test . Limma is an R package for the analysis of gene expression microarray data, especially the use of linear models for analyzing designed experiments and the assessment of differential expression. Limma provides the ability to analyze comparisons between many RNA targets simultaneously in arbitrary complicated designed experiments. An unpaired t-test (also known as an independent t-test) is a statistical procedure that compares the averages/means of two independent or unrelated groups to determine if there is a significant difference between the two. P-Value Correction: You can select the Benjamini-Hochberg method to correct the p -value for False Discovery Rate or the Bonferroni method to correct the p -value for Type I errors. P-Value Metric: Plot and calculate significance using the selected metric. p-value is the value returned by the algorithm while Adjusted p-value is the corrected value after applying one of the correction methods above. P-value threshold: You can select the appropriate threshold for the selected p -value metric. *p8-values lower than this threshold will be marked as significant. Absolute Log2FC Threshold: You can select the appropriate fold change threshold. Log 2 fold change values higher than this will be marked as significant. Once all the parameters are selected, execute the component by clicking on Run Task . It will generate thwo outputs: Differential Expression: Table with Differential Expression results with p -value and fold change. Volcano Plot: Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 18. Differential expression outputs Dashboard Data Studio lets you visualize your data with the number fo highly configurable charts and tables, which you can save and add to dashboards and then customize as needed. The Visualization Dashboard provides an at-a-glance view of the selected visualization charts. The dashboard is customizable and can be organized in the most effective way to help you understand complex relationships in your data and can be used to create engaging and easy-to-understand reports. A template of the report can also be defined to generate the output if required. The generated reports are interactive and can be shared with the collaborators. You can easily communicate and act on the customized data where all the members of your team can compare, filter and organize the exact data they need on the fly, in one report.","title":"CCLE Correlation Analysis"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#overview","text":"Oncology is in the midst of a paradigm shift towards personalized medicine with the increased wave of next-generation sequence data from labs and clinics. To the modern cancer biologist, the genomic and transcriptomic features of a patient\u2019s tumor have the potential to function as a key guide for selecting patient treatments. Clinical trials remain costly, however, and the need for reliable biomarkers for cancer drug sensitivity is as great as ever. Enter the Cancer Cell Line Encyclopedia (CCLE), a collection of open access whole genome, whole exome, and RNA-seq datasets encompassing nearly 1000 human cancer cell lines. A project of the Broad Institute , Novartis Institutes for Biomedical Research , and the Genomics Institute of the Novartis Research Foundation , the CCLE correlates genomic data from 947 human cancer cell lines with pharmacological profiles of 24 anticancer drugs. The availability of these datasets allows for large-scale comparative analysis. The CCLE Correlation Analysis App can be used to identify features correlated with a gene mutation such as mutations in other genes, expression, and sample-level metadata.","title":"Overview"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#scope-of-the-app","text":"Annotate the column metadata with a gene mutation. Filter samples from the expression data and metadata based on selection. Analyze the correlation between the mutation status of the selected gene against the other features. Performs univariate analysis like limma t-test along with multivariate analysis like PCA analysis.","title":"Scope of the App"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#user-input","text":"The data files can be downloaded from DepMap portal . The DepMap release version and file type to be used are mentioned below. CCLE expression: File \u201cCCLE_expression_v2.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal. CCLE mutation: Mutation File - File \u201cCCLE_mutations.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal. CCLE metadata: File \u201csample_info.csv\u201d from DepMap Public 20Q2 data release at the DepMap portal.","title":"User Input"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#tutorial","text":"Select CCLE Correlation Analysis App from the dashboard under the Studio Presets tab. Figure 1. Polly Dashboard Select an existing Workspace from the drop-down and provide the Name of the Session to be redirected to CCLE Correlation Analysis Preset's upload page. Figure 2. App selection","title":"Tutorial"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#data-curation","text":"","title":"Data Curation"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#upload-files","text":"The first component is the Upload component which allows you to upload the input files required for processing through the app. It includes the raw expression, sample metadata and mutation files from CCLE. Figure 3. CCLE Correlation Analaysis Interface To upload the input files, Click on Browse , which will open a sliding menu containing the data files in the selected workspace. Select the desired file and click on Import . Figure 4. Browse Workspace files Once all the files are imported, click on Run Task to execute the component. Figure 5. Execute component It will generate three output files: Figure 6. Generated output Expression matrix: Table for the expression values of genes in cell lines with genes in rows and samples in columns. Mutation matrix: Table depicting the presence and absence of gene mutations in cell lines. 0s represent the wild-type genes and 1s represent mutated genes. CCLE metadata: Table for the metadata information associated with each cell line. Rows represent samples and columns represent various attributes of the samples.","title":"Upload Files"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#annotate-samples-with-gene-mutation","text":"This component allows you to annotate the column metadata with the gene mutation. Mutation Gene: Select a gene for annotation using the dropdown menu of Mutation Gene . Across samples, the mutation status of this gene will be correlated with various features such as the expression of other genes and the sample level metadata. Once all the fields are filled, Click on Run Task to execute the component. Note: The expression outputs generated at the previous step are used as an input here. These input files are not required to be added at each step. They automatically get mapped from the previous component. Figure 7. Annotation parameters The generated output: CCLE annotated metadata: New column will be added which has the information about the mutation status of the selected gene across all the samples. If multiple genes are selected, then multiple columns will be added. Figure 8. Annotation output","title":"Annotate samples with gene mutation"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#filtering","text":"This component is used to filter samples from the expression data and metadata based on your selection. The component provides an option to choose a column and subsequent value from within that column to subset the data. Figure 9. Filtering parameters Filter samples?: It provides two options - Yes and No . If ' No \u2019 is selected, then no filtering is applied to the data and all the samples are available for usage in the downstream components. If ' Yes ' is selected, then you must select the options for filtering from the beneath dropdowns. Select Column: Names of the columns from the metadata file are enlisted here for you to choose. Column Value: Once a column has been selected in the Select Column dropdown, then all the unique values from the selected column are available for you to pick from. Once all the fields are selected, click on Run Task to execute the component. It will generate three output files: Figure 10. Filtering output Expression data: Subsetted expression data with filtered samples. Mutation data: Subsetted mutation matrix with filtered samples. Metadata: Subsetted metadata with filtered samples.","title":"Filtering"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#data-exploration","text":"","title":"Data Exploration"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#gene-expression-box-plot","text":"This component generated a gene expression Boxplot. It is a visualization component that uses a five-number summary (minimum, first quartile, median, third quartile, and maximum) to display the distribution of data based on quartiles. Outliers are also displayed as individual points. Figure 11. Boxplot parameters Select gene: Dropdown for picking the gene for which the boxplot is to be made. Select metadata column: Dropdown to select one of the metadata columns. After executing the component, an interactive Boxplot will be generated as an output. Figure 12. Boxplot","title":"Gene expression Box plot"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#pca","text":"This component allows you to simplify the complexity of high-dimensional data while retaining the trends and patterns in it. It projects the data onto a lower dimension with an objective to find the best summary of the data using a limited number of principal components that help in understanding the clustering pattern between biologically grouped and ungrouped samples. Figure 13. PCA parameters Cohort Column: Dropdown to select one of the metadata columns. Top N Variants: The top N variable entities will be used for PCA calculation. Define the number in this box. The default number used is 1500. It generates two outputs: PCA Plot: A plot is created where the samples are labeled based on the cohort selected in the metadata column. When you hovers over the points, sample ID and percentage of variance explained by each principal component are displayed along with the cohort. PCA Score: Table of the first 10 PC values and metadata columns. Figure 14. PCA outputs","title":"PCA"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#correlation","text":"This component uses Fisher's exact test to test for correlation between the mutation status of the selected gene against the other selected features. Correlation with other gene mutations - A contingency table for each gene with the mutation count across all the samples for the selected gene and the gene against which the test is to be done. Correlation with metadata - The selected metadata column is split so that each newly created column represents one unique value from the column and the values represent the presence(1) or absence(0) of that feature. Contingency table created for each of these columns separately. Correlation with expression - The gene expression values are binarised and samples are labeled Highly expressed where there is at least one standard deviation difference on the higher side. Rest are labeled Not expressed . A contingency table is then created. Figure 15. Correlation parameters Mutation gene: Dropdown for picking the gene for which its mutation status across samples will be correlated with various features such as expression of other genes and the sample level metadata. Metadata column: Dropdown to select one of the metadata columns. Test correlation: Multi-select dropdown with two options - expression data and mutation data. You can select either of them or both. The component performs Fisher's exact test to test for correlation of the mutation status of the selected gene vs the options picked here. Once all the parameters are selected, execute the component by clicking on Run Task . It will generate three output files: Figure 16. Correlation outputs Pvals Metadata: Table for p -values of Fisher's exact test of selected gene mutation against all the unique values from the metadata column selected. Pvals Expression: Table for p -values of Fisher's exact test of selected gene mutation against the binarised expression values of all the genes present in the expression matrix. Pvals Mutation: Table for p -values of Fisher's exact test of selected gene mutation against the mutation status of all the genes in the mutation matrix.","title":"Correlation"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#differential-expression","text":"This component allows the search for differentially expressed (DE) genes, that is, genes that show differences in expression level between conditions or in other ways are associated with given predictors or responses. Figure 17. Differential expression parameters Cohort Column: Dropdown to select one of the metadata columns. Cohort A: Dropdown to select a cohort from the metadata column selected. Cohort B: Dropdown to select another cohort from the metadata column selected. Normalization: Perform log 2 normalization if data is not normalized. Algorithm: You can select any one of the two algorithms - limma or Unpaired t-test . Limma is an R package for the analysis of gene expression microarray data, especially the use of linear models for analyzing designed experiments and the assessment of differential expression. Limma provides the ability to analyze comparisons between many RNA targets simultaneously in arbitrary complicated designed experiments. An unpaired t-test (also known as an independent t-test) is a statistical procedure that compares the averages/means of two independent or unrelated groups to determine if there is a significant difference between the two. P-Value Correction: You can select the Benjamini-Hochberg method to correct the p -value for False Discovery Rate or the Bonferroni method to correct the p -value for Type I errors. P-Value Metric: Plot and calculate significance using the selected metric. p-value is the value returned by the algorithm while Adjusted p-value is the corrected value after applying one of the correction methods above. P-value threshold: You can select the appropriate threshold for the selected p -value metric. *p8-values lower than this threshold will be marked as significant. Absolute Log2FC Threshold: You can select the appropriate fold change threshold. Log 2 fold change values higher than this will be marked as significant. Once all the parameters are selected, execute the component by clicking on Run Task . It will generate thwo outputs: Differential Expression: Table with Differential Expression results with p -value and fold change. Volcano Plot: Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 18. Differential expression outputs","title":"Differential Expression"},{"location":"Apps/Data Studio/CCLE Correlation Analysis.html#dashboard","text":"Data Studio lets you visualize your data with the number fo highly configurable charts and tables, which you can save and add to dashboards and then customize as needed. The Visualization Dashboard provides an at-a-glance view of the selected visualization charts. The dashboard is customizable and can be organized in the most effective way to help you understand complex relationships in your data and can be used to create engaging and easy-to-understand reports. A template of the report can also be defined to generate the output if required. The generated reports are interactive and can be shared with the collaborators. You can easily communicate and act on the customized data where all the members of your team can compare, filter and organize the exact data they need on the fly, in one report.","title":"Dashboard"},{"location":"Apps/Data Studio/Data Studio.html","text":"What is Data Studio? Data Studio application lets you explore data, make multiple workflows, and variate endlessly on those workflows. It allows the ingestion of all kinds of data and their combinations so that you can perform any analysis easily. The studio is made of smaller components that are split into various categories for ingestion, cleaning, analyzing, and visualizing your data. You can select these components and arrange them according to your analysis needs to create your own custom workflow. You can also save a particular sequence of the components that can be reused again. Furthermore, these components can be configured or custom components can be added by anyone to cater to their workflow requirements. Detailed documentation on the customization and creation of components can be found here . Scope of the application Explore any kind of data without being tied to a single workflow Perform any kind of analysis on any data within the same umbrella using the Polly components Lets you draw from a large library of components that allows you to clean, filter, analyze and visualize the data Allows usage of scripts and jupyter notebooks Allows you to add your own component to the workflow Lets you add the components in any desired order Provides workflow customization Allows you to extract the selected component order and save it as a defined workflow Data Studio Overview What is a Component? A component in Data Studio is a containerized script with specified input and output files along with its defined visualizations. Each component is a separate docker with only the essential library installations and a main script that can read inputs. The main script can be in R or Python. Apart from the output files, components are also responsible for writing the files required for data visualization, along with visualization parameters. Data Studio is fully customizable as you can select the component of your choice from the predefined list, or write your own component and add it to the list. Component templates are available to help with component creation. The Python3 and R templates are available on GitHub. You can easily download the templates. Read on about the creation of the components in detail from here . Data Studio Journeys You can explore the Polly Data Studio in two ways: Studio Core Studio core allows you to build and use your own custom workflow in Data Studio. It enables you to explore and define a workflow and ultimately visualize your data on a fully customizable dashboard and report. Build: It provides the flexibility to break down the goal of a workflow into the steps that should be executed by the workflow. Use any of the components from the component library for the chosen step of the analysis. Select and add different components to grow your workflow. You can then arrange the steps to complete your workflow. If you can\u2019t find the one you are looking for, make your own custom component and host it within your workflow. Visualize: Easily interact with the parameters of the selected components and customize your data visualizations through highly configurable charts like line, bar, and pie charts, area, and bubble graphs and tables, and more. Report: Ultimately create a custom version of the visualization dashboard to represent the report from the workflow. Easily annotate your report, apply styles, and color themes to make your data story work. Studio Preset Once the goal and the steps of the workflow are defined in Data Studio. It can be exported as a Studio Preset that will include a predefined series of steps that are required to complete an end-to-end process. Studio preset is present in the form of an application with the selected components that can be interacted with. It is made with the help of Studio Core for the ingestion of high throughputs of data which often requires performing the same analysis steps over and over again. Now if you are worrying that once the workflow is exported into a preset and cannot be customized again. We got you covered there as well. In case you need to again add a library component or a custom component within the workflow, it is fairly easy to do so. Getting Started The Studio Core has three main panels: Component Panel: Displays a list of all the steps/components selected in this particular session Parameters Panel: Allows you to select the component and input files Visualization Panel: Displays the selected visualization plot along with visualization parameters Add and Configure Components In order to add a component to your workflow, click on the Select Component option present in the parameter panel. You can select the desired component from the dropdown menu. You can search for the component as well from the search option. To add subsequent components, Click on the NEW option present in the component panel. Adding Input Files Once a component is selected it is required to add the input files. In order to do that, click on the Browse option. A slide menu will appear displaying the data files present in the selected workspace. Select the file and click on Import . Your file will be added. Note: Input files are not required to be added at each step. They automatically get mapped from the previous component. Executing the Components Each component comes with its associated parameters that are displayed in the parameter panel. Fill in all the mandatory fields using the dropdown menu or using the checkboxes. Once the parameters are specified, click on the Run Task option to execute your component. You can have a look at the state of your execution with the displayed status bar. The default visualization of the component will be generated on the Visualization Panel . Chart Selection If there are multiple visualizations for a particular component, you can use the Select a Chart option to choose the visualization you want displayed on your screen. At a time only a single visualization can be displayed on the Visualization Panel . In order to look at all the visualization simultaneously, save your visualization to a dashboard. Dashboard Data Studio lets you visualize your data with the number fo highly configurable charts and tables, which you can add to dashboards and then customize as needed. The Visualization Dashboard provides an at-a-glance view of the selected visualization charts. The dashboard is customizable and can be organized in the most effective way to help you understand complex relationships in your data and can be used to create engaging and easy-to-understand reports. The generated reports are interactive and can be shared with the collaborators. You can easily communicate and act on the customized data where all the members of your team can compare, filter and organize the exact data they need on the fly, in one report. Saving a Chart In order to add a visualization to your dashboard, you first need to save it under the desired dashboard name. In order to do that, click on the Save option present beside Run Query option. It will display a menu: Overwrite chart cohort: Chart would get saved with the default name Save as: Specify the name of your chart using this option Add the chart to an existing dashboard: Use the drop-down menu to select an existing dashboard. The chart will get added to the selected dashboard Add to new dashboard: Provide a new name of the dashboard and a new dashboard with the current chart will be created Once all the selections are done, click on Save . Visualizing the Dashboard Click on the Visualize Data tab. It will display the list of all your created dashboards. Select a dashboard to look at the added visualization charts. Editing a dashboard In order to edit your dashboard, click on the Edit Dashboard option present at the top right corner. There are a few ways to edit the dashboard Resizing the table/charts: By selecting the bottom right-hand corner of the table/chart (the cursor will change too), you can resize it by dragging and dropping. Inserting a Component Your charts and filters: You can use this tab to sort your charts Tabs: This option lets you create different tabs within the same dashboard Let's say you have two plots in your dashboard. After you insert the component, just drag and drop the two charts. Once the two charts have been separated, label the tabs with the corresponding dataset names (or any desired name) represented in the charts. Column: Lets you add an additional column in the dashboard Row: Lets you add an additional row in the dashboard Header: Lets you add a header to the dashboard Markdown: In this section, we will add some text to our dashboard. Within the Insert components pane, drag and drop a Markdown box on the dashboard. Now to edit the text, select the box. You can enter text in markdown format (see this Markdown Cheatsheet for more information about this format). You can toggle between Edit and Preview using the menu on the top of the box. Divider: Use this to divide different sections of your dashboard Note: Look for the blue lines which indicate the anchor where the box will go. Color The color tabs let you choose the color scheme from the 9 default schemes present in the dropdown. Finally, save your changes by selecting Save changes on the top right. Other Useful Features Restoring the analysis You can restore your analysis at any point in time. Data Studio contains the restore functionality that allows any analysis to be restored to the last step. Analyses can be restored by navigating to the desired workspace. Click on the specific analysis which will enable the Restore option on the right panel. Clicking on Restore will take you back to the application with the same data used before. Accessing the logs The log option present at top right corner of the screen allows you to look at the logs of the current session. You can look at the input files uploaded, the output files generated as well as the parameters used in this session. View Docker Logs : Enable View Docker Logs option to view the execution of the component on the code level. Select a job for which you want the logs from the drop-down menu. Once the job is selected, you will be able to visualize the docker logs. It will let you know what is happening, or what happened at every layer of the stack.","title":"Introduction"},{"location":"Apps/Data Studio/Data Studio.html#what-is-data-studio","text":"Data Studio application lets you explore data, make multiple workflows, and variate endlessly on those workflows. It allows the ingestion of all kinds of data and their combinations so that you can perform any analysis easily. The studio is made of smaller components that are split into various categories for ingestion, cleaning, analyzing, and visualizing your data. You can select these components and arrange them according to your analysis needs to create your own custom workflow. You can also save a particular sequence of the components that can be reused again. Furthermore, these components can be configured or custom components can be added by anyone to cater to their workflow requirements. Detailed documentation on the customization and creation of components can be found here .","title":"What is Data Studio?"},{"location":"Apps/Data Studio/Data Studio.html#scope-of-the-application","text":"Explore any kind of data without being tied to a single workflow Perform any kind of analysis on any data within the same umbrella using the Polly components Lets you draw from a large library of components that allows you to clean, filter, analyze and visualize the data Allows usage of scripts and jupyter notebooks Allows you to add your own component to the workflow Lets you add the components in any desired order Provides workflow customization Allows you to extract the selected component order and save it as a defined workflow","title":"Scope of the application"},{"location":"Apps/Data Studio/Data Studio.html#data-studio-overview","text":"","title":"Data Studio Overview"},{"location":"Apps/Data Studio/Data Studio.html#what-is-a-component","text":"A component in Data Studio is a containerized script with specified input and output files along with its defined visualizations. Each component is a separate docker with only the essential library installations and a main script that can read inputs. The main script can be in R or Python. Apart from the output files, components are also responsible for writing the files required for data visualization, along with visualization parameters. Data Studio is fully customizable as you can select the component of your choice from the predefined list, or write your own component and add it to the list. Component templates are available to help with component creation. The Python3 and R templates are available on GitHub. You can easily download the templates. Read on about the creation of the components in detail from here .","title":"What is a Component?"},{"location":"Apps/Data Studio/Data Studio.html#data-studio-journeys","text":"You can explore the Polly Data Studio in two ways: Studio Core Studio core allows you to build and use your own custom workflow in Data Studio. It enables you to explore and define a workflow and ultimately visualize your data on a fully customizable dashboard and report. Build: It provides the flexibility to break down the goal of a workflow into the steps that should be executed by the workflow. Use any of the components from the component library for the chosen step of the analysis. Select and add different components to grow your workflow. You can then arrange the steps to complete your workflow. If you can\u2019t find the one you are looking for, make your own custom component and host it within your workflow. Visualize: Easily interact with the parameters of the selected components and customize your data visualizations through highly configurable charts like line, bar, and pie charts, area, and bubble graphs and tables, and more. Report: Ultimately create a custom version of the visualization dashboard to represent the report from the workflow. Easily annotate your report, apply styles, and color themes to make your data story work. Studio Preset Once the goal and the steps of the workflow are defined in Data Studio. It can be exported as a Studio Preset that will include a predefined series of steps that are required to complete an end-to-end process. Studio preset is present in the form of an application with the selected components that can be interacted with. It is made with the help of Studio Core for the ingestion of high throughputs of data which often requires performing the same analysis steps over and over again. Now if you are worrying that once the workflow is exported into a preset and cannot be customized again. We got you covered there as well. In case you need to again add a library component or a custom component within the workflow, it is fairly easy to do so.","title":"Data Studio Journeys"},{"location":"Apps/Data Studio/Data Studio.html#getting-started","text":"The Studio Core has three main panels: Component Panel: Displays a list of all the steps/components selected in this particular session Parameters Panel: Allows you to select the component and input files Visualization Panel: Displays the selected visualization plot along with visualization parameters","title":"Getting Started"},{"location":"Apps/Data Studio/Data Studio.html#add-and-configure-components","text":"In order to add a component to your workflow, click on the Select Component option present in the parameter panel. You can select the desired component from the dropdown menu. You can search for the component as well from the search option. To add subsequent components, Click on the NEW option present in the component panel.","title":"Add and Configure Components"},{"location":"Apps/Data Studio/Data Studio.html#adding-input-files","text":"Once a component is selected it is required to add the input files. In order to do that, click on the Browse option. A slide menu will appear displaying the data files present in the selected workspace. Select the file and click on Import . Your file will be added. Note: Input files are not required to be added at each step. They automatically get mapped from the previous component.","title":"Adding Input Files"},{"location":"Apps/Data Studio/Data Studio.html#executing-the-components","text":"Each component comes with its associated parameters that are displayed in the parameter panel. Fill in all the mandatory fields using the dropdown menu or using the checkboxes. Once the parameters are specified, click on the Run Task option to execute your component. You can have a look at the state of your execution with the displayed status bar. The default visualization of the component will be generated on the Visualization Panel .","title":"Executing the Components"},{"location":"Apps/Data Studio/Data Studio.html#chart-selection","text":"If there are multiple visualizations for a particular component, you can use the Select a Chart option to choose the visualization you want displayed on your screen. At a time only a single visualization can be displayed on the Visualization Panel . In order to look at all the visualization simultaneously, save your visualization to a dashboard.","title":"Chart Selection"},{"location":"Apps/Data Studio/Data Studio.html#dashboard","text":"Data Studio lets you visualize your data with the number fo highly configurable charts and tables, which you can add to dashboards and then customize as needed. The Visualization Dashboard provides an at-a-glance view of the selected visualization charts. The dashboard is customizable and can be organized in the most effective way to help you understand complex relationships in your data and can be used to create engaging and easy-to-understand reports. The generated reports are interactive and can be shared with the collaborators. You can easily communicate and act on the customized data where all the members of your team can compare, filter and organize the exact data they need on the fly, in one report.","title":"Dashboard"},{"location":"Apps/Data Studio/Data Studio.html#saving-a-chart","text":"In order to add a visualization to your dashboard, you first need to save it under the desired dashboard name. In order to do that, click on the Save option present beside Run Query option. It will display a menu: Overwrite chart cohort: Chart would get saved with the default name Save as: Specify the name of your chart using this option Add the chart to an existing dashboard: Use the drop-down menu to select an existing dashboard. The chart will get added to the selected dashboard Add to new dashboard: Provide a new name of the dashboard and a new dashboard with the current chart will be created Once all the selections are done, click on Save .","title":"Saving a Chart"},{"location":"Apps/Data Studio/Data Studio.html#visualizing-the-dashboard","text":"Click on the Visualize Data tab. It will display the list of all your created dashboards. Select a dashboard to look at the added visualization charts.","title":"Visualizing the Dashboard"},{"location":"Apps/Data Studio/Data Studio.html#editing-a-dashboard","text":"In order to edit your dashboard, click on the Edit Dashboard option present at the top right corner. There are a few ways to edit the dashboard Resizing the table/charts: By selecting the bottom right-hand corner of the table/chart (the cursor will change too), you can resize it by dragging and dropping.","title":"Editing a dashboard"},{"location":"Apps/Data Studio/Data Studio.html#inserting-a-component","text":"Your charts and filters: You can use this tab to sort your charts Tabs: This option lets you create different tabs within the same dashboard Let's say you have two plots in your dashboard. After you insert the component, just drag and drop the two charts. Once the two charts have been separated, label the tabs with the corresponding dataset names (or any desired name) represented in the charts. Column: Lets you add an additional column in the dashboard Row: Lets you add an additional row in the dashboard Header: Lets you add a header to the dashboard Markdown: In this section, we will add some text to our dashboard. Within the Insert components pane, drag and drop a Markdown box on the dashboard. Now to edit the text, select the box. You can enter text in markdown format (see this Markdown Cheatsheet for more information about this format). You can toggle between Edit and Preview using the menu on the top of the box. Divider: Use this to divide different sections of your dashboard Note: Look for the blue lines which indicate the anchor where the box will go. Color The color tabs let you choose the color scheme from the 9 default schemes present in the dropdown. Finally, save your changes by selecting Save changes on the top right.","title":"Inserting a Component"},{"location":"Apps/Data Studio/Data Studio.html#other-useful-features","text":"Restoring the analysis You can restore your analysis at any point in time. Data Studio contains the restore functionality that allows any analysis to be restored to the last step. Analyses can be restored by navigating to the desired workspace. Click on the specific analysis which will enable the Restore option on the right panel. Clicking on Restore will take you back to the application with the same data used before. Accessing the logs The log option present at top right corner of the screen allows you to look at the logs of the current session. You can look at the input files uploaded, the output files generated as well as the parameters used in this session. View Docker Logs : Enable View Docker Logs option to view the execution of the component on the code level. Select a job for which you want the logs from the drop-down menu. Once the job is selected, you will be able to visualize the docker logs. It will let you know what is happening, or what happened at every layer of the stack.","title":"Other Useful Features"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html","text":"Introduction Overview Analysis of lipidomic data involves complex lipidome analysis. Lipidomics analysis across large sample sizes produces large datasets that require dedicated lipid identification, quantification and lipidome visualization. The wide diversity of native lipids and their modifications make their analyses challenging. Lipidomics Visualization Dashboard allows you to visualize, process and analyze the concentration data based on lipid species or classes along with sum of carbons and saturation type. The current version can calculate summary statistics, percentages and perform univariate analysis like ANOVA, Limma t-test along with Multivariate analysis like PCA. It helps you view lipids by their main class, sub class across cohorts. Scope of the app Performs preprocessing on input data. Calculates lipid species concentration percentage for each lipid class and species across multiple cohorts. Plots the lipid class and species concentrations and their percentages along with sum of carbons and saturation type in the form of pie charts and bar plots. Performs PCA for quality check. Performs differential expression using ANOVA or Limma t-test to find significant lipid species or class. Analyze clustering profiles through a heatmap. Figure 1. Lipidomics Visualization Dashboard Getting Started User Input Lipidomics analysis requires the following two files as input: Concentration File The concentration file is the data output from Lipidizer. The file should be in .xlsx format and must contain Lipids/ Fatty Acid concentrations sheet. Figure 2. Concentration file Metadata File The metadata file contains two columns, Name which contains all the samples used under study and Cohort which contains the cohort information for all the samples. Figure 3. Metadata file Species Metadata File The species metadata file is optional. It is helpful to classify the lipid species the way you desire. It contains three main columns: Species , Class which contains all the lipid species, and class used understudy and Chain which contains the fatty acid chain information for each lipid species. Figure 4. Species Metadata file Steps involved in data processing Upload input files Perform preprocessing of data Visualize input data based on lipid class or species, fatty acids or sum of carbons Perform quality checks on your data Perform differential expression using ANOVA or Limma t-test Visualize data in the form of a heatmap Caveats Lipidomics Visualization Dashboard only works with the output of SCIEX Lipidyzer TM Platform. Tutorial Upload Files Select Lipidomics Visulalization Dashboard from the dashboard under the Lipidomics Data Tab. Figure 5. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the session to be redirected to Lipidomics Visualization Dashboard's upload page. Figure 6. Workspace Selection Upload Interface Upload the concentration and metadata file in the upload space. You can chose to add species metadata file as well by selecting the checkbox. Once all the files are uploaded, click on Go . You can preview the uploaded raw data, metadata and species metadata. At the top, example files are available for download. You can either upload these example files or click on Run Example to automatically load the example data. Figure 7. Upload interface Preprocessing Interface The Preprocessing interface allows you to perform a multitude of functions on the data such as: Figure 8. Preprocessing Interface Drop Lipids This allows you to drop/remove unwanted lipids from further analysis. You can choose to drop lipids by their Species or Class . Select the lipid(s) from the drop-down menu and click on Drop Lipids to remove the selected lipids on the fly. Figure 9. Drop Lipids Drop Samples This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 10 after selecting the sample(s) from the drop-down menu. Figure 10. Drop Samples Visualization Interface This interface contains two tabs which allows you to visualize data for concentrations across multiple cohorts using pie charts and bar plots. You can visualize summary of each lipid class as well as each lipid species. Pie-Charts This interface allows you to visualize a summary of each Lipid species Fatty acids: Data is summarized by an individual fatty acid chain. For example \u201cDAG(16:0/16:1)\u201d fatty acid will be \u201cDAG(FA16:0)\u201d and \u201cDAG(FA16:1)\u201d Sum of Carbons: Data is summarized by the total number of carbons within the lipid species. For example for \u201cDAG(16:0/16:1)\u201d the sum of carbons will be \u201cDAG(32:1)\u201d. Select Data Type provides you the option to choose either of the above-mentioned options from the input excel file. Select Cohort allows you to select cohorts from the list of all cohorts for which the pie chart will be formed. You can select multiple cohorts for cross-cohort comparisons. Figure 11. Pie Charts Interface Saturation Type Summary Plot This plot allows you to view pie charts for selected cohorts, each representing portions of the saturation type within them. Figure 12. Saturation Type Summary Plot Select Lipid Class allows you to select the lipid class for which they can view the summary of lipid sub-class concentration percentages. Select Cohort allows you to select cohorts from the list of all cohorts for which the pie chart will be formed. You can select multiple cohorts to compare concentrations across cohorts a shown in Figure 9. Figure 13. Pie Charts Interface showing Lipid Species Summary Concentration Plots These are the bar plots for the given concentration where the bars would be created by taking the mean value of samples within the cohort. It allows you to select the data type which includes lipid species, fatty acids, and sum of carbons from the input file. You can further select the lipid type from the Lipid Species, Lipid Class, and Saturation Type Figure 14. Concentration plot interface Filter lipids using the the drop down as showed in Figure 15. Multiple lipids can be selected simultaneously. Figure 15. Filtering of lipids Note: Data selected by you will be taken as input for all the statistical analysis performed in other tabs. Bar plot is based on the mean concentration values across each cohort. Filter lipid class/species to visualize mean concentration values for those lipid classes/species only as shown in Figure 16. Figure 16. Concentration plot of filtered lipids Click on Table tab to visualize the respective data-table for individual sample concentration as shown in Figure 17. Figure 17. Data table Statistical Analysis ANOVA Test ANOVA Test is used to identify the most significant lipid species/lipid classes. Here the results will be calculated based on the data selected in the previous tab (visualization summary) and results are displayed in a tabular format without pressing any button. You can sort any column from the data table and filter the ANOVA results based on a p -value cutoff value. Figure 18. ANOVA Test results Limma t-test Limma t-test helps to identify significant differentially expressed lipid classes/species between two cohorts. The cohorts to be used can be selected from the drop down menu's labeled Cohort A and Cohort B . Click on Run Limma to perform differential expression and display the results in both volcano plot and tabular form. Figure 19. Limma t-test results The p -value and log 2 fold change cutoff parameters can be changed to alter the volcano plot generated after clicking on Filter Limma Results for Volcano Plot as shown in Figure 16. Figure 20. Volcano Plot Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 21. Filtered differential expression table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 22. Differential Expression Table Heatmap A heatmap is a graphical representation of data where the individual values contained in a matrix are represented on a color scale. Click on RESET HEATMAP to generate the heatmap. Here the Heatmap will be based on the data selected in the previous tab (visualization summary). Figure 23. Heatmap Interface It would represent the selected data type corresponding to the number of samples in the data. The filter icon can be used to filter out the desired samples and lipids. Once the filter is applied, the heatmap containing the selected lipids and sample would be generated. Figure 24. HeatMap Quality Checks Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features. High-dimensional data are very common in biology and arise when multiple features, such as expression of many genes, are measured for each sample. PCA is an unsupervised learning method similar to clustering wherein it finds patterns without reference to prior knowledge about whether the samples come from different treatment groups or have phenotypic differences. PCA reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs. The first PC is chosen to minimize the total distance between the data and their projection onto the PC. The second (and subsequent) PCs are selected similarly, with the additional requirement that they be uncorrelated with all previous PCs. The PCA Plot interface allows visualizing PC1 to PC11 using the drop-down menu's labeled PC on x axis and PC on y axis . Select the PCs to visualize and click on Run PCA to generate the PCA plot. PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 25. PCA (2D) Plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 26. PCA (3D) Plot","title":"Lipidomics Visualization Dashboard"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#overview","text":"Analysis of lipidomic data involves complex lipidome analysis. Lipidomics analysis across large sample sizes produces large datasets that require dedicated lipid identification, quantification and lipidome visualization. The wide diversity of native lipids and their modifications make their analyses challenging. Lipidomics Visualization Dashboard allows you to visualize, process and analyze the concentration data based on lipid species or classes along with sum of carbons and saturation type. The current version can calculate summary statistics, percentages and perform univariate analysis like ANOVA, Limma t-test along with Multivariate analysis like PCA. It helps you view lipids by their main class, sub class across cohorts.","title":"Overview"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#scope-of-the-app","text":"Performs preprocessing on input data. Calculates lipid species concentration percentage for each lipid class and species across multiple cohorts. Plots the lipid class and species concentrations and their percentages along with sum of carbons and saturation type in the form of pie charts and bar plots. Performs PCA for quality check. Performs differential expression using ANOVA or Limma t-test to find significant lipid species or class. Analyze clustering profiles through a heatmap. Figure 1. Lipidomics Visualization Dashboard","title":"Scope of the app"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#user-input","text":"Lipidomics analysis requires the following two files as input: Concentration File The concentration file is the data output from Lipidizer. The file should be in .xlsx format and must contain Lipids/ Fatty Acid concentrations sheet. Figure 2. Concentration file Metadata File The metadata file contains two columns, Name which contains all the samples used under study and Cohort which contains the cohort information for all the samples. Figure 3. Metadata file Species Metadata File The species metadata file is optional. It is helpful to classify the lipid species the way you desire. It contains three main columns: Species , Class which contains all the lipid species, and class used understudy and Chain which contains the fatty acid chain information for each lipid species. Figure 4. Species Metadata file","title":"User Input"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#steps-involved-in-data-processing","text":"Upload input files Perform preprocessing of data Visualize input data based on lipid class or species, fatty acids or sum of carbons Perform quality checks on your data Perform differential expression using ANOVA or Limma t-test Visualize data in the form of a heatmap","title":"Steps involved in data processing"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#caveats","text":"Lipidomics Visualization Dashboard only works with the output of SCIEX Lipidyzer TM Platform.","title":"Caveats"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#upload-files","text":"Select Lipidomics Visulalization Dashboard from the dashboard under the Lipidomics Data Tab. Figure 5. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the session to be redirected to Lipidomics Visualization Dashboard's upload page. Figure 6. Workspace Selection","title":"Upload Files"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#upload-interface","text":"Upload the concentration and metadata file in the upload space. You can chose to add species metadata file as well by selecting the checkbox. Once all the files are uploaded, click on Go . You can preview the uploaded raw data, metadata and species metadata. At the top, example files are available for download. You can either upload these example files or click on Run Example to automatically load the example data. Figure 7. Upload interface","title":"Upload Interface"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#preprocessing-interface","text":"The Preprocessing interface allows you to perform a multitude of functions on the data such as: Figure 8. Preprocessing Interface Drop Lipids This allows you to drop/remove unwanted lipids from further analysis. You can choose to drop lipids by their Species or Class . Select the lipid(s) from the drop-down menu and click on Drop Lipids to remove the selected lipids on the fly. Figure 9. Drop Lipids Drop Samples This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 10 after selecting the sample(s) from the drop-down menu. Figure 10. Drop Samples","title":"Preprocessing Interface"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#visualization-interface","text":"This interface contains two tabs which allows you to visualize data for concentrations across multiple cohorts using pie charts and bar plots. You can visualize summary of each lipid class as well as each lipid species.","title":"Visualization Interface"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#pie-charts","text":"This interface allows you to visualize a summary of each Lipid species Fatty acids: Data is summarized by an individual fatty acid chain. For example \u201cDAG(16:0/16:1)\u201d fatty acid will be \u201cDAG(FA16:0)\u201d and \u201cDAG(FA16:1)\u201d Sum of Carbons: Data is summarized by the total number of carbons within the lipid species. For example for \u201cDAG(16:0/16:1)\u201d the sum of carbons will be \u201cDAG(32:1)\u201d. Select Data Type provides you the option to choose either of the above-mentioned options from the input excel file. Select Cohort allows you to select cohorts from the list of all cohorts for which the pie chart will be formed. You can select multiple cohorts for cross-cohort comparisons. Figure 11. Pie Charts Interface Saturation Type Summary Plot This plot allows you to view pie charts for selected cohorts, each representing portions of the saturation type within them. Figure 12. Saturation Type Summary Plot Select Lipid Class allows you to select the lipid class for which they can view the summary of lipid sub-class concentration percentages. Select Cohort allows you to select cohorts from the list of all cohorts for which the pie chart will be formed. You can select multiple cohorts to compare concentrations across cohorts a shown in Figure 9. Figure 13. Pie Charts Interface showing Lipid Species Summary","title":"Pie-Charts"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#concentration-plots","text":"These are the bar plots for the given concentration where the bars would be created by taking the mean value of samples within the cohort. It allows you to select the data type which includes lipid species, fatty acids, and sum of carbons from the input file. You can further select the lipid type from the Lipid Species, Lipid Class, and Saturation Type Figure 14. Concentration plot interface Filter lipids using the the drop down as showed in Figure 15. Multiple lipids can be selected simultaneously. Figure 15. Filtering of lipids Note: Data selected by you will be taken as input for all the statistical analysis performed in other tabs. Bar plot is based on the mean concentration values across each cohort. Filter lipid class/species to visualize mean concentration values for those lipid classes/species only as shown in Figure 16. Figure 16. Concentration plot of filtered lipids Click on Table tab to visualize the respective data-table for individual sample concentration as shown in Figure 17. Figure 17. Data table","title":"Concentration Plots"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#statistical-analysis","text":"","title":"Statistical Analysis"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#anova-test","text":"ANOVA Test is used to identify the most significant lipid species/lipid classes. Here the results will be calculated based on the data selected in the previous tab (visualization summary) and results are displayed in a tabular format without pressing any button. You can sort any column from the data table and filter the ANOVA results based on a p -value cutoff value. Figure 18. ANOVA Test results","title":"ANOVA Test"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#limma-t-test","text":"Limma t-test helps to identify significant differentially expressed lipid classes/species between two cohorts. The cohorts to be used can be selected from the drop down menu's labeled Cohort A and Cohort B . Click on Run Limma to perform differential expression and display the results in both volcano plot and tabular form. Figure 19. Limma t-test results The p -value and log 2 fold change cutoff parameters can be changed to alter the volcano plot generated after clicking on Filter Limma Results for Volcano Plot as shown in Figure 16. Figure 20. Volcano Plot Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 21. Filtered differential expression table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 22. Differential Expression Table","title":"Limma t-test"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#heatmap","text":"A heatmap is a graphical representation of data where the individual values contained in a matrix are represented on a color scale. Click on RESET HEATMAP to generate the heatmap. Here the Heatmap will be based on the data selected in the previous tab (visualization summary). Figure 23. Heatmap Interface It would represent the selected data type corresponding to the number of samples in the data. The filter icon can be used to filter out the desired samples and lipids. Once the filter is applied, the heatmap containing the selected lipids and sample would be generated. Figure 24. HeatMap","title":"Heatmap"},{"location":"Apps/Lipidomics Data/Lipidomics Visualization Dashboard.html#quality-checks","text":"Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features. High-dimensional data are very common in biology and arise when multiple features, such as expression of many genes, are measured for each sample. PCA is an unsupervised learning method similar to clustering wherein it finds patterns without reference to prior knowledge about whether the samples come from different treatment groups or have phenotypic differences. PCA reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs. The first PC is chosen to minimize the total distance between the data and their projection onto the PC. The second (and subsequent) PCs are selected similarly, with the additional requirement that they be uncorrelated with all previous PCs. The PCA Plot interface allows visualizing PC1 to PC11 using the drop-down menu's labeled PC on x axis and PC on y axis . Select the PCs to visualize and click on Run PCA to generate the PCA plot. PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 25. PCA (2D) Plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 26. PCA (3D) Plot","title":"Quality Checks"},{"location":"Apps/Metabolomic Data/CompoundDiscoverer QuantFit.html","text":"Overview We have used Thermo Scientific TM Compound Discoverer TM to set up a node for peak picking on raw data obtained from a thermo mass spectrometer. The absolute quantification is then derived for this data with the Polly QuantFit Node. The following tools were used in the process: Thermo Scientific TM Compound Discoverer TM to set up a node for peak integration, and quantification MSConvert for raw data conversion to an El-MAVEN accepted format El-MAVEN to identify and automatically pick the significant peaks in the data Polly QuantFit used for absolute quantification of data Pre Requisites You should have Thermo Scientific TM Compound Discoverer TM version 3.1 to be able to use this Node Thermo Scientific TM Compound Discoverer TM should not be running in the background when you are installing the Polly QuantFit Node The path for MSConvert installation directory is C:/Users/ UserName /AppData/Local/Apps/Proteowizard/msconvert.exe You should have an account on Polly User Guide Steps to set up a node in the Thermo Scientific TM Compound Discoverer TM : Install the QuantFit Node from the installer here . Thermo Scientific TM Compound Discoverer TM Open Thermo Scientific TM Compound Discoverer TM 3.1. Select New Study and Analysis On Begin a new Study page, Click Next Add a Study Name , Studies Folder and click Next Click on Add files . Browse to the folder with all the .raw files and select them. Click Finish . Go to Workflows In Workflow Tree drag and drop Input Files from Input/Output within Workflow Nodes In Post Processing Nodes drag and drop Polly QuantFit from Elucidata Nodes within Workflow Nodes Click on Polly QuantFit in Post Processing Nodes The values corresponding to the parameters for conversion and filtering of data have to be entered within this node MSConvert Parameters MSConvert Installation Directory: It is likely to be at a location similar to C:/Users/ UserName /AppData/Local/Apps/Proteowizard/msconvert.exe Threshold: A parameter defined in MSConvert to remove noise from the data. Zlib Compression: The converted files can be compressed to allow processing a larger number of files. El-MAVEN Parameters Compound Database: It has the compound name, formula and retention time information. An example file is attached here . Accuracy: The resolution of the data defined in parts per million (ppm). Isotope Detection: You can choose to do a Labeled or Unlabeled peak detection. You can select the isotopes for which you would want to detect the peaks. Match Retention Time: You can choose to identify peaks for a metabolite at a particular retention time if you have defined the retention time in your compound database. If retention time is not present in your compound database, then please select \"No\" here. Retention Time Window: You can define a time range (in minutes) for the retention time in which you would want to detect a peak for a metabolite. E.g The retention time for nicotinamide is defined as 1.495 min in the compound database. If you selected match retention time as Yes and defined the retention time window as 1 min then peaks for nicotinamide would be detected in the time range 0.495 - 2.495 min . Polly QuantFit Polly Username: Enter the email id registered on Polly. Password: Enter the password for this account. Polly Workspace Name: Enter a new/existing Workspace name for your workspaces on Polly. Click on Run within Analysis The processing starts. On completion, you will receive an email on the Polly Email Address used in QuantFit Node. Open the email and click on Start Quantification . Click here for a detailed documentation about Polly QuantFit. QuantFit Metadata Interface You will be redirected to QuantFit Metadata Interface on Polly Upload the metadata or create one for your dataset. Watch the demo video below for more information. Ion Efficiency Correction Interface This is a normalization method with respect to internal standards in the data. The concentration of biosamples is divided by the concentration of internal standards in the data here. You can also skip this step if you do not have internal standards. Quantification Dashboard Calibration Curve: It gives you the fitted curve for your data on a linear equation by default. The fit details are shown on the panel, right of the curve. Concentration Plot: It gives you the biosample concentration Different Fit types: You can also change the fit type from linear to polynomial , exponential , log-log or power equation . Removing Standards from data: You can remove standards from the data to improve the goodness of the fit. Suggested Fits: QuantFit also suggests good fits for your data with other fit types and standard rejection that can be used in further analysis by selecting the suggested fit as a Default Fit . Scaling: Neutralize any internal dilution by scaling the data by any factor within the Processed Data tab. Data Export to Thermo Scientific TM Compound Discoverer TM Click on Export on the QuantFit dashboard to export this data back to Thermo Scientific TM Compound Discoverer TM . Go back to your analysis on Thermo Scientific TM Compound Discoverer TM and visualize your data by clicking on Open Results within Job Queue . This data can be used in further analysis.","title":"Compound Discoverer<sup>TM</sup> QuantFit"},{"location":"Apps/Metabolomic Data/CompoundDiscoverer QuantFit.html#overview","text":"We have used Thermo Scientific TM Compound Discoverer TM to set up a node for peak picking on raw data obtained from a thermo mass spectrometer. The absolute quantification is then derived for this data with the Polly QuantFit Node. The following tools were used in the process: Thermo Scientific TM Compound Discoverer TM to set up a node for peak integration, and quantification MSConvert for raw data conversion to an El-MAVEN accepted format El-MAVEN to identify and automatically pick the significant peaks in the data Polly QuantFit used for absolute quantification of data","title":"Overview"},{"location":"Apps/Metabolomic Data/CompoundDiscoverer QuantFit.html#pre-requisites","text":"You should have Thermo Scientific TM Compound Discoverer TM version 3.1 to be able to use this Node Thermo Scientific TM Compound Discoverer TM should not be running in the background when you are installing the Polly QuantFit Node The path for MSConvert installation directory is C:/Users/ UserName /AppData/Local/Apps/Proteowizard/msconvert.exe You should have an account on Polly","title":"Pre Requisites"},{"location":"Apps/Metabolomic Data/CompoundDiscoverer QuantFit.html#user-guide","text":"Steps to set up a node in the Thermo Scientific TM Compound Discoverer TM : Install the QuantFit Node from the installer here . Thermo Scientific TM Compound Discoverer TM Open Thermo Scientific TM Compound Discoverer TM 3.1. Select New Study and Analysis On Begin a new Study page, Click Next Add a Study Name , Studies Folder and click Next Click on Add files . Browse to the folder with all the .raw files and select them. Click Finish . Go to Workflows In Workflow Tree drag and drop Input Files from Input/Output within Workflow Nodes In Post Processing Nodes drag and drop Polly QuantFit from Elucidata Nodes within Workflow Nodes Click on Polly QuantFit in Post Processing Nodes The values corresponding to the parameters for conversion and filtering of data have to be entered within this node MSConvert Parameters MSConvert Installation Directory: It is likely to be at a location similar to C:/Users/ UserName /AppData/Local/Apps/Proteowizard/msconvert.exe Threshold: A parameter defined in MSConvert to remove noise from the data. Zlib Compression: The converted files can be compressed to allow processing a larger number of files. El-MAVEN Parameters Compound Database: It has the compound name, formula and retention time information. An example file is attached here . Accuracy: The resolution of the data defined in parts per million (ppm). Isotope Detection: You can choose to do a Labeled or Unlabeled peak detection. You can select the isotopes for which you would want to detect the peaks. Match Retention Time: You can choose to identify peaks for a metabolite at a particular retention time if you have defined the retention time in your compound database. If retention time is not present in your compound database, then please select \"No\" here. Retention Time Window: You can define a time range (in minutes) for the retention time in which you would want to detect a peak for a metabolite. E.g The retention time for nicotinamide is defined as 1.495 min in the compound database. If you selected match retention time as Yes and defined the retention time window as 1 min then peaks for nicotinamide would be detected in the time range 0.495 - 2.495 min . Polly QuantFit Polly Username: Enter the email id registered on Polly. Password: Enter the password for this account. Polly Workspace Name: Enter a new/existing Workspace name for your workspaces on Polly. Click on Run within Analysis The processing starts. On completion, you will receive an email on the Polly Email Address used in QuantFit Node. Open the email and click on Start Quantification . Click here for a detailed documentation about Polly QuantFit. QuantFit Metadata Interface You will be redirected to QuantFit Metadata Interface on Polly Upload the metadata or create one for your dataset. Watch the demo video below for more information. Ion Efficiency Correction Interface This is a normalization method with respect to internal standards in the data. The concentration of biosamples is divided by the concentration of internal standards in the data here. You can also skip this step if you do not have internal standards. Quantification Dashboard Calibration Curve: It gives you the fitted curve for your data on a linear equation by default. The fit details are shown on the panel, right of the curve. Concentration Plot: It gives you the biosample concentration Different Fit types: You can also change the fit type from linear to polynomial , exponential , log-log or power equation . Removing Standards from data: You can remove standards from the data to improve the goodness of the fit. Suggested Fits: QuantFit also suggests good fits for your data with other fit types and standard rejection that can be used in further analysis by selecting the suggested fit as a Default Fit . Scaling: Neutralize any internal dilution by scaling the data by any factor within the Processed Data tab. Data Export to Thermo Scientific TM Compound Discoverer TM Click on Export on the QuantFit dashboard to export this data back to Thermo Scientific TM Compound Discoverer TM . Go back to your analysis on Thermo Scientific TM Compound Discoverer TM and visualize your data by clicking on Open Results within Job Queue . This data can be used in further analysis.","title":"User Guide"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html","text":"Introduction Overview The Dual Mode Data Visualization (Metabolomics App) allows you to perform downstream analysis on single mode (either positive or negative mode) as well as dual mode (both positive and negative mode) targeted, semi-targeted (without retention time) and untargeted unlabeled metabolomics data along with insightful visualizations. The app provides a variety of normalization methods, scaling options and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights. Scope of the App The application supports data with a simple matrix having samples in the columns and metabolites in the rows. It provides different normalization and scaling methods to perform on the data. Performs quality checks for internal standards, metabolites, and samples. Performs statistical analysis using limma and provides interactive visualizations. Provides heatmap visualization along with different algorithms like hierarchical clustering, k-means, correlation etc. Performs comparative analysis for the different cohort comparisons. Figure 1. Dual Mode Data Visualization Getting Started User Input To process single mode data, the following files are required: El-MAVEN Output File Figure 2. El-MAVEN raw intensity file obtained using a compound database El-MAVEN Internal Standards File (Optional) Figure 3. El-MAVEN raw intensity file of internal standards Cohort Mapping File Figure 4. Cohort file To process dual mode data, the following files are required: El-MAVEN Output Files from both positive and negative mode Figure 5. El-MAVEN raw intensity files for positive and negative modes El-MAVEN Internal Standards File from both, positive and negative modes (Optional) Figure 6. El-MAVEN raw intensity files for positive and negative modes of internal standards only Cohort Mapping File Figure 7. Sample-Cohort metadata file/center> NOTE: An already processed .gct file can also serve as input to the app. The internal standard file is optional. Caveats Pathways analysis only works when the data has KEGG Ids within the \u201ccompoundId\u201d column. Tutorial Select Dual Mode Data Visualization (Metabolomics App) from the dashboard under the Metabolomics Data Tab as shown in Figure 8. Create a New Workspace or choose from the existing one from the dop-down and provide the Name of the Session to be redirected to Dual Mode Data Visualisation (Metabolomics App)'s upload page. Figure 8. Polly Dashboard and Workspace selection Upload Files The Upload Files interface allows you to upload the input files required for processing through the app which includes the group summary matrix files from El-MAVEN and metadata file, or a .gct file. Figure 9. Upload Files Tab Download all sample files: This option would allow you to download the demo files which include the El-MAVEN output in group summary matrix format and cohort mapping file. Upload GCT file: Checking this box prompts the app that the input data is in the .gct format. Figure 10. Upload Files Tab: Upload gct file option is checked Select mode: This drop-down allows you to select the specific mode of the data in case of single mode, meaning whether it is: Positive, or Negative Figure 11. Upload Files Tab for single-mode data: default view To upload the output file of internal standards, click on Upload internal standards file . Figure 12. Upload Files Tab for single-mode data: Upload internal standards file option is checked El-MAVEN positive/negative mode file: This allows you to upload the positive/negative mode El-MAVEN output file (depending on the mode selected) in the .csv peak table format. Internal Standards positive/negative mode file: This allows you to upload the positive/negative mode internal standards El-MAVEN output file (depending on the mode selected) in the .csv peak table format. Metadata file: This allows you to upload the cohort mapping file in the .csv format. Data is dual mode: Checking this box prompts the app that the input data is from dual-mode (positive and negative modes). Figure 13. Upload Files interface for dual-mode data To upload the output file of internal standards, click on Upload internal standards file . Figure 14. Upload Files Tab for dual-mode data: Upload internal standards file option is checked El-MAVEN negative mode file: This allows you to upload the negative mode El-MAVEN output file in the .csv peak table format. El-MAVEN positive mode file: This allows you to upload the positive mode El-MAVEN output file in the .csv peak table format. Internal Standards negative mode file (optional): This allows you to upload the negative mode internal standards El-MAVEN output file in the .csv peak table format. Internal Standards positive mode file (optional): This allows you to upload the positive mode internal standards El-MAVEN output file in the .csv peak table format. Metadata file: This allows you to upload the cohort mapping file in the .csv format. Click on Go to proceed to the next step. Note: The format of the metadata file for dual-mode should be in a specific format. To make common sample names across the different modes, click on Replace samples to common sample names . Figure 15. Make common sample names: Replace samples to common sample names option is checked Pre-processing The Pre-processing interface allows you to perform a multitude of functions on the data such as: Figure 16. Preprocessing Tab Select Internal Standards: This allows you to select the internal standard(s) from within the El-MAVEN output file when a separate internal standards file is not provided as input. Note: In case, the internal standard(s) are not in the El-MAVEN output file but in the separate internal standards file, they will not show up in the drop down menu. To select the desired internal standards, select them in Normalize by individual internal standards option under Normalize by Internal standards in the Perform Normalization > Normalization . Figure 17. Selecting internal standards from the data Drop Samples: This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during MS processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 18 after selecting the sample(s) from the drop down menu. Figure 18. Drop Samples option Normalize by Internal standards performs normalization using the internal standards. Figure 19. Normalize by internal standards options Normalize by sum of internal standards normalizes by the sum of the standards provided. Normalize by average of internal standards normalizes by the average of the standards provided. Normalize by individual internal standards normalizes by the internal standards selected previously. Normalize by metabolites normalizes by any particular metabolite selected. Figure 20. Normalize by metabolites option Normalize by sum of metabolites normalizes by the sum of metabolites. Here, the user can select the metabolites from the dropdown option. Normalize by metadata column normalizes by any additional column specified in the metadata file. such as cell number etc. Normalize by control normalizes by control samples present in the data. Figure 21. Normalize by metadata options log2 y + log2(x) [where data is shifted by max value of data plus one] Figure 22. Scaling options Note: If internal standard(s) have already been selected in the Select Internal Standards option, they would be present in the drop down. Clicking on Run will perform the normalization and scaling based on the parameters selected. Table: This displays the data table and visualizations for both pre- and post- normalization. Metadata: This displays the metadata uploaded. This data can be downloaded in the .csv format as shown in Figure 23. Metabolite Mapping data: This displays the metabolite data uploaded. This data can be downloaded in the .csv format as shown in Figure 24. Raw data: This displays the raw El-MAVEN data uploaded. This data can be downloaded in the .gct format as shown in Figure 25. Processed data: This displays the normalized El-MAVEN data based on the parameters selected. This data can be downloaded in .gct format as shown in Figure 26. Pre-Processing Results: This allows you to have a look at the sample distribution with the help of density plot and box-plot before normalization as shown in Figure 27. Post-Processing Results: This allows you to have a look at the sample distribution with the help of the density plot and box-plot after normalization. This provides you with the ability to check the effect of the normalization parameters on the data as shown in Figure 28. Figure 23. Metadata table Figure 24. Metabolite Mapping data table Figure 25. Raw data table Figure 26. Processed data table Figure 27. Pre-Processing Results Figure 28. Post-Processing Results Quality Checks This tab allows you to perform quality checks for the internal standards, metabolites and across samples with the help of interactive visualizations. Internal Standards It allows you to have a look at the quality of the internal standards used in the data with the help of the different visualizations for any individual as well as for all internal standards. Internal Standards (Individual): You can visualize the quality checks for any internal standard specifically. This allows you to select the internal standard by name, followed by another drop down to select by uniqueId of the feature. It\u2019s also possible to specify the cohort order for the plots. For dual mode data, you can specify the internal standard of the particular mode from the Select uniqueIds drop down. Figure 29. Internal Standards (Individual) options Figure 30. CV Distribution across cohorts Figure 31. CV Distribution across samples Metabolites: It allows you to have a look at the quality of the metabolites present in the data with the help of the Coefficient of Variation plots Metabolites CoV Boxplot visualizes the Coefficient of Variation across different cohorts in the data in the form of the boxplot. It\u2019s also possible to specify the cohort order for the plots as shown in Figure 32. Metabolites CoV Barplot visualizes the Coefficient of Variation as a quality check for any specific metabolite. To use this, select the metabolite followed by the unique id of the feature using the drop downs shown in Figure 33. It\u2019s also possible to specify the cohort order for the plots as shown in FIgure 33. Figure 32. Metabolites CoV Boxplot option and CV Distribution across Cohorts boxplot Figure 33. Metabolites CoV Barplot Figure 34. CV Distribution across cohorts for selected metabolite Figure 35. CV Distribution across samples for the selected metabolite PCA This allows you to understand the clustering pattern between biologically grouped and ungrouped samples. PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 36. Two-dimensional PCA plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 37. Three-dimensional PCA plot Statistical Analysis This interface allows you to perform differential expression analysis with the aim to identify metabolites whose expression differs between any specified cohort conditions. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the two experimental conditions and an 'adjusted' p -value that rates the significance of the difference. Figure 38. Statistical Analysis interface The following parameters are available for selection: Select Cohort A and Cohort B: Default values are filled automatically for a selected cohort condition, which can be changed as per the cohorts of interest. Select p-val or adj. p-val : Select either p -value or adj. p -value for significance. p-val or adj. p-val cutoff: By default, the value is 0.05 but can be changed if required. log2FC: Specify the cut-off for log 2 fold change with the help of the slider. Once the parameters are specified, click on the Update button to plot the volcano plot. Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 39. Volcano plot Filtered Metabolites Visualization provides the visualization of cohort-based distribution of the metabolites that are significant based on the parameters specified. Figure 40. Filtered Metabolites Visualization Filtered Normalized Table contains the normalized data of the metabolites that are significant based on the parameters specified. Figure 41. Filtered Normalized Table Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 42. Filtered Differential Expression Table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 43. Differential Expression Table Pathway Enrichment Analysis performs the pathway enrichment analysis for the significant metabolites based on the parameters specified for the particular cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 44. Metabolite Set Enrichment Analysis Plot Figure 45. Pathway Topology Analysis Plot Figure 46. Metabolite Set Enrichment Analysis Table Figure 47. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolites they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolism selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 48. Pathway View Plot Visualization This interface allows you to visualize the cohort-based distribution of a specific metabolite or a group of metabolites on the basis on its normalized intensity values. Figure 49. Visualization tab options Enter metabolite: Select the metabolite(s) of interest from the drop down option. Select uniqueIds: You can specifically select the metabolic feature of interest for the metabolite from the drop down option. Select order of cohort: You can also specify the particular order of the cohort to visualize the bar plot. Once the parameters are selected, click on Load Plots to plot the bar plot for the metabolite. Figure 50. Cohort-wise bar plot with the normalized intensity of selected metabolite IntOmix Input This tab allows you to generate the input for IntOmix where you can visualize the significantly altered metabolic network modules between any two experimental conditions. Figure 51. IntOmix Input Tab options Specify two or more cohorts from the Select cohorts drop down option for which you want to generate the IntOmix input. Once the required cohorts are selected, click on Generate to generate the IntOmix input. Figure 52. IntOmix Table for the cohort conditions specified NOTE: At least two cohorts are required to create the input file. Heatmap This tab allows you to produce a heatmap of the processed data, so that you can observe the level of expression in a visual form. Click on Load Heatmap button to generate the heatmap. Figure 53. Heatmap Comparative Analysis This tab allows you to perform comparative analysis between a set of cohorts in your data. As a result of which you can visualize the UpSet plot of the unique and overlapping metabolites for the selected cohort comparisons. Further, you can also perform pathway analysis on the metabolites for the set intersections of interest. Comparison Parameters tab allows you to select the cohorts of interest for which you would want to get the set intersections. You can select the cohorts from the Select cohorts drop-down and click on Run button. Further, you can also specify the p -value cut-off and log 2 FC threshold. Figure 54. Comparison Parameters Tab options You will get a table as a result of the parameters specified which will have the significant metabolites for the different cohort comparisons along with their corresponding p -values and log 2 FC values. You can also download this table as a .CSV file. Figure 55. Comparison Table UpSet Plot tab allows you to visualize the set intersections for the cohort comparisons selected where every comparison consists of the significant metabolites associated with the same. You can select the cohort comparisons of interest from the Select Cohort Comparison drop-down which represents all the possible comparisons for the cohorts specified in the previous tab. Click on Plot to get the UpSet plot for the specified comparisons. Figure 56. UpSet Plot options Figure 57. UpSet Plot Along with the plot, you can also get all the constituent metabolites for the respective comparisons in a tabular format that can be downloaded as a .CSV file. Figure 58. UpSet Plot Table Pathway Enrichment Analysis tab allows you to perform the pathway enrichment analysis for the significant metabolites that show up based on the parameters specified in the Comparison Parameters tab for the particular set of cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 59. Metabolite Set Enrichment Analysis Plot Figure 60. Pathway Topology Analysis Plot Figure 61. Metabolite Set Enrichment Analysis Table Figure 62. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolisms they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolite selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 63. Pathway View options Figure 64. Pathway View Plot Data for MetaboAnalyst This tab allows you to generate the input for MetaboAnalyst that enables statistical, functional and integrative analysis of metabolomics data by providing a variety of modules for different functionalities. Figure 65. Data for MetaboAnalyst Tab options Specify the cohort comparison of interest by sleecting the cohorts from the Select cohort A and Select cohort B drop downs. Click on Download data for metaboanalyst to download the .csv file. Clicking on Go to metaboanalyst will redirect you to MetaboAnalyst\u2019s homepage. NOTE: Normalized intensity data (normalization method chosen in pre-processing tab) is downloaded.","title":"Dual Mode Data Visualization"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#overview","text":"The Dual Mode Data Visualization (Metabolomics App) allows you to perform downstream analysis on single mode (either positive or negative mode) as well as dual mode (both positive and negative mode) targeted, semi-targeted (without retention time) and untargeted unlabeled metabolomics data along with insightful visualizations. The app provides a variety of normalization methods, scaling options and data visualization functionalities, thereby allowing an efficient analysis of the data to get actionable insights.","title":"Overview"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#scope-of-the-app","text":"The application supports data with a simple matrix having samples in the columns and metabolites in the rows. It provides different normalization and scaling methods to perform on the data. Performs quality checks for internal standards, metabolites, and samples. Performs statistical analysis using limma and provides interactive visualizations. Provides heatmap visualization along with different algorithms like hierarchical clustering, k-means, correlation etc. Performs comparative analysis for the different cohort comparisons. Figure 1. Dual Mode Data Visualization","title":"Scope of the App"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#user-input","text":"To process single mode data, the following files are required: El-MAVEN Output File Figure 2. El-MAVEN raw intensity file obtained using a compound database El-MAVEN Internal Standards File (Optional) Figure 3. El-MAVEN raw intensity file of internal standards Cohort Mapping File Figure 4. Cohort file To process dual mode data, the following files are required: El-MAVEN Output Files from both positive and negative mode Figure 5. El-MAVEN raw intensity files for positive and negative modes El-MAVEN Internal Standards File from both, positive and negative modes (Optional) Figure 6. El-MAVEN raw intensity files for positive and negative modes of internal standards only Cohort Mapping File Figure 7. Sample-Cohort metadata file/center> NOTE: An already processed .gct file can also serve as input to the app. The internal standard file is optional.","title":"User Input"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#caveats","text":"Pathways analysis only works when the data has KEGG Ids within the \u201ccompoundId\u201d column.","title":"Caveats"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#tutorial","text":"Select Dual Mode Data Visualization (Metabolomics App) from the dashboard under the Metabolomics Data Tab as shown in Figure 8. Create a New Workspace or choose from the existing one from the dop-down and provide the Name of the Session to be redirected to Dual Mode Data Visualisation (Metabolomics App)'s upload page. Figure 8. Polly Dashboard and Workspace selection","title":"Tutorial"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#upload-files","text":"The Upload Files interface allows you to upload the input files required for processing through the app which includes the group summary matrix files from El-MAVEN and metadata file, or a .gct file. Figure 9. Upload Files Tab Download all sample files: This option would allow you to download the demo files which include the El-MAVEN output in group summary matrix format and cohort mapping file. Upload GCT file: Checking this box prompts the app that the input data is in the .gct format. Figure 10. Upload Files Tab: Upload gct file option is checked Select mode: This drop-down allows you to select the specific mode of the data in case of single mode, meaning whether it is: Positive, or Negative Figure 11. Upload Files Tab for single-mode data: default view To upload the output file of internal standards, click on Upload internal standards file . Figure 12. Upload Files Tab for single-mode data: Upload internal standards file option is checked El-MAVEN positive/negative mode file: This allows you to upload the positive/negative mode El-MAVEN output file (depending on the mode selected) in the .csv peak table format. Internal Standards positive/negative mode file: This allows you to upload the positive/negative mode internal standards El-MAVEN output file (depending on the mode selected) in the .csv peak table format. Metadata file: This allows you to upload the cohort mapping file in the .csv format. Data is dual mode: Checking this box prompts the app that the input data is from dual-mode (positive and negative modes). Figure 13. Upload Files interface for dual-mode data To upload the output file of internal standards, click on Upload internal standards file . Figure 14. Upload Files Tab for dual-mode data: Upload internal standards file option is checked El-MAVEN negative mode file: This allows you to upload the negative mode El-MAVEN output file in the .csv peak table format. El-MAVEN positive mode file: This allows you to upload the positive mode El-MAVEN output file in the .csv peak table format. Internal Standards negative mode file (optional): This allows you to upload the negative mode internal standards El-MAVEN output file in the .csv peak table format. Internal Standards positive mode file (optional): This allows you to upload the positive mode internal standards El-MAVEN output file in the .csv peak table format. Metadata file: This allows you to upload the cohort mapping file in the .csv format. Click on Go to proceed to the next step. Note: The format of the metadata file for dual-mode should be in a specific format. To make common sample names across the different modes, click on Replace samples to common sample names . Figure 15. Make common sample names: Replace samples to common sample names option is checked","title":"Upload Files"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#pre-processing","text":"The Pre-processing interface allows you to perform a multitude of functions on the data such as: Figure 16. Preprocessing Tab Select Internal Standards: This allows you to select the internal standard(s) from within the El-MAVEN output file when a separate internal standards file is not provided as input. Note: In case, the internal standard(s) are not in the El-MAVEN output file but in the separate internal standards file, they will not show up in the drop down menu. To select the desired internal standards, select them in Normalize by individual internal standards option under Normalize by Internal standards in the Perform Normalization > Normalization . Figure 17. Selecting internal standards from the data Drop Samples: This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during MS processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 18 after selecting the sample(s) from the drop down menu. Figure 18. Drop Samples option Normalize by Internal standards performs normalization using the internal standards. Figure 19. Normalize by internal standards options Normalize by sum of internal standards normalizes by the sum of the standards provided. Normalize by average of internal standards normalizes by the average of the standards provided. Normalize by individual internal standards normalizes by the internal standards selected previously. Normalize by metabolites normalizes by any particular metabolite selected. Figure 20. Normalize by metabolites option Normalize by sum of metabolites normalizes by the sum of metabolites. Here, the user can select the metabolites from the dropdown option. Normalize by metadata column normalizes by any additional column specified in the metadata file. such as cell number etc. Normalize by control normalizes by control samples present in the data. Figure 21. Normalize by metadata options log2 y + log2(x) [where data is shifted by max value of data plus one] Figure 22. Scaling options Note: If internal standard(s) have already been selected in the Select Internal Standards option, they would be present in the drop down. Clicking on Run will perform the normalization and scaling based on the parameters selected. Table: This displays the data table and visualizations for both pre- and post- normalization. Metadata: This displays the metadata uploaded. This data can be downloaded in the .csv format as shown in Figure 23. Metabolite Mapping data: This displays the metabolite data uploaded. This data can be downloaded in the .csv format as shown in Figure 24. Raw data: This displays the raw El-MAVEN data uploaded. This data can be downloaded in the .gct format as shown in Figure 25. Processed data: This displays the normalized El-MAVEN data based on the parameters selected. This data can be downloaded in .gct format as shown in Figure 26. Pre-Processing Results: This allows you to have a look at the sample distribution with the help of density plot and box-plot before normalization as shown in Figure 27. Post-Processing Results: This allows you to have a look at the sample distribution with the help of the density plot and box-plot after normalization. This provides you with the ability to check the effect of the normalization parameters on the data as shown in Figure 28. Figure 23. Metadata table Figure 24. Metabolite Mapping data table Figure 25. Raw data table Figure 26. Processed data table Figure 27. Pre-Processing Results Figure 28. Post-Processing Results","title":"Pre-processing"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#quality-checks","text":"This tab allows you to perform quality checks for the internal standards, metabolites and across samples with the help of interactive visualizations. Internal Standards It allows you to have a look at the quality of the internal standards used in the data with the help of the different visualizations for any individual as well as for all internal standards. Internal Standards (Individual): You can visualize the quality checks for any internal standard specifically. This allows you to select the internal standard by name, followed by another drop down to select by uniqueId of the feature. It\u2019s also possible to specify the cohort order for the plots. For dual mode data, you can specify the internal standard of the particular mode from the Select uniqueIds drop down. Figure 29. Internal Standards (Individual) options Figure 30. CV Distribution across cohorts Figure 31. CV Distribution across samples Metabolites: It allows you to have a look at the quality of the metabolites present in the data with the help of the Coefficient of Variation plots Metabolites CoV Boxplot visualizes the Coefficient of Variation across different cohorts in the data in the form of the boxplot. It\u2019s also possible to specify the cohort order for the plots as shown in Figure 32. Metabolites CoV Barplot visualizes the Coefficient of Variation as a quality check for any specific metabolite. To use this, select the metabolite followed by the unique id of the feature using the drop downs shown in Figure 33. It\u2019s also possible to specify the cohort order for the plots as shown in FIgure 33. Figure 32. Metabolites CoV Boxplot option and CV Distribution across Cohorts boxplot Figure 33. Metabolites CoV Barplot Figure 34. CV Distribution across cohorts for selected metabolite Figure 35. CV Distribution across samples for the selected metabolite PCA This allows you to understand the clustering pattern between biologically grouped and ungrouped samples. PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 36. Two-dimensional PCA plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 37. Three-dimensional PCA plot","title":"Quality Checks"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#statistical-analysis","text":"This interface allows you to perform differential expression analysis with the aim to identify metabolites whose expression differs between any specified cohort conditions. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the two experimental conditions and an 'adjusted' p -value that rates the significance of the difference. Figure 38. Statistical Analysis interface The following parameters are available for selection: Select Cohort A and Cohort B: Default values are filled automatically for a selected cohort condition, which can be changed as per the cohorts of interest. Select p-val or adj. p-val : Select either p -value or adj. p -value for significance. p-val or adj. p-val cutoff: By default, the value is 0.05 but can be changed if required. log2FC: Specify the cut-off for log 2 fold change with the help of the slider. Once the parameters are specified, click on the Update button to plot the volcano plot. Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 39. Volcano plot Filtered Metabolites Visualization provides the visualization of cohort-based distribution of the metabolites that are significant based on the parameters specified. Figure 40. Filtered Metabolites Visualization Filtered Normalized Table contains the normalized data of the metabolites that are significant based on the parameters specified. Figure 41. Filtered Normalized Table Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 42. Filtered Differential Expression Table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 43. Differential Expression Table Pathway Enrichment Analysis performs the pathway enrichment analysis for the significant metabolites based on the parameters specified for the particular cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 44. Metabolite Set Enrichment Analysis Plot Figure 45. Pathway Topology Analysis Plot Figure 46. Metabolite Set Enrichment Analysis Table Figure 47. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolites they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolism selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 48. Pathway View Plot","title":"Statistical Analysis"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#visualization","text":"This interface allows you to visualize the cohort-based distribution of a specific metabolite or a group of metabolites on the basis on its normalized intensity values. Figure 49. Visualization tab options Enter metabolite: Select the metabolite(s) of interest from the drop down option. Select uniqueIds: You can specifically select the metabolic feature of interest for the metabolite from the drop down option. Select order of cohort: You can also specify the particular order of the cohort to visualize the bar plot. Once the parameters are selected, click on Load Plots to plot the bar plot for the metabolite. Figure 50. Cohort-wise bar plot with the normalized intensity of selected metabolite","title":"Visualization"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#intomix-input","text":"This tab allows you to generate the input for IntOmix where you can visualize the significantly altered metabolic network modules between any two experimental conditions. Figure 51. IntOmix Input Tab options Specify two or more cohorts from the Select cohorts drop down option for which you want to generate the IntOmix input. Once the required cohorts are selected, click on Generate to generate the IntOmix input. Figure 52. IntOmix Table for the cohort conditions specified NOTE: At least two cohorts are required to create the input file.","title":"IntOmix Input"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#heatmap","text":"This tab allows you to produce a heatmap of the processed data, so that you can observe the level of expression in a visual form. Click on Load Heatmap button to generate the heatmap. Figure 53. Heatmap","title":"Heatmap"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#comparative-analysis","text":"This tab allows you to perform comparative analysis between a set of cohorts in your data. As a result of which you can visualize the UpSet plot of the unique and overlapping metabolites for the selected cohort comparisons. Further, you can also perform pathway analysis on the metabolites for the set intersections of interest. Comparison Parameters tab allows you to select the cohorts of interest for which you would want to get the set intersections. You can select the cohorts from the Select cohorts drop-down and click on Run button. Further, you can also specify the p -value cut-off and log 2 FC threshold. Figure 54. Comparison Parameters Tab options You will get a table as a result of the parameters specified which will have the significant metabolites for the different cohort comparisons along with their corresponding p -values and log 2 FC values. You can also download this table as a .CSV file. Figure 55. Comparison Table UpSet Plot tab allows you to visualize the set intersections for the cohort comparisons selected where every comparison consists of the significant metabolites associated with the same. You can select the cohort comparisons of interest from the Select Cohort Comparison drop-down which represents all the possible comparisons for the cohorts specified in the previous tab. Click on Plot to get the UpSet plot for the specified comparisons. Figure 56. UpSet Plot options Figure 57. UpSet Plot Along with the plot, you can also get all the constituent metabolites for the respective comparisons in a tabular format that can be downloaded as a .CSV file. Figure 58. UpSet Plot Table Pathway Enrichment Analysis tab allows you to perform the pathway enrichment analysis for the significant metabolites that show up based on the parameters specified in the Comparison Parameters tab for the particular set of cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 59. Metabolite Set Enrichment Analysis Plot Figure 60. Pathway Topology Analysis Plot Figure 61. Metabolite Set Enrichment Analysis Table Figure 62. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolisms they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolite selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 63. Pathway View options Figure 64. Pathway View Plot","title":"Comparative Analysis"},{"location":"Apps/Metabolomic Data/Dual Mode Visualization.html#data-for-metaboanalyst","text":"This tab allows you to generate the input for MetaboAnalyst that enables statistical, functional and integrative analysis of metabolomics data by providing a variety of modules for different functionalities. Figure 65. Data for MetaboAnalyst Tab options Specify the cohort comparison of interest by sleecting the cohorts from the Select cohort A and Select cohort B drop downs. Click on Download data for metaboanalyst to download the .csv file. Clicking on Go to metaboanalyst will redirect you to MetaboAnalyst\u2019s homepage. NOTE: Normalized intensity data (normalization method chosen in pre-processing tab) is downloaded.","title":"Data for MetaboAnalyst"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html","text":"Introduction Overview Quantification of pathway activity with metabolic flux studies has become an integral part of understanding cellular phenotypes and mechanisms of action. Labeled studies with isotope tracers have emerged as key protocols to gain a deeper understanding of cellular metabolism. Processing and analysis of labeled LC-MS data follows a two-part protocol beginning with peak curation followed by downstream analysis. In this workflow, peak curation is performed using El-MAVEN . Peaks curated using automated peak picking on El-MAVEN can be pushed directly to Phi Relative LC-MS app using the built-in integration. The Phi Relative Labeled LC-MS app allows you to process labeled LC-MS targeted data with insightful visualizations. The application performs natural abundance (NA) correction and provides fractional enrichment, pool totals and NA corrected intensity visualizations across selected cohorts. Scope of the App Raw data to visualizations in less than 5 minutes. Process high-resolution and low-resolution data of up to 100 samples. Analyze single labeled 2 D, 13 C, 15 N and 34 S data. Automated peak picking with manual curation. Quality Check of samples to identify and reject outliers. Perform NA Correction, calculate Fractional Enrichment and Pool Total values. Figure 1. El-MAVEN Phi Relative LC-MS workflow Getting Started User input El-MAVEN Phi Relative LC-MS requires two types of files: Raw MS files These files are instrument generated and can be in different formats such as: .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .d (Bruker) .wiff (ABSciex) Compound Database Compound database is a .csv file that consists of two mandatory columns, compound which contains metabolite names and formula which contains the metabolite formula. Additionally, you can the HMDB or KEGG ID in the column id or retention time in the column rt if known. Any other information present in this file is ignored by El-MAVEN and PollyPhiTM Relative LC-MS. Figure 2. Compound Database Steps involved in data processing Convert instrument specific files to .mzML or .mzXML format using MSConvert Perform peak annotation in El-MAVEN Push data to PollyPhiTM Relative LC-MS using the built-in integration Visualize fractional enrichment, pool total and NA corrected intensity plots for each metabolite across cohorts Visualize PCA and Correlation Plot and remove outliers if any Visualize EIC and reject metabolites or specific labels if required Tutorial MSConvert MSConvert is a command-line/ GUI tool that is used to convert between various mass spectroscopy data formats, developed and maintained by ProteoWizard. Raw data files obtained from mass spectrometers need to be converted to certain acceptable formats before processing in El-MAVEN. msConvert supports the following formats: .mzML .mzXML .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .d (Bruker) .wiff (ABSciex) Figure 3. MSConvert GUI NOTE: Zlib compression is enabled by default in msConvert. El-MAVEN now supports Zlib compression. El-MAVEN Once sample files are ready for processing, launch El-MAVEN. El-MAVEN consists of a sample space and EIC window as shown in Figure 4. Figure 4. El-MAVEN UI Load Samples and Compound Database You can load the sample by going to Load Samples|Workspaces|Peaks option in the Sample space menu. Then navigate to the folder containing the sample data and select all .mzXML or .mzML files. A loading bar displays the progress at the bottom. When the samples have loaded, you should see a sample panel on the left side. If it is not displayed automatically, click on the Samples Widget button on the widget toolbar. El-MAVEN automatically assigns a color to every sample. You can select/deselect any sample by clicking the checkbox on the left of the sample name or completely remove it from the Remove sample option. To mark samples as blank, click on the Mark the Blanks icon in the Samples menu. This will change the color of the marked samples to black to easily differentiate them from other samples. Multiple blanks can be marked together. Figure 5. El-MAVEN toolbars Figure 6. After uploading the samples To load the compound database, click on Compounds tab in the sample space, import standard database file from Library Manager, select the appropriate .csv file. Figure 7. Uploading compound database Adjust Global Settings Global Settings can be changed from the Options dialog. There are 9 tabs in the dialog. Each of these tabs has parameters related to a different module in El-MAVEN. Figure 8. Options dialog you has an option to set Ionization mode to Positive, Negative or Neutral on the top left corner. The [m/z] option scans the groups to find any specific m/z value and plot its corresponding EIC. The +/- option to its right is to specify the expected mass resolution error in parts per million (ppm). Figure 9. Set m/z value Alignment Prolonged use of the LC column can lead to a drift in retention time across samples. Alignment shifts the peak RTs in every sample to correct for this drift and brings the peaks closer to median RT of the group. Alignment visualization can be used to judge the extent of deviation from median RT. Figure 10. Peaks before alignment In the above visualization, each box represents a peak from the selected group at its current RT. Samples are said to be perfectly aligned when all peak boxes lie on the same vertical axis. The peaks are considerably scattered in the above image and therefore the samples should be aligned for better grouping of peaks. Pre-alignment, the peaks were scattered while the aligned peaks lie nearly on the same axis. You can run alignment again with different parameters if required. Alignment settings can be adjusted using the Align button. This example was set to OBI-Warp algorithm. Figure 11. OBI-Warp Algorithm Post-alignment the peaks in the group should appear closer to the median RT of the group. Figure 12. Peak group after alignment Watch the demo video below for more information. Isotope Settings If the samples are labeled, check on Report Isotopic Peaks and select the isotopic tracer and set the parameters for isotope filtering. Figure 13. Isotope selection Peak Curation Generally there are two broad workflows to curate peaks in El-MAVEN: Manual Peak Curation using Compound DB widget : To use manual curation using the compound DB widget, you have to iterate over all the compounds in the compound DB, as highlighted in the images below. Figure 14. Compound selection Once on a compound, El-MAVEN shows the highest ranked group for that m/z. you can now choose a group or reject it. You can read more about the bookmarks table here . Figure 15. Double click on peak group to get details on Bookmark table Automated Peak Curation: Clicking the Peaks icon on the top opens the Feature Detection Selection dialog. For Labeled LC-MS analysis, check on \u2018Compound Database Search\u2019 algorithm. In case compound database has the RT information, you can check on Match Retention Time option. In the Group Filtering dialog, you can set the parameters by using the slide bar. Figure 16. Automated peak detection Figure 17. Group filtering dialog After setting the parameters, click on Find Peaks. You can read more about the peak detection settings here . El-MAVEN Phi Relative LC-MS Integration Export data to Phi Relative LC-MS You can push the curated data to the cloud with the Polly Widget for the labeled LC-MS workflow. Click on th ewidget and enter your Polly credentials to create a folder on Polly to export files. Figure 18. Peak Table After curating the detected metabolites, go to option to export the data directly to PollyPhi . Figure 19. El-MAVEN Phi Interface You can select the peak table and/or create a new workspace to be pushed to PollyPhi for Flux analysis and hit on Upload . This will take a while. Click on Start Fluxing. After giving the command to upload, you would receive an email containing the link to the analysis which can be accessed anytime. After clicking on Start Fluxing , you will be redirected to the GSheet interface where cohort information has to be entered. Figure 20. Automated email with analysis link Figure 21. GSheet Interface Visualization Interface After this, you will land on the Visualization Interface . This interface allows you to visualize NA Corrected Intensities, Fractional Enrichment and Pool Total values. You can switch between the metabolites from the metabolite carousel at the top of the interface. You can remove any isotopologues they don\u2019t wish to visualize from the plots by clicking on the label. You can visualize the position of the metabolite on the metabolism pathway. If HMDB ID is not provided or not detected, you will the see complete central carbon metabolism pathway. You can select cohorts for visualization by clicking on the Settings icon present on the top right corner. Figure 22. Visualization interface Quality Check Interface This interface allows you to visualize sample outliers with the help of a PCA (Principle Component Analysis) plot and a Correlation Plot. Figure 23. QC Interface To reject any outliers, you can hover over the data points and click on Remove Sample . To apply this change, click on Update towards the bottom right of the screen upon which the outliers will be removed from the quality check plots and the subsequent analyses as well. Below the plots, you can view the list of the samples that have been rejected. The rejected samples can be added back to the plots and the subsequent analyses by clicking on the cross icon present alongside each sample. To finalize this selection, click on Update towards the bottom right of the screen upon which the outliers will be added back to the plots and the subsequent analyses as well. Peak Picking Interface After annotating peaks, a new window with the list of detected metabolites their peaks and intensities are shown. You can click on any metabolite and find the peak of interest in EIC. Hover on any peak to view its Sample Name , Quality , Retention Time and Intensity . Figure 24. Peak picking Interface This interface allows you to visualize the EIC plot and reject metabolites or isotopologues with erroneous peaks. You can visualize EIC plot by selecting the metabolite of interest from the metabolite drop down. EIC plot of a particular isotopologue can be visualized by selecting the label from the list of labels. You can reject an erroneous peak by clicking on Reject Label . All isotopologues of a metabolite can also be rejected by clicking on Reject Metabolite . To accept a rejected isotopologue or a metabolite by clicking on Accept Label or Accept Metabolite respectively. Note: Label M0 corresponds to C12 PARENT, M1 to C13-label-1 and so on. Videos Frequently Asked Questions (FAQs) S.No. Questions Answers 1. My files are not in .mzXML or .mzML format. What should I do? You can easily convert your raw files to .mzXML or .mzML using MSConvert. 2. What is the compound database? The compound database is a .csv file which contains the metabolite of interest and their formulas. It can also contain retention time and KEGG/HMDB ID's though they are not mandatory. 3. Can I use El-MAVEM Phi Relative LC-MS to analyze unlabeled data? No. El-MAVEN Phi Relative LC-MS is built to analyze labeled data. 4. Can I use public datasets on El-MAVEN Phi Relative LC-MS? Yes. All you need is to convert the instrument specific raw files to .mzXML or .mzML format and a compound database. 5. Can I check the quality of my samples in El-MAVEN Phi Relative LC-MS? Yes. you can check quality of your dataset using PCA and Correlation Plot. 6. Can I reject any metabolites from the analysis? On the Peak Picking Interfac, click on Reject Metabolite to reject any particular metabolite from your analysis. 7. Can I reject any label of a selected metabolite from the analysis? As mentioned above, you can use the Reject Metabolite option for any specific isotopologue as well. 8. Is there any limit on the file size tha can be uploaded on El-MAVEN Phi Relative LC-MS? No. You can upload data irrespective of its size. 9. How can I edit parameters for natural abundance correction? El-MAVEN Phi Relative LC-MS does not allow you to edit the natural abundance correction parameters. To have more flexibility over your analysis, please refer to Labeled LC-MS Workflow . 10. Can the values be normalized based on the external standards I have? Yes. To support user specific customizations, you can use Polly Notebooks .","title":"El-MAVEN Phi Relative LC-MS"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#overview","text":"Quantification of pathway activity with metabolic flux studies has become an integral part of understanding cellular phenotypes and mechanisms of action. Labeled studies with isotope tracers have emerged as key protocols to gain a deeper understanding of cellular metabolism. Processing and analysis of labeled LC-MS data follows a two-part protocol beginning with peak curation followed by downstream analysis. In this workflow, peak curation is performed using El-MAVEN . Peaks curated using automated peak picking on El-MAVEN can be pushed directly to Phi Relative LC-MS app using the built-in integration. The Phi Relative Labeled LC-MS app allows you to process labeled LC-MS targeted data with insightful visualizations. The application performs natural abundance (NA) correction and provides fractional enrichment, pool totals and NA corrected intensity visualizations across selected cohorts.","title":"Overview"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#scope-of-the-app","text":"Raw data to visualizations in less than 5 minutes. Process high-resolution and low-resolution data of up to 100 samples. Analyze single labeled 2 D, 13 C, 15 N and 34 S data. Automated peak picking with manual curation. Quality Check of samples to identify and reject outliers. Perform NA Correction, calculate Fractional Enrichment and Pool Total values. Figure 1. El-MAVEN Phi Relative LC-MS workflow","title":"Scope of the App"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#user-input","text":"El-MAVEN Phi Relative LC-MS requires two types of files: Raw MS files These files are instrument generated and can be in different formats such as: .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .d (Bruker) .wiff (ABSciex) Compound Database Compound database is a .csv file that consists of two mandatory columns, compound which contains metabolite names and formula which contains the metabolite formula. Additionally, you can the HMDB or KEGG ID in the column id or retention time in the column rt if known. Any other information present in this file is ignored by El-MAVEN and PollyPhiTM Relative LC-MS. Figure 2. Compound Database","title":"User input"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#steps-involved-in-data-processing","text":"Convert instrument specific files to .mzML or .mzXML format using MSConvert Perform peak annotation in El-MAVEN Push data to PollyPhiTM Relative LC-MS using the built-in integration Visualize fractional enrichment, pool total and NA corrected intensity plots for each metabolite across cohorts Visualize PCA and Correlation Plot and remove outliers if any Visualize EIC and reject metabolites or specific labels if required","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#msconvert","text":"MSConvert is a command-line/ GUI tool that is used to convert between various mass spectroscopy data formats, developed and maintained by ProteoWizard. Raw data files obtained from mass spectrometers need to be converted to certain acceptable formats before processing in El-MAVEN. msConvert supports the following formats: .mzML .mzXML .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .d (Bruker) .wiff (ABSciex) Figure 3. MSConvert GUI NOTE: Zlib compression is enabled by default in msConvert. El-MAVEN now supports Zlib compression.","title":"MSConvert"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#el-maven","text":"Once sample files are ready for processing, launch El-MAVEN. El-MAVEN consists of a sample space and EIC window as shown in Figure 4. Figure 4. El-MAVEN UI Load Samples and Compound Database You can load the sample by going to Load Samples|Workspaces|Peaks option in the Sample space menu. Then navigate to the folder containing the sample data and select all .mzXML or .mzML files. A loading bar displays the progress at the bottom. When the samples have loaded, you should see a sample panel on the left side. If it is not displayed automatically, click on the Samples Widget button on the widget toolbar. El-MAVEN automatically assigns a color to every sample. You can select/deselect any sample by clicking the checkbox on the left of the sample name or completely remove it from the Remove sample option. To mark samples as blank, click on the Mark the Blanks icon in the Samples menu. This will change the color of the marked samples to black to easily differentiate them from other samples. Multiple blanks can be marked together. Figure 5. El-MAVEN toolbars Figure 6. After uploading the samples To load the compound database, click on Compounds tab in the sample space, import standard database file from Library Manager, select the appropriate .csv file. Figure 7. Uploading compound database Adjust Global Settings Global Settings can be changed from the Options dialog. There are 9 tabs in the dialog. Each of these tabs has parameters related to a different module in El-MAVEN. Figure 8. Options dialog you has an option to set Ionization mode to Positive, Negative or Neutral on the top left corner. The [m/z] option scans the groups to find any specific m/z value and plot its corresponding EIC. The +/- option to its right is to specify the expected mass resolution error in parts per million (ppm). Figure 9. Set m/z value Alignment Prolonged use of the LC column can lead to a drift in retention time across samples. Alignment shifts the peak RTs in every sample to correct for this drift and brings the peaks closer to median RT of the group. Alignment visualization can be used to judge the extent of deviation from median RT. Figure 10. Peaks before alignment In the above visualization, each box represents a peak from the selected group at its current RT. Samples are said to be perfectly aligned when all peak boxes lie on the same vertical axis. The peaks are considerably scattered in the above image and therefore the samples should be aligned for better grouping of peaks. Pre-alignment, the peaks were scattered while the aligned peaks lie nearly on the same axis. You can run alignment again with different parameters if required. Alignment settings can be adjusted using the Align button. This example was set to OBI-Warp algorithm. Figure 11. OBI-Warp Algorithm Post-alignment the peaks in the group should appear closer to the median RT of the group. Figure 12. Peak group after alignment Watch the demo video below for more information. Isotope Settings If the samples are labeled, check on Report Isotopic Peaks and select the isotopic tracer and set the parameters for isotope filtering. Figure 13. Isotope selection Peak Curation Generally there are two broad workflows to curate peaks in El-MAVEN: Manual Peak Curation using Compound DB widget : To use manual curation using the compound DB widget, you have to iterate over all the compounds in the compound DB, as highlighted in the images below. Figure 14. Compound selection Once on a compound, El-MAVEN shows the highest ranked group for that m/z. you can now choose a group or reject it. You can read more about the bookmarks table here . Figure 15. Double click on peak group to get details on Bookmark table Automated Peak Curation: Clicking the Peaks icon on the top opens the Feature Detection Selection dialog. For Labeled LC-MS analysis, check on \u2018Compound Database Search\u2019 algorithm. In case compound database has the RT information, you can check on Match Retention Time option. In the Group Filtering dialog, you can set the parameters by using the slide bar. Figure 16. Automated peak detection Figure 17. Group filtering dialog After setting the parameters, click on Find Peaks. You can read more about the peak detection settings here .","title":"El-MAVEN"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#el-maven-phi-relative-lc-ms-integration","text":"Export data to Phi Relative LC-MS You can push the curated data to the cloud with the Polly Widget for the labeled LC-MS workflow. Click on th ewidget and enter your Polly credentials to create a folder on Polly to export files. Figure 18. Peak Table After curating the detected metabolites, go to option to export the data directly to PollyPhi . Figure 19. El-MAVEN Phi Interface You can select the peak table and/or create a new workspace to be pushed to PollyPhi for Flux analysis and hit on Upload . This will take a while. Click on Start Fluxing. After giving the command to upload, you would receive an email containing the link to the analysis which can be accessed anytime. After clicking on Start Fluxing , you will be redirected to the GSheet interface where cohort information has to be entered. Figure 20. Automated email with analysis link Figure 21. GSheet Interface Visualization Interface After this, you will land on the Visualization Interface . This interface allows you to visualize NA Corrected Intensities, Fractional Enrichment and Pool Total values. You can switch between the metabolites from the metabolite carousel at the top of the interface. You can remove any isotopologues they don\u2019t wish to visualize from the plots by clicking on the label. You can visualize the position of the metabolite on the metabolism pathway. If HMDB ID is not provided or not detected, you will the see complete central carbon metabolism pathway. You can select cohorts for visualization by clicking on the Settings icon present on the top right corner. Figure 22. Visualization interface Quality Check Interface This interface allows you to visualize sample outliers with the help of a PCA (Principle Component Analysis) plot and a Correlation Plot. Figure 23. QC Interface To reject any outliers, you can hover over the data points and click on Remove Sample . To apply this change, click on Update towards the bottom right of the screen upon which the outliers will be removed from the quality check plots and the subsequent analyses as well. Below the plots, you can view the list of the samples that have been rejected. The rejected samples can be added back to the plots and the subsequent analyses by clicking on the cross icon present alongside each sample. To finalize this selection, click on Update towards the bottom right of the screen upon which the outliers will be added back to the plots and the subsequent analyses as well. Peak Picking Interface After annotating peaks, a new window with the list of detected metabolites their peaks and intensities are shown. You can click on any metabolite and find the peak of interest in EIC. Hover on any peak to view its Sample Name , Quality , Retention Time and Intensity . Figure 24. Peak picking Interface This interface allows you to visualize the EIC plot and reject metabolites or isotopologues with erroneous peaks. You can visualize EIC plot by selecting the metabolite of interest from the metabolite drop down. EIC plot of a particular isotopologue can be visualized by selecting the label from the list of labels. You can reject an erroneous peak by clicking on Reject Label . All isotopologues of a metabolite can also be rejected by clicking on Reject Metabolite . To accept a rejected isotopologue or a metabolite by clicking on Accept Label or Accept Metabolite respectively. Note: Label M0 corresponds to C12 PARENT, M1 to C13-label-1 and so on.","title":"El-MAVEN Phi Relative LC-MS Integration"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#videos","text":"","title":"Videos"},{"location":"Apps/Metabolomic Data/El-MAVEN Phi Relative LC-MS.html#frequently-asked-questions-faqs","text":"S.No. Questions Answers 1. My files are not in .mzXML or .mzML format. What should I do? You can easily convert your raw files to .mzXML or .mzML using MSConvert. 2. What is the compound database? The compound database is a .csv file which contains the metabolite of interest and their formulas. It can also contain retention time and KEGG/HMDB ID's though they are not mandatory. 3. Can I use El-MAVEM Phi Relative LC-MS to analyze unlabeled data? No. El-MAVEN Phi Relative LC-MS is built to analyze labeled data. 4. Can I use public datasets on El-MAVEN Phi Relative LC-MS? Yes. All you need is to convert the instrument specific raw files to .mzXML or .mzML format and a compound database. 5. Can I check the quality of my samples in El-MAVEN Phi Relative LC-MS? Yes. you can check quality of your dataset using PCA and Correlation Plot. 6. Can I reject any metabolites from the analysis? On the Peak Picking Interfac, click on Reject Metabolite to reject any particular metabolite from your analysis. 7. Can I reject any label of a selected metabolite from the analysis? As mentioned above, you can use the Reject Metabolite option for any specific isotopologue as well. 8. Is there any limit on the file size tha can be uploaded on El-MAVEN Phi Relative LC-MS? No. You can upload data irrespective of its size. 9. How can I edit parameters for natural abundance correction? El-MAVEN Phi Relative LC-MS does not allow you to edit the natural abundance correction parameters. To have more flexibility over your analysis, please refer to Labeled LC-MS Workflow . 10. Can the values be normalized based on the external standards I have? Yes. To support user specific customizations, you can use Polly Notebooks .","title":"Frequently Asked Questions (FAQs)"},{"location":"Apps/Metabolomic Data/El-MAVEN.html","text":"Introduction Overview Polly is capable of hosting any desktop applications that can be dockerized. One such desktop application is El-MAVEN . It is an open-source LC-MS data processing engine that is optimal for isotopomer labeling and untargeted metabolomic profiling experiments. Currently El-MAVEN exists as a desktop application that runs on Windows, Linux, and Mac systems. The software can be used to view the mass spectra, align chromatograms, perform peak-feature detection, and alignment for labeled and unlabeled mass spectrometry data. The aim of this software package is to reduce the complexity of metabolomics analysis by using a highly intuitive interface for exploring and validating metabolomics data. El-MAVEN is now hosted on the Polly platform. This means El-MAVEN and all your Polly applications can reside on a single platform and can be shared with your colleagues easily. The following are the advantages of having El-MAVEN on Polly: The application scales automatically on the Polly infrastructure for multiple users to use simultaneously. Access to the application is controlled by Polly authentication. Only users in your organization with required Polly authorization can access the application. Have input and output data along with analysis stored at the same location in the Polly Workspaces . Share the analyses along with input and output data by simply sharing the workspace. Getting Started Select El-MAVEN from Polly Dashboard under the Metabolomics Data Tab as shown in Figure 1. Create a New Workspace or choose form the existing one from the drop down and provide the Name of the Session to be redirected to choose the machine configuration. Figure 1. Select Application Figure 2. Workspace selection The size of the data varies from a few MBs to hundreds of GBs, and in order to process and analyze this huge data, you would need the computation power from a small machine to a large workstation. Polly supports configurations having 16 to 120 GB Ram and 4 to 16 CPU cores as of now. While other machine configurations can be added on customer request. Figure 3. Mchine configurations General purpose: Configuration 4 CPU cores 16 GB RAM fall under this category. Name CPU/Cores RAM Polly x-large 4 16 GB Memory optimized: Configurations from 4 to 16 CPU cores and 32 to 120 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 4 32GB Polly 3x-large 8 64GB Polly 4x-large 16 120GB Choose from these pre-configured machines. Upon selecting you will see a progress bar that will tell you the status of your app launch. Figure 4. App Launch Status This application act as a one-stop interface for all your peak curation needs. It contains: El-MAVEN desktop application: For peak picking MSConvert : For raw file conversions Browser: To access data from anywhere on the cloud Terminal: For transferring your raw and processed data to and from the workspace. Also it can be used as a general terminal for other operations as well. Figure 5. Provided Apps and functionalities El-MAVEN Once the server is ready, you will see El-MAVEN opened on the browser. The interface is similar to the desktop version. You can start analyzing by adding your files. Figure 6. El-MAVEN Application MSConvert To cater to the needs of your file conversions. Polly also hosts MSConvert application. It is a command-line/GUI tool that is used to convert between various mass spectroscopy data formats. Raw data files obtained from mass spectrometers need to be converted to certain acceptable formats before processing in El-MAVEN. MSConvert supports the following formats: .mzXML .mzML .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .wiff (ABSciex) Select the tab next to El-MAVEN to access it as and when required. Figure 7. MSConvert Application Browser Polly also provides a web browser for you to surf the internet on the fly. You can now search for any data related query or get data from anywhere on the cloud (Polly workspaces, FTP, etc) within the same interface. Figure 8. Browser Terminal Once the Terminal option is selected, it provides access to the command-line interface to execute any sets of commands. You have access to all the file types which are available in Polly through the terminal. You can use it for getting data into the El-MAVEN instance from the Workspaces and sending processed data back. The terminal option also allows you to install Python or R packages, managing system binaries and system configurations and helps you working with code repositories hosted on GitHub, Bitbucket, etc. Figure 9. Terminal Accessing directories You can fetch directories from the Workspaces. The contents of any directory within a Workspace can be listed using the following command on the terminal. polly files list --workspace-path \"path/to/polly/folder\" -y Here the path of the directory has to start with \u201c polly:// \u201d. To view the contents within a folder called \u201cData\u201d in the workspace, the following command will have to be executed on the terminal. polly files list --workspace-path \"polly://Data\" -y To access the directory, the following command will have to be executed on the terminal. polly files sync -s \"path/to/polly/folder\" -d \"path/on/elmaven/instance\" -y If the folder called \u201cData\u201d is to be accessed from Workspace in the folder called \u201cInput\u201d, execute the following command. polly files sync -s \"polly://Data\" -d \"Input\" -y To save directories back to the Workspace, keep the source as it is and destination as Polly Workspace in the same command as mentioned above. polly files sync -s \"path/on/elmaven/instance\" -d \"path/to/polly/folder\" -y To save the folder called \u201cOutput\u201d back to Polly Workspace, use the following command. polly files sync -s \"Output\" -d \"polly://\" -y Instance Termination Since peak picking can take days, Polly provides three options: You can choose to close the tab without stoping or terminating it. This would keep the instance running in the background even if the browser tab is closed. You can later restore the same analysis with the same data used, from the Analysis section. In case you choose to close the instance, you have separate options to Stop and Terminate the instance. Stopping the instance would keep the data (picked peaks) intact in the machine and the instance can be later resumed (soon to be available on Polly). If the tab is closed without Terminating or Stopping the instance, it would continue to run in the background. In case of termination of instance, the data would not remain intact in the machine. You would have to add the files again from the Workspace to perform the analysis. Note : Make sure you transfer your results to the workspace before selecting the terminate option. Figure 10. Instance Termination Restore Analysis Polly contains the restore functionality that allows any analysis to be restored to the last step. This functionality would be available only if the instance has not been terminated. Analyses can be restored by navigating to the Analysis section of a workspace. Click on History for the specific analysis to restore. Figure 11. History Figure 12. Restore Analysis Clicking on Restore Analysis will take you back to the application with the same data used before. It helps you to restore the incomplete analysis and improves reproducibility. In case you have terminated the instance, the option to restore will not be available. Instead, there will be an option to Restart Analysis , which will take you back to the analysis with the same machine as used earlier in the analysis without restoring the data back. You would have to add the files again from the workspace to perform the analysis. Figure 13. Restart Analysis","title":"El-MAVEN"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#overview","text":"Polly is capable of hosting any desktop applications that can be dockerized. One such desktop application is El-MAVEN . It is an open-source LC-MS data processing engine that is optimal for isotopomer labeling and untargeted metabolomic profiling experiments. Currently El-MAVEN exists as a desktop application that runs on Windows, Linux, and Mac systems. The software can be used to view the mass spectra, align chromatograms, perform peak-feature detection, and alignment for labeled and unlabeled mass spectrometry data. The aim of this software package is to reduce the complexity of metabolomics analysis by using a highly intuitive interface for exploring and validating metabolomics data. El-MAVEN is now hosted on the Polly platform. This means El-MAVEN and all your Polly applications can reside on a single platform and can be shared with your colleagues easily. The following are the advantages of having El-MAVEN on Polly: The application scales automatically on the Polly infrastructure for multiple users to use simultaneously. Access to the application is controlled by Polly authentication. Only users in your organization with required Polly authorization can access the application. Have input and output data along with analysis stored at the same location in the Polly Workspaces . Share the analyses along with input and output data by simply sharing the workspace.","title":"Overview"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#getting-started","text":"Select El-MAVEN from Polly Dashboard under the Metabolomics Data Tab as shown in Figure 1. Create a New Workspace or choose form the existing one from the drop down and provide the Name of the Session to be redirected to choose the machine configuration. Figure 1. Select Application Figure 2. Workspace selection The size of the data varies from a few MBs to hundreds of GBs, and in order to process and analyze this huge data, you would need the computation power from a small machine to a large workstation. Polly supports configurations having 16 to 120 GB Ram and 4 to 16 CPU cores as of now. While other machine configurations can be added on customer request. Figure 3. Mchine configurations General purpose: Configuration 4 CPU cores 16 GB RAM fall under this category. Name CPU/Cores RAM Polly x-large 4 16 GB Memory optimized: Configurations from 4 to 16 CPU cores and 32 to 120 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 4 32GB Polly 3x-large 8 64GB Polly 4x-large 16 120GB Choose from these pre-configured machines. Upon selecting you will see a progress bar that will tell you the status of your app launch. Figure 4. App Launch Status This application act as a one-stop interface for all your peak curation needs. It contains: El-MAVEN desktop application: For peak picking MSConvert : For raw file conversions Browser: To access data from anywhere on the cloud Terminal: For transferring your raw and processed data to and from the workspace. Also it can be used as a general terminal for other operations as well. Figure 5. Provided Apps and functionalities","title":"Getting Started"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#el-maven","text":"Once the server is ready, you will see El-MAVEN opened on the browser. The interface is similar to the desktop version. You can start analyzing by adding your files. Figure 6. El-MAVEN Application","title":"El-MAVEN"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#msconvert","text":"To cater to the needs of your file conversions. Polly also hosts MSConvert application. It is a command-line/GUI tool that is used to convert between various mass spectroscopy data formats. Raw data files obtained from mass spectrometers need to be converted to certain acceptable formats before processing in El-MAVEN. MSConvert supports the following formats: .mzXML .mzML .RAW (Thermofisher) .RAW (Waters) .d (Agilent) .wiff (ABSciex) Select the tab next to El-MAVEN to access it as and when required. Figure 7. MSConvert Application","title":"MSConvert"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#browser","text":"Polly also provides a web browser for you to surf the internet on the fly. You can now search for any data related query or get data from anywhere on the cloud (Polly workspaces, FTP, etc) within the same interface. Figure 8. Browser","title":"Browser"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#terminal","text":"Once the Terminal option is selected, it provides access to the command-line interface to execute any sets of commands. You have access to all the file types which are available in Polly through the terminal. You can use it for getting data into the El-MAVEN instance from the Workspaces and sending processed data back. The terminal option also allows you to install Python or R packages, managing system binaries and system configurations and helps you working with code repositories hosted on GitHub, Bitbucket, etc. Figure 9. Terminal","title":"Terminal"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#accessing-directories","text":"You can fetch directories from the Workspaces. The contents of any directory within a Workspace can be listed using the following command on the terminal. polly files list --workspace-path \"path/to/polly/folder\" -y Here the path of the directory has to start with \u201c polly:// \u201d. To view the contents within a folder called \u201cData\u201d in the workspace, the following command will have to be executed on the terminal. polly files list --workspace-path \"polly://Data\" -y To access the directory, the following command will have to be executed on the terminal. polly files sync -s \"path/to/polly/folder\" -d \"path/on/elmaven/instance\" -y If the folder called \u201cData\u201d is to be accessed from Workspace in the folder called \u201cInput\u201d, execute the following command. polly files sync -s \"polly://Data\" -d \"Input\" -y To save directories back to the Workspace, keep the source as it is and destination as Polly Workspace in the same command as mentioned above. polly files sync -s \"path/on/elmaven/instance\" -d \"path/to/polly/folder\" -y To save the folder called \u201cOutput\u201d back to Polly Workspace, use the following command. polly files sync -s \"Output\" -d \"polly://\" -y","title":"Accessing directories"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#instance-termination","text":"Since peak picking can take days, Polly provides three options: You can choose to close the tab without stoping or terminating it. This would keep the instance running in the background even if the browser tab is closed. You can later restore the same analysis with the same data used, from the Analysis section. In case you choose to close the instance, you have separate options to Stop and Terminate the instance. Stopping the instance would keep the data (picked peaks) intact in the machine and the instance can be later resumed (soon to be available on Polly). If the tab is closed without Terminating or Stopping the instance, it would continue to run in the background. In case of termination of instance, the data would not remain intact in the machine. You would have to add the files again from the Workspace to perform the analysis. Note : Make sure you transfer your results to the workspace before selecting the terminate option. Figure 10. Instance Termination","title":"Instance Termination"},{"location":"Apps/Metabolomic Data/El-MAVEN.html#restore-analysis","text":"Polly contains the restore functionality that allows any analysis to be restored to the last step. This functionality would be available only if the instance has not been terminated. Analyses can be restored by navigating to the Analysis section of a workspace. Click on History for the specific analysis to restore. Figure 11. History Figure 12. Restore Analysis Clicking on Restore Analysis will take you back to the application with the same data used before. It helps you to restore the incomplete analysis and improves reproducibility. In case you have terminated the instance, the option to restore will not be available. Instead, there will be an option to Restart Analysis , which will take you back to the analysis with the same machine as used earlier in the analysis without restoring the data back. You would have to add the files again from the workspace to perform the analysis. Figure 13. Restart Analysis","title":"Restore Analysis"},{"location":"Apps/Metabolomic Data/FirstView.html","text":"Introduction Overview El-MAVEN , our open source LC-MS data processing engine is optimized for isotopomer labeling and untargeted metabolomic profiling experiments. You can view the mass spectra, align chromatograms, perform peak-feature detection and alignment for labeled and unlabeled MS data. FirstView enables you to have a first look at your data using basic visualizations to get preliminary insights. Scope of the App Provides visualizations for both labeled and unlabeled data Provides visualizations on both raw and log 2 transformed intensities View intensity plot for each metabolite View Isotope plot for each metabolite in case of labeled data View heat map for the entire dataset Figure 1. FirstView Getting Started User Input There are two ways to upload files to FirstView: El-MAVEN FirstView interface It allows you to directly push El-MAVEN output to FirstView as shown in Figure 2. Figure 2. El-MAVEN FirstView Interface Application upload interface To use FirstView from Polly, you only require the El-MAVEN Output file: Figure 3. El-MAVEN Output FIle Steps in data processing Upload data to FirstView Visualize the intensity plot for each metabolite Visualize the isotope plot for each metabolite Visualize the heat map for the entire dataset Caveats The El-MAVEN output should be generated using a compound database For labeled data, the intensities in the Isotope Plot are NOT corrected for natural abundance Log 2 Transformation does not change the Isotope Plot For unlabeled data, the Isotope Plot section will be blank Tutorial Upload files El-MAVEN FirstView Interface Once you have derived a peak table containing the list of metabolites. You can click on the Polly Icon at the Menu Bar to export the data directly to PollyTM FirstView. Provide your Polly login credentials and a window will pop-up which allows you to select the peak table to be uploaded to FirstView. You can select the workspace from the list of your existing Polly Workspaces or choose to create a new one from this menu itself and then click on Upload. Once the files are uploaded, click on Go to FirstView and you would be redirected to FirstView. Figure 4. El-MAVEN FirstView Interface Application upload interface Alternatively, select FirstView from the dashboard under the Metabolomics Data Tab. Figure 5. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the Session to be redirected to FirstView's upload page as shown in Figure 6. Figure 6. Workspace selection Figure 7. Upload page of FirstView Upload your intensity file by either clicking on Browse or using the drag and drop functionality. Visualization The visualization interface provides the data summary (number of samples, groups and metabolites present in the uploaded data) and normalization on the left side whereas the rest of the interface provides the generated plots. Intensity Plot The Intensity Plot displays the intensity values either raw or log 2 transformed, for a given metabolite across all samples. For labeled data, the intensity for all isotopologues are displayed in different colors in stacked bar plots as illustrated in Figure 8. below. Figure 8. Intensity Plot with raw intensities Click on the drop down to visualize the intensity for a different metabolite. Figure 9. Drop down to Intensity Plot with raw intensities To see a normalized Intensity Plot, select the Log2 Transformed option in Normalization section. The plot would get updated to accommodate the normalized intensities. Figure 10. Intensity Plot with normalized intensities Isotope Plot Click on Isotope Plot option beside Intensity Plot to see a fractional enrichment vs sample plot. The values seen are calculated as: intensity of parent or isotopologue / sum (intensity of metabolite for a sample) Figure 11. Isotope Plot with raw intensities Heat map Click on Heat map option beside Isotope Plot to open the heat map. Heat map is a graphical representation of data where the individual values contained in a matrix are represented as different colors corresponding to the intensity. Here, the heat map represents intensity value of each metabolite corresponding to the number of samples in the data. Figure 12. Heat map with raw intensities You can view this plot with both raw intensities and normalized intensities by choosing the desired normalization option from the left section of the page. Hovering over the plot will display the specific values. Figure 13. Heat map with normalized intensities","title":"FirstView"},{"location":"Apps/Metabolomic Data/FirstView.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/FirstView.html#overview","text":"El-MAVEN , our open source LC-MS data processing engine is optimized for isotopomer labeling and untargeted metabolomic profiling experiments. You can view the mass spectra, align chromatograms, perform peak-feature detection and alignment for labeled and unlabeled MS data. FirstView enables you to have a first look at your data using basic visualizations to get preliminary insights.","title":"Overview"},{"location":"Apps/Metabolomic Data/FirstView.html#scope-of-the-app","text":"Provides visualizations for both labeled and unlabeled data Provides visualizations on both raw and log 2 transformed intensities View intensity plot for each metabolite View Isotope plot for each metabolite in case of labeled data View heat map for the entire dataset Figure 1. FirstView","title":"Scope of the App"},{"location":"Apps/Metabolomic Data/FirstView.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/FirstView.html#user-input","text":"There are two ways to upload files to FirstView: El-MAVEN FirstView interface It allows you to directly push El-MAVEN output to FirstView as shown in Figure 2. Figure 2. El-MAVEN FirstView Interface Application upload interface To use FirstView from Polly, you only require the El-MAVEN Output file: Figure 3. El-MAVEN Output FIle","title":"User Input"},{"location":"Apps/Metabolomic Data/FirstView.html#steps-in-data-processing","text":"Upload data to FirstView Visualize the intensity plot for each metabolite Visualize the isotope plot for each metabolite Visualize the heat map for the entire dataset","title":"Steps in data processing"},{"location":"Apps/Metabolomic Data/FirstView.html#caveats","text":"The El-MAVEN output should be generated using a compound database For labeled data, the intensities in the Isotope Plot are NOT corrected for natural abundance Log 2 Transformation does not change the Isotope Plot For unlabeled data, the Isotope Plot section will be blank","title":"Caveats"},{"location":"Apps/Metabolomic Data/FirstView.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Metabolomic Data/FirstView.html#upload-files","text":"El-MAVEN FirstView Interface Once you have derived a peak table containing the list of metabolites. You can click on the Polly Icon at the Menu Bar to export the data directly to PollyTM FirstView. Provide your Polly login credentials and a window will pop-up which allows you to select the peak table to be uploaded to FirstView. You can select the workspace from the list of your existing Polly Workspaces or choose to create a new one from this menu itself and then click on Upload. Once the files are uploaded, click on Go to FirstView and you would be redirected to FirstView. Figure 4. El-MAVEN FirstView Interface Application upload interface Alternatively, select FirstView from the dashboard under the Metabolomics Data Tab. Figure 5. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the Session to be redirected to FirstView's upload page as shown in Figure 6. Figure 6. Workspace selection Figure 7. Upload page of FirstView Upload your intensity file by either clicking on Browse or using the drag and drop functionality.","title":"Upload files"},{"location":"Apps/Metabolomic Data/FirstView.html#visualization","text":"The visualization interface provides the data summary (number of samples, groups and metabolites present in the uploaded data) and normalization on the left side whereas the rest of the interface provides the generated plots. Intensity Plot The Intensity Plot displays the intensity values either raw or log 2 transformed, for a given metabolite across all samples. For labeled data, the intensity for all isotopologues are displayed in different colors in stacked bar plots as illustrated in Figure 8. below. Figure 8. Intensity Plot with raw intensities Click on the drop down to visualize the intensity for a different metabolite. Figure 9. Drop down to Intensity Plot with raw intensities To see a normalized Intensity Plot, select the Log2 Transformed option in Normalization section. The plot would get updated to accommodate the normalized intensities. Figure 10. Intensity Plot with normalized intensities Isotope Plot Click on Isotope Plot option beside Intensity Plot to see a fractional enrichment vs sample plot. The values seen are calculated as: intensity of parent or isotopologue / sum (intensity of metabolite for a sample) Figure 11. Isotope Plot with raw intensities Heat map Click on Heat map option beside Isotope Plot to open the heat map. Heat map is a graphical representation of data where the individual values contained in a matrix are represented as different colors corresponding to the intensity. Here, the heat map represents intensity value of each metabolite corresponding to the number of samples in the data. Figure 12. Heat map with raw intensities You can view this plot with both raw intensities and normalized intensities by choosing the desired normalization option from the left section of the page. Hovering over the plot will display the specific values. Figure 13. Heat map with normalized intensities","title":"Visualization"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html","text":"Introduction Overview The use of isotopically labeled tracer substrates is an experimental approach for measuring in vivo and in vitro intracellular metabolic dynamics. Stable isotopes that alter the mass but not the chemical behavior of a molecule are commonly used in isotope tracer studies. Because stable isotopes of some atoms naturally occur at non-negligible abundances, it is important to account for the natural abundance of these isotopes when analyzing data from isotope labeling experiments. Specifically, a distinction must be made between isotopes introduced experimentally via an isotopically labeled tracer and the isotopes naturally present at the start of an experiment. The Labeled LC-MS Workflow factors out the contribution from the Natural Abundance (NA) of each element from the signal of each isotopologue peak obtained from LC-MS techniques in a labeling experiment. It provides visualization for fractional enrichment and pool totals for single as well as dual-labeled data. It also allows you to download an output Excel Workbook with worksheets containing the raw abundances, raw intensities, fractional contributions and corrected isotopologues of the metabolites belonging to different pathways based on the library data provided. Scope of the app Performs NA correction on data from LC-MS experiments. Supports data from single ( 2 D, 13 C, 15 N and 34 S) as well as dual-labeled experiments. Plots fractional enrichment, pool total and NA corrected intensity plots for a selected metabolite. Generates a consolidated Excel workbook of outputs. Figure 1. Labeled LC-MS Workflow Getting Started User Input Labeled LC-MS Workflow requires the following two files as input: Intensity file The intensity file should be in CSV format as shown in Figure 1. The .csv file exported after peak picking in El-MAVEN is the input file. Figure 2. El-MAVEN output file Cohort file The cohort file is optional and is used for generating visualizations. In case you only want to perform NA correction, you can skip this step. The cohort file should be in .csv format as shown in Figure 3. This file should contain two columns, Sample containing sample names along with Cohort for its cohort information. Figure 3. Cohort file Steps involved in data processing Upload intensity file obtained from El-MAVEN Set analysis parameters and perform NA correction Download NA corrected output files Visualize fractional enrichment, pool total and NA corrected intensity plots Download consolidated output workbook using a library data Caveats All the samples present in the intensity files should also be present in the cohort file. The cohort file is optional but visualizations will only be generated if this file is provided. If using the Edit NA Option , the natural abundances of all the isotopes of an element should sum to 1. The library file is not required for NA Correction. The name of the metabolites should match exactly between the input and the library file. Tutorial Upload files The Upload Files tab allows you to upload the input files required for processing through the app. Once the files have been uploaded, click on Run. This will validate and process the uploaded files and give error messages or warnings if the format or data in files are incorrect. You can also run the demo files directly by clicking on Run Demo Files . Figure 4. Upload interface Cohort Creation Interface If you do not wish to create a cohort file, you can utilize the cohort creation interface that allows you to enter the cohort information on the app itself. Upload your intensity file and uncheck Upload Cohort File option. Once the intensity file is uploaded, click on Run and a cohort creation interface with sample names with input files will open as shown in Figure 4. After entering the cohort information, click on NA Correction on the top left side to be taken to the NA correction interface. Figure 5. Cohort creation interface NA Correction Isotopologue identification and quantification of thousands of metabolites in the metabolomic experiments can provide a wealth of data for modeling the flux through metabolic networks. But before isotopologue intensity data can be properly interpreted, the contributions from isotopic natural abundance must be factored out (deisotoped). This is referred to as natural abundance (NA) correction. The various analysis parameters for performing NA Correction shown in Figure 5 are defined below: Figure 6. NA Correction Tab Indistinguishable isotopes : Indistinguishable isotopes can be corrected for natural abundance by electing either of the following options: Manually: You can manually select a standard list of indistinguishable isotopes with respect to the labelled isotope for NA correction. Using PPM: You can enable auto detection of indistinguishable isotopes, by selecting Using PPM option and providing PPM resolution of the mass spectrometer used for the experiment. Edit NA values: Default NA values used in the tool are taken from CRC Handbook of Chemistry and Physics. You can change these values by clicking on Edit NA Values . as shown in Figure 6. Figure 7. Edit NA values Replace negatives with zero: Sometimes negative intensities arise which can be replaced with zero. This option is selected by default. After all parameters have been decided click on Perform NA Correction . Once NA correction has been performed, the output will be displayed in in a tabular format as shown in Figure 7. There are three sections on this interface; NA Corrected Values , Fractional Enrichment and Merged Output . The first two sections only display the NA corrected values and fractional enrichment respectively as shown in Figure 7 and Figure 8 whereas the Third section combines these information to provide you with a single file with the entire information as shown in Figure 9. A detailed description of the merged output file is provided below. Figure 8. NA Corrected Values Figure 9. Fractional Enrichment Figure 10. Merged Output where, Name is Name of the metabolite Label is Label information Formula is Chemical formula Sample is Sample names Intensity is Raw intensities NA Corrected is NA corrected intensities NA Corrected with zero is NA corrected intensities with negative intensities replaced with zero Indistinguishable isotopes is List of indistinguishable isotopes for the particular metabolite Pool total is Pool total of a metabolite in a sample Fractional Enrichment is Fractional enrichment calculated after NA Correction has been done Fractional Enrichment with zero is Negative values are replaced with zero compoundid is KEGG or HMDB ID of the metabolites as specified oin the input file Visualization The visualization tab provides fractional enrichment, pool total and NA corrected intensity plots that are generated according to the metabolite and quantification type specified before. Figure 11. Visualization Interface To generate visualizations, select a metabolite using the drop down that contains metabolites present in the intensity file uploaded and a quantification type from Intensity , NA Corrected , NA Corrected with zero , Fractional Enrichment and Fractional Enrichment with zero . After this, click on Run to get the selected visualization. Visualization on the basis of the number of labels shows all the available labels of a metabolite based on the quantification type selected across cohorts. Figure 12. Fractional enrichment with zero for ATP In case of dual labeled data, plots for each element can be visualized individually as shown in Figure 13 and Figure 14. Figure 13. Fractional enrichment with 0 for ATP and label type C Figure 14. Fractional enrichment with 0 for ATP and label type N Visualization of labels in stacked plot shows comparison between different labels present across the samples. Each bar in the chart represents the sample and the segments in the bar represent different isotopic labels of selected metabolite. Figure 15. Visualization of labels in stacked plot Visualization on the basis of pool totals shows the pool totals plot for the selected metabolite. Figure 16. Pool Total Plot Note: The pool total plot will be shown regardless of the quantification type selection. Generate Output File This tab allows you to generate the output workbook which consists of the Raw Intensities, Raw Abundances, Fractional Contribution and Corrected Isotopologues in four different worksheets where the metabolites belonging to a particular pathway are grouped together based on the metabolite-pathway mapping provided in the Library File uploaded. The various options available are provided below: Figure 17. Generate Output File Download Demo Library File: Clicking on this would download the demo library file containing the metabolite and pathway mapping. Upload Library File (.csv): The library file contains metabolite to pathway mapping information such as name of metabolite, pathway identifiers as supercategory and subcategory as shown in Figure 16. This file should be in .csv format. A detailed description of this file is provided below. Figure 18. Library File where, compound is the column that contains the names of the metabolites as in the El-MAVEN output formula is the column that contains the elemental formula of the metabolites id is the column that contains the compound ids of the metabolites rt is the column that contains the retention times of the metabolites mz is the column that contains the mzs of the metabolites category is the column that contains the pathway name to which the metabolite belongs to. The entries should ideally be pathwaynumber_pathwayname. Example: 01_Glycolysis subcategory is the column that contains the pathway number according to the Supercategory column followed by the metabolite number. Example: 9 for glutathione. Select whether to proceed with Fractional Enrichment or Fractional Enrichment with zero or NA Corrected or NA Corrected with zero values using Select a fractional enrichment and Select a NA correction drop down. Clicking on Run will generate a workbook that consists of the worksheets rawIntensities , rawAbundances , CorrectedIsotopologues and FracContribution with entries for all the metabolite isotopologues across different samples. Further, all the metabolite isotopologues are grouped based on the pathway they belong to (information provided in the library file). Figure 19. Output Workbook Details of the App Corna, the algorithm used for NA correction has the ability to auto-detect indistinguishable isotopes in experiments with dual tracers. Some high-resolution mass spectrometer instruments are able to distinguish isotopes like 13 C from 15 N. The packages available for performing natural abundance correction on these data assume infinite resolution, which means that it is assumed that if 13 C can be resolved from 15 N at all masses and it can also be resolved from other isotopes like 2 D. However, this assumption becomes invalid as the resolution of the machine varies with the mass of the metabolite. Corna does not assume ultra-high resolution and corrects for partial indistinguishability in the data. Fractional enrichment: Fractional enrichment is calculated for each label corresponding to every metabolite in a sample using the formula: Fractional enrichment = corrected intensity of a label / sum of corrected intensities for all labels Pool total: Sum total of corrected intensities of every isotopologue for each metabolite in a sample using the formula: pool total of a metabolite in a sample = sum (all corrected intensities for a metabolite) Calculations in the output workbook: rawIntensities : It consists of raw intensity values obtained from the NA Corrected output. rawAbundances : It consists of the pool total values obtained from the NA Corrected output. CorrectedIsotopologues : It consists of the fractional enrichment values obtained from the NA Corrected output in percentage FracContribution : It consists of the fractional contribution of the labelled carbons. It is calculated by the the following formula: Fractional contribution = Sum(Label * Fractional Enrichment)/ total number of carbons. Eg: for glycine, fractional contribution = (0 * FE0 + 1 * FE1 + 2 * FE2)/2, where FE = Fractional Enrichment Videos References Midani FS, Wynn ML, Schnell S. The importance of accurately correcting for the natural abundance of stable isotopes. Anal Biochem . 2017;520:27\u201343. doi:10.1016/j.ab.2016.12.011 Lide, D. R., \u201cCRC Handbook of Chemistry and Physics (83rd ed.). Boca Raton\u201d, FL: CRC Press. ISBN 0-8493-0483-0, 2002. Moseley H., \u201cCorrecting for the effects of natural abundance in stable isotope resolved metabolomics experiments involving ultra-high resolution mass spectrometry\u201d,BMC Bioinformatics, 2010, 11:139.","title":"Labeled LC-MS Workflow"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#overview","text":"The use of isotopically labeled tracer substrates is an experimental approach for measuring in vivo and in vitro intracellular metabolic dynamics. Stable isotopes that alter the mass but not the chemical behavior of a molecule are commonly used in isotope tracer studies. Because stable isotopes of some atoms naturally occur at non-negligible abundances, it is important to account for the natural abundance of these isotopes when analyzing data from isotope labeling experiments. Specifically, a distinction must be made between isotopes introduced experimentally via an isotopically labeled tracer and the isotopes naturally present at the start of an experiment. The Labeled LC-MS Workflow factors out the contribution from the Natural Abundance (NA) of each element from the signal of each isotopologue peak obtained from LC-MS techniques in a labeling experiment. It provides visualization for fractional enrichment and pool totals for single as well as dual-labeled data. It also allows you to download an output Excel Workbook with worksheets containing the raw abundances, raw intensities, fractional contributions and corrected isotopologues of the metabolites belonging to different pathways based on the library data provided.","title":"Overview"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#scope-of-the-app","text":"Performs NA correction on data from LC-MS experiments. Supports data from single ( 2 D, 13 C, 15 N and 34 S) as well as dual-labeled experiments. Plots fractional enrichment, pool total and NA corrected intensity plots for a selected metabolite. Generates a consolidated Excel workbook of outputs. Figure 1. Labeled LC-MS Workflow","title":"Scope of the app"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#user-input","text":"Labeled LC-MS Workflow requires the following two files as input: Intensity file The intensity file should be in CSV format as shown in Figure 1. The .csv file exported after peak picking in El-MAVEN is the input file. Figure 2. El-MAVEN output file Cohort file The cohort file is optional and is used for generating visualizations. In case you only want to perform NA correction, you can skip this step. The cohort file should be in .csv format as shown in Figure 3. This file should contain two columns, Sample containing sample names along with Cohort for its cohort information. Figure 3. Cohort file","title":"User Input"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#steps-involved-in-data-processing","text":"Upload intensity file obtained from El-MAVEN Set analysis parameters and perform NA correction Download NA corrected output files Visualize fractional enrichment, pool total and NA corrected intensity plots Download consolidated output workbook using a library data","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#caveats","text":"All the samples present in the intensity files should also be present in the cohort file. The cohort file is optional but visualizations will only be generated if this file is provided. If using the Edit NA Option , the natural abundances of all the isotopes of an element should sum to 1. The library file is not required for NA Correction. The name of the metabolites should match exactly between the input and the library file.","title":"Caveats"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#upload-files","text":"The Upload Files tab allows you to upload the input files required for processing through the app. Once the files have been uploaded, click on Run. This will validate and process the uploaded files and give error messages or warnings if the format or data in files are incorrect. You can also run the demo files directly by clicking on Run Demo Files . Figure 4. Upload interface","title":"Upload files"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#cohort-creation-interface","text":"If you do not wish to create a cohort file, you can utilize the cohort creation interface that allows you to enter the cohort information on the app itself. Upload your intensity file and uncheck Upload Cohort File option. Once the intensity file is uploaded, click on Run and a cohort creation interface with sample names with input files will open as shown in Figure 4. After entering the cohort information, click on NA Correction on the top left side to be taken to the NA correction interface. Figure 5. Cohort creation interface","title":"Cohort Creation Interface"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#na-correction","text":"Isotopologue identification and quantification of thousands of metabolites in the metabolomic experiments can provide a wealth of data for modeling the flux through metabolic networks. But before isotopologue intensity data can be properly interpreted, the contributions from isotopic natural abundance must be factored out (deisotoped). This is referred to as natural abundance (NA) correction. The various analysis parameters for performing NA Correction shown in Figure 5 are defined below: Figure 6. NA Correction Tab Indistinguishable isotopes : Indistinguishable isotopes can be corrected for natural abundance by electing either of the following options: Manually: You can manually select a standard list of indistinguishable isotopes with respect to the labelled isotope for NA correction. Using PPM: You can enable auto detection of indistinguishable isotopes, by selecting Using PPM option and providing PPM resolution of the mass spectrometer used for the experiment. Edit NA values: Default NA values used in the tool are taken from CRC Handbook of Chemistry and Physics. You can change these values by clicking on Edit NA Values . as shown in Figure 6. Figure 7. Edit NA values Replace negatives with zero: Sometimes negative intensities arise which can be replaced with zero. This option is selected by default. After all parameters have been decided click on Perform NA Correction . Once NA correction has been performed, the output will be displayed in in a tabular format as shown in Figure 7. There are three sections on this interface; NA Corrected Values , Fractional Enrichment and Merged Output . The first two sections only display the NA corrected values and fractional enrichment respectively as shown in Figure 7 and Figure 8 whereas the Third section combines these information to provide you with a single file with the entire information as shown in Figure 9. A detailed description of the merged output file is provided below. Figure 8. NA Corrected Values Figure 9. Fractional Enrichment Figure 10. Merged Output where, Name is Name of the metabolite Label is Label information Formula is Chemical formula Sample is Sample names Intensity is Raw intensities NA Corrected is NA corrected intensities NA Corrected with zero is NA corrected intensities with negative intensities replaced with zero Indistinguishable isotopes is List of indistinguishable isotopes for the particular metabolite Pool total is Pool total of a metabolite in a sample Fractional Enrichment is Fractional enrichment calculated after NA Correction has been done Fractional Enrichment with zero is Negative values are replaced with zero compoundid is KEGG or HMDB ID of the metabolites as specified oin the input file","title":"NA Correction"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#visualization","text":"The visualization tab provides fractional enrichment, pool total and NA corrected intensity plots that are generated according to the metabolite and quantification type specified before. Figure 11. Visualization Interface To generate visualizations, select a metabolite using the drop down that contains metabolites present in the intensity file uploaded and a quantification type from Intensity , NA Corrected , NA Corrected with zero , Fractional Enrichment and Fractional Enrichment with zero . After this, click on Run to get the selected visualization. Visualization on the basis of the number of labels shows all the available labels of a metabolite based on the quantification type selected across cohorts. Figure 12. Fractional enrichment with zero for ATP In case of dual labeled data, plots for each element can be visualized individually as shown in Figure 13 and Figure 14. Figure 13. Fractional enrichment with 0 for ATP and label type C Figure 14. Fractional enrichment with 0 for ATP and label type N Visualization of labels in stacked plot shows comparison between different labels present across the samples. Each bar in the chart represents the sample and the segments in the bar represent different isotopic labels of selected metabolite. Figure 15. Visualization of labels in stacked plot Visualization on the basis of pool totals shows the pool totals plot for the selected metabolite. Figure 16. Pool Total Plot Note: The pool total plot will be shown regardless of the quantification type selection.","title":"Visualization"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#generate-output-file","text":"This tab allows you to generate the output workbook which consists of the Raw Intensities, Raw Abundances, Fractional Contribution and Corrected Isotopologues in four different worksheets where the metabolites belonging to a particular pathway are grouped together based on the metabolite-pathway mapping provided in the Library File uploaded. The various options available are provided below: Figure 17. Generate Output File Download Demo Library File: Clicking on this would download the demo library file containing the metabolite and pathway mapping. Upload Library File (.csv): The library file contains metabolite to pathway mapping information such as name of metabolite, pathway identifiers as supercategory and subcategory as shown in Figure 16. This file should be in .csv format. A detailed description of this file is provided below. Figure 18. Library File where, compound is the column that contains the names of the metabolites as in the El-MAVEN output formula is the column that contains the elemental formula of the metabolites id is the column that contains the compound ids of the metabolites rt is the column that contains the retention times of the metabolites mz is the column that contains the mzs of the metabolites category is the column that contains the pathway name to which the metabolite belongs to. The entries should ideally be pathwaynumber_pathwayname. Example: 01_Glycolysis subcategory is the column that contains the pathway number according to the Supercategory column followed by the metabolite number. Example: 9 for glutathione. Select whether to proceed with Fractional Enrichment or Fractional Enrichment with zero or NA Corrected or NA Corrected with zero values using Select a fractional enrichment and Select a NA correction drop down. Clicking on Run will generate a workbook that consists of the worksheets rawIntensities , rawAbundances , CorrectedIsotopologues and FracContribution with entries for all the metabolite isotopologues across different samples. Further, all the metabolite isotopologues are grouped based on the pathway they belong to (information provided in the library file). Figure 19. Output Workbook","title":"Generate Output File"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#details-of-the-app","text":"Corna, the algorithm used for NA correction has the ability to auto-detect indistinguishable isotopes in experiments with dual tracers. Some high-resolution mass spectrometer instruments are able to distinguish isotopes like 13 C from 15 N. The packages available for performing natural abundance correction on these data assume infinite resolution, which means that it is assumed that if 13 C can be resolved from 15 N at all masses and it can also be resolved from other isotopes like 2 D. However, this assumption becomes invalid as the resolution of the machine varies with the mass of the metabolite. Corna does not assume ultra-high resolution and corrects for partial indistinguishability in the data. Fractional enrichment: Fractional enrichment is calculated for each label corresponding to every metabolite in a sample using the formula: Fractional enrichment = corrected intensity of a label / sum of corrected intensities for all labels Pool total: Sum total of corrected intensities of every isotopologue for each metabolite in a sample using the formula: pool total of a metabolite in a sample = sum (all corrected intensities for a metabolite) Calculations in the output workbook: rawIntensities : It consists of raw intensity values obtained from the NA Corrected output. rawAbundances : It consists of the pool total values obtained from the NA Corrected output. CorrectedIsotopologues : It consists of the fractional enrichment values obtained from the NA Corrected output in percentage FracContribution : It consists of the fractional contribution of the labelled carbons. It is calculated by the the following formula: Fractional contribution = Sum(Label * Fractional Enrichment)/ total number of carbons. Eg: for glycine, fractional contribution = (0 * FE0 + 1 * FE1 + 2 * FE2)/2, where FE = Fractional Enrichment","title":"Details of the App"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#videos","text":"","title":"Videos"},{"location":"Apps/Metabolomic Data/Labeled LC-MS Workflow.html#references","text":"Midani FS, Wynn ML, Schnell S. The importance of accurately correcting for the natural abundance of stable isotopes. Anal Biochem . 2017;520:27\u201343. doi:10.1016/j.ab.2016.12.011 Lide, D. R., \u201cCRC Handbook of Chemistry and Physics (83rd ed.). Boca Raton\u201d, FL: CRC Press. ISBN 0-8493-0483-0, 2002. Moseley H., \u201cCorrecting for the effects of natural abundance in stable isotope resolved metabolomics experiments involving ultra-high resolution mass spectrometry\u201d,BMC Bioinformatics, 2010, 11:139.","title":"References"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html","text":"Introduction Overview The Labeled LC-MS/MS Workflow tool factors out the contribution from the natural abundance of each element from the signal of each isotopologue peak obtained from LC-MS techniques in a labeling experiment. It also allows you to visualize plots for NA corrected, fractional enrichment and pool totals for single as well as dual labeled data. The output of the natural abundance correction step is a table containing the raw abundances, fractional contributions and corrected isotopologues of the metabolites, calculated based on the library data provided. Scope of the tool The tool supports data from LC-MS/MS and MRM/SRM experiments. Supports data from single as well as dual labeled experiments. Corrects data from 13 C labeled experiments Plot fractional enrichment and pool total for a selected metabolite. It performs Phi calculations. Figure 1. Workflow Getting Started User Input Labeled LC-MS/MS Workflow requrires three types of input files: El-MAVEN Output This file is the .csv output from El-MAVEN in peak detailed format. Column Name Description Required/Optional Validation Checks Original Filename This column contains the name of the samples Required No format constraints Component Name This column contains the fragment names Required Should be a non-negative number or NA Area It contains the intensities values Required string of form parent mass/daughter mass Mass Info It contains mass of parent fragment and daughter fragment \u201cParent Fragment Mass/Fragment Mass\u201d Required string of form parent mass/daughter mass Sample Name It contains cohort names. Required to identify cohort replicates. example A 0 min SCS, 10 min etc. Required only if background correction is needed No format constraints here Figure 2. El-MAVEN output file Fragment Mapping Metadata The fragment mapping metadata is a .csv, .xls, .xlsx or .txt file that contains mapping information for fragments, their metabolites, isotopic tracer, parent formula and daughter formula. This file's name should start with \"metadata_mq\". Column Name Description Required/Optional Validation Checks Component Name This column contains entries from \"Component Name\" column of the raw Multiquant file Required* No format constraints here Unlabeled Fragment The component name of the fragment which is completely unlabeled. example Citrate has two Unlabeled Fragments in the sample input data Citrate 191/111 (C5H3O3) and Citrate 191/67(C4H3O). Both are processed separately Example: metab, citrate, citrate 191/111 Required* No format constraints here Formula This column contains the chemical formula of the daughter fragment written in the format C4H3O Required* Validates formula by checking if elements in periodic table and atom numbers are integers Parent Formula This column contains the chemical formula of the parent fragment Required* Validates formula by checking if elements in periodic table and atom numbers are integers Isotopic Tracer This column contains the iso tracer. Example: if the iso tracer is carbon 13, it is written as C13 Required* Checks if the iso tracer is correctly Figure 3. Fragment mapping metadata file Background Mapping Metadata The background mapping metadata is a .csv, .xls, .xlsx or .txt file that contains sample metadata like names, background sample, cohort, etc. This file's name should start with \"metadata_sample\". As this file is used for background correction, it is optional. Column Name Description Required/Optional Validation Checks Original Filename This column contains the name of the samples. Each sample name occurs only once in this file and the names come from Original Filename column of raw multiquant file Required* Unique sample names, intersection with multiquant file not empty Sample Name This column contains cohort names. It is needed to identify the replicates of each cohort. Required only if background correction needed No format constraints here Any number of additional columns These columns contain metadata associated with sample names Optional Background Sample It contains the sample which is to be taken as reference for background correction Required only if background correction is needed The entries in this column are subsets of Original Filename Figure 4. Background mapping metadata file Steps involved in data processing Prepare data files Upload data files Set analysis parameters and perform natural abundance correction Download corrected output files Visualize fractional enrichment and pool total plots Perform Phi calculation by uploading NA corrected file Caveats The intensity file should be the El-MAVEN outout in peak detailed format. The fragment mapping metadata file's name should start with \"metadata_mq\". The backround mapping metadata file's name should start with \"metadata_sample\". Tutorial Select Labeled LC-MS/MS Workflow from the dashboard under the Metabolomics Data Tab as shown in Figure 5. Create a New Workspace or choose from the existing ones from the drop-down and provide the Name of the Session to be redirected to Labeled LC-MS/MS Workflow's upload page. Figure 5. Polly Dashboard and Labeled LC-MS/MS Workflow Upload Files Click on Upload El-MAVEN Output and Upload Cohort FIle to upload the intensity and metadata files respectively. y or from Polly. This is optional. The metadata is not used in the NA correction, however, if you wants to visualize plots, the sample-cohort mapping would be required. Click on Run to proceed. Figure 6. Upload interface Figure 7. Upload files from the target folder Figure 8. Check/uncheck the option to upload Metadata Sample file If you do not upload a metadata file and upload a metadata sample file you will be provided with the option to Drop Samples as well as Map Replicate to Background Samples . Figure 9. Drop Samples and Map Replicates to Background Samples option To Drop Samples check the drop samples checkbox and select sample(s) to drop and hit Drop Samples . Figure 10. Drop Samples option To replicate samples to background samples, select the Replicate Cohort and Background Sample Cohort and hit Update metadata . Figure 11. Map Replicates to Background Samples NA Correction You can navigate to NA correction tab by either clicking on Go to NA Correction option or by selecting the NA Correction tab. There are two options to perform NA correction: Check Perform Background Correction checkbox to perform background correction. Replace negatives with zero is selected by ddefault. This option replaces any negative intensities obtained with 0. Note: The sum of natural abundances of all the isotopes of an element should be 1. After selecting the parameters, click on Perform NA Correction , to get the NA corrected intensities. Figure 12. NA Correction interface Figure 13. NA Corrected values Output NA Correction data file Column Name Description Required/Optional Validation Checks Name Name of the unlabelled fragment metadata_mq Unlabelled fragment Label It gives label information by the entries of type isotracer_parentmass_daughtermass Example: C13_123_23 Entries come from Isotracer column of metadata_mq masses from Mass Info of raw multiquant Formula Contains chemical formula of the daughter fragment metadata_mq Sample Contains sample name Raw intensity file Original filename NA corrected NA corrected intensities NA Corrected with 0 Negative values are replaced with zero Background Correction data file Column Name Description Required/Optional Validation Checks Name Name of the unlabelled fragment metadata_mq Unlabelled fragment Formula Contains chemical formula of the daughter fragment metadata_mq Label It gives label information by the entries of type isotracer_parentmass_daughtermass Example: C13_123_23 Entries come from Isotracer column of metadata_mq masses from Mass Info of raw multiquant Original filename Sample Contains sample name Raw intensity file Background corrected NA corrected intensities Background Corrected with 0 Negative values are replaced with zero Fractional enrichment data file Column Name Description Required/Optional Validation Checks Name Name of the unlabelled fragment metadata_mq Unlabelled fragment Formula Contains chemical formula of the daughter fragment metadata_mq Label It gives label information by the entries of type isotracer_parentmass_daughtermass Example: C13_123_23 Entries come from Isotracer column of metadata_mq masses from Mass Info of raw multiquant Sample Contains sample name Raw intensity file Original file name Fractional enrichment NA corrected intensities Merged Output Column Name Description Required/Optional Validation Checks Sample Contains sample name Raw intensity file Original file name Parent Formula Contains formula of the parent fragment metadata_mq Parent Formula Name Name of the unlabelled fragment metadata_mq Unlabelled fragment NA Corrected NA corrected intensities Entries come from Isotracer column of metadata_mq Label 14\">It gives label information by the entries of type isotracer_parentmass_daughtermassc3\">: C13_123_23 Entries come from Isotracer column of metadata_mqmasses from Mass Info of raw multiquant Intensities Contains raw intensities Raw intensity file Area Formula Contains chemical formula of the daughter fragment metadata_mq Enrichment Fractional enrichment values Component name Contains the name of the fragments Raw intensity file Component name Background corrected Background corrected intensities Pool total pool total of a metabolite in a sample Figure 14. Merged Output Visualization After NA correction, you can visualize the NA corrected intensities, fractional enrichment and pool total plots that are generated according to the metabolite and quantification type. Select component type: This allows you to select the component types: Component name, Isotopologue or Pool total for the plot. Select a metabolite : This allows you to select a metabolite from the drop down options of all metabolites detected as per the intensity file uploaded. Select quantification type : This allows you to select a quantification type from the drop down options of Intensity , NA corrected , NA corrected with zero , Fractional Enrichment and Fractional Enrichment with zero . The default value, in this case, is NA corrected . Select the cohort order to display: Here you can select which cohort(s) to display. The tab by default shows all the cohorts. Click \"Run* to plot the fractional enrichment plot with all the labels present and for separate label elements and the pool totals plot. Figure 15. Visualization interface Figure 16. Visualization plot Phi BETA Tab This tab enables you to perform Phi analysis. The term 'Phi' denoted by '\u03c6' is ratio of fractional enrichments of isotopomers/isotopologues of a product to the fractional enrichments of isotopomers/isotopologues of a precursor in the pathway. Consider, , then , which means if there is another flux contributing to the generation of B, then other pathway contributes to FE B* , in this case, \ud835\udef7AB < 1 For eg: In the image shown below, Acetyl-CoA is generated from Pyruvate and with some contribution from Fatty acids. Then, \ud835\udf31PAc < 1 as Acetyl-CoA is also formed from Fatty Acids. If the only contributor is Pyruvate then \ud835\udf31PAc = 1 Figure 17. Acetyl CoA is generated from Pyruvate and Fatty acids Calculate Phi: This allows you to set the parameters to calculate the Phi values based on the formula specified above. You can upload NA corrected file if it has been performed externally. In case, NA correction is performed within the app, there is no need to upload the NA corrected file and you can directly perform Phi calculation. Figure 18. Calculate Phi Select the following options for Phi calculation: Glucose labeled (for TCA cycle): In case the data is 13 C Glucose labeled, you can select this option for calculating the Phi. This will point to the metabolites of the TCA cycle contributing to the generation of that metabolite. Upload your Glucose label identifier file: By default, identifiers file with the expressions present within the app is used. If you want to make use of the identifier file for Glucose with additional expressions, you can upload the Phi expression file. Here, you will be provided with the option to upload Phi Expression File and Intermediate Expression File. Figure 19. Upload Glucose label identifier file Figure 20. Glucose Phi expressions and intermediate expressions identifier file formats Any generic label (eg: Glutamine Labeled): In case you have any other generic label data other than Glucose, you can make use of this option. The app will perform Phi calculation in the same way as for 13 C Glucose labeled elements. Upload your generic label identifier file: The app by default has the identifier expressions file for glutamine label. If at all, you have the identifier file for glutamine with additional expressions or any other generic label, you can select onto this option and further upload the identifier expression file for the same. Figure 21. Upload Generic label identifier file Figure 22. Generic label Phi expressions identifier file format Upload your metadata mq with identifier file: The app by default has the metadata mq identifier loaded, but in case you have made use of any additional expressions for the above parameters, you will have to upload your own metadata mq identifier file. Keeping it unchecked would utilize the metadata mq file within the app. Figure 23. Metadata mq identifier file format Subset by cohorts for glucose labeled (for TCA Cycle) : You can subset the specific cohorts that belong to the Glucose labeled (for TCA Cycle) to proceed with the Phi calculation. You can type out a characteristic string that denotes the particular cohort and select onto the required cohorts from the dropdown. Subset by cohorts for generic labeled : You can subset the specific cohorts that belong to the belong to generic labeled to proceed with the Phi calculation. You can type out a characteristic string that denotes the particular cohort and select onto the required cohorts from the dropdown. After uploading the necessary files, click on Calculate Phi . Results: Glucose Labeled: This tab contains the output to Phi calculation performed on 13 C Glucose samples. It consists of the following sub-tabs namely: Calculated Phis: This sub-tab contains a table of Phi values belonging to each identifier across the different samples. You can download the data as a CSV file as well. Figure 24. Calculated Phis Absent Phis: This sub-tab contains a downloadable list of absent Phi values that could not be calculated possibly because of missing fragments withing the data provided. Identifier Expression: This sub-tab displays the list of identifier formulas used to calculate Phi values for the Glucose labeled data. Figure 25. Identifier Expressions Visualization: The Visualization sub-tab consists of the type of visualizations to view the Phi of identifiers across the cohorts. Select Individual Plot or Multi-select Plot and enter cohorts to view in the visualization space. Further, you can select the identifiers of interest from the dropdown provided under Select the identifier option. You can as well specify the cohorts order. Figure 26. Individual plot visualization Figure 27. Multi-select plot Results: Generic Labeled: In case, you have selected generic labels i.e., other than 13 C Glucose, this tab contains the output to Phi calculation performed on the other samples. It consists of the following sub-tabs namely: Calculated Phis: This sub-tab contains a downloadable list of Phi values that have been calculated for each identifier. Figure 28. Calculated Phis (generic label) Absent Phis: This sub-tab contains a downloadable list of absent Phi values that the app could not match with the identifier table. Identifier Expression: This sub-tab displays the list of identifier formulas used to calculate Phi values for the generic labeled data. Figure 29. Identifier Expression (generic label) Visualization: The Visualization sub-tab consists of the type of visualizations to view the Phi of identifiers across the cohorts. Select Individual Plot or Multi-select Plot and enter cohorts to view in the visualization space. Further, you can select the identifiers of interest from the dropdown provided under Select the identifier option. You can as well specify the cohorts order. Figure 30. Individual Plot (generic label) Figure 31. Multi-select plot (generic label) Details about the app Autodetection of indistinguishable isotope A new feature in Corna has the ability to auto-detect indistinguishable isotopes in experiments with dual tracers. Some high-resolution mass spectrometer instruments are able to distinguish isotopes like 13 C from 15 N. The packages available for performing natural abundance correction on these data assume infinite resolution, which means that it is assumed that if 13 C can be resolved from 15 N at all masses and it can also be resolved from other isotopes like 2 D. However, this assumption becomes invalid as the resolution of the machine varies with the mass of the metabolite. Corna does not assume ultra-high resolution and corrects for partial indistinguishability in the data. Fractional enrichment Fractional enrichment is calculated for each label corresponding to every metabolite in a sample. Fractional enrichment = corrected intensity of a label / sum of corrected intensities for all isotopologues including PARENT Pool total Sum total of intensities of every isotopologue (label) for each metabolite in a sample. Pool total of metabolite \u2018m\u2019 in sample 1 = Sum (intensities of all labels of \u2018m\u2019 in sample 1) References Lide, D. R., \u201cCRC Handbook of Chemistry and Physics (83rd ed.). Boca Raton\u201d, FL: CRC Press. ISBN 0-8493-0483-0, 2002. Moseley H., \u201cCorrecting for the effects of natural abundance in stable isotope resolved metabolomics experiments involving ultra-high resolution mass spectrometry\u201d,BMC Bioinformatics, 2010, 11:139.","title":"Labeled LC-MS/MS Workfow"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#overview","text":"The Labeled LC-MS/MS Workflow tool factors out the contribution from the natural abundance of each element from the signal of each isotopologue peak obtained from LC-MS techniques in a labeling experiment. It also allows you to visualize plots for NA corrected, fractional enrichment and pool totals for single as well as dual labeled data. The output of the natural abundance correction step is a table containing the raw abundances, fractional contributions and corrected isotopologues of the metabolites, calculated based on the library data provided.","title":"Overview"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#scope-of-the-tool","text":"The tool supports data from LC-MS/MS and MRM/SRM experiments. Supports data from single as well as dual labeled experiments. Corrects data from 13 C labeled experiments Plot fractional enrichment and pool total for a selected metabolite. It performs Phi calculations. Figure 1. Workflow","title":"Scope of the tool"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#user-input","text":"Labeled LC-MS/MS Workflow requrires three types of input files: El-MAVEN Output This file is the .csv output from El-MAVEN in peak detailed format.","title":"User Input"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#steps-involved-in-data-processing","text":"Prepare data files Upload data files Set analysis parameters and perform natural abundance correction Download corrected output files Visualize fractional enrichment and pool total plots Perform Phi calculation by uploading NA corrected file","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#caveats","text":"The intensity file should be the El-MAVEN outout in peak detailed format. The fragment mapping metadata file's name should start with \"metadata_mq\". The backround mapping metadata file's name should start with \"metadata_sample\".","title":"Caveats"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#tutorial","text":"Select Labeled LC-MS/MS Workflow from the dashboard under the Metabolomics Data Tab as shown in Figure 5. Create a New Workspace or choose from the existing ones from the drop-down and provide the Name of the Session to be redirected to Labeled LC-MS/MS Workflow's upload page. Figure 5. Polly Dashboard and Labeled LC-MS/MS Workflow","title":"Tutorial"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#upload-files","text":"Click on Upload El-MAVEN Output and Upload Cohort FIle to upload the intensity and metadata files respectively. y or from Polly. This is optional. The metadata is not used in the NA correction, however, if you wants to visualize plots, the sample-cohort mapping would be required. Click on Run to proceed. Figure 6. Upload interface Figure 7. Upload files from the target folder Figure 8. Check/uncheck the option to upload Metadata Sample file If you do not upload a metadata file and upload a metadata sample file you will be provided with the option to Drop Samples as well as Map Replicate to Background Samples . Figure 9. Drop Samples and Map Replicates to Background Samples option To Drop Samples check the drop samples checkbox and select sample(s) to drop and hit Drop Samples . Figure 10. Drop Samples option To replicate samples to background samples, select the Replicate Cohort and Background Sample Cohort and hit Update metadata . Figure 11. Map Replicates to Background Samples","title":"Upload Files"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#na-correction","text":"You can navigate to NA correction tab by either clicking on Go to NA Correction option or by selecting the NA Correction tab. There are two options to perform NA correction: Check Perform Background Correction checkbox to perform background correction. Replace negatives with zero is selected by ddefault. This option replaces any negative intensities obtained with 0. Note: The sum of natural abundances of all the isotopes of an element should be 1. After selecting the parameters, click on Perform NA Correction , to get the NA corrected intensities. Figure 12. NA Correction interface Figure 13. NA Corrected values","title":"NA Correction"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#output","text":"NA Correction data file","title":"Output"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#visualization","text":"After NA correction, you can visualize the NA corrected intensities, fractional enrichment and pool total plots that are generated according to the metabolite and quantification type. Select component type: This allows you to select the component types: Component name, Isotopologue or Pool total for the plot. Select a metabolite : This allows you to select a metabolite from the drop down options of all metabolites detected as per the intensity file uploaded. Select quantification type : This allows you to select a quantification type from the drop down options of Intensity , NA corrected , NA corrected with zero , Fractional Enrichment and Fractional Enrichment with zero . The default value, in this case, is NA corrected . Select the cohort order to display: Here you can select which cohort(s) to display. The tab by default shows all the cohorts. Click \"Run* to plot the fractional enrichment plot with all the labels present and for separate label elements and the pool totals plot. Figure 15. Visualization interface Figure 16. Visualization plot","title":"Visualization"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#phibeta-tab","text":"This tab enables you to perform Phi analysis. The term 'Phi' denoted by '\u03c6' is ratio of fractional enrichments of isotopomers/isotopologues of a product to the fractional enrichments of isotopomers/isotopologues of a precursor in the pathway. Consider, , then , which means if there is another flux contributing to the generation of B, then other pathway contributes to FE B* , in this case, \ud835\udef7AB < 1 For eg: In the image shown below, Acetyl-CoA is generated from Pyruvate and with some contribution from Fatty acids. Then, \ud835\udf31PAc < 1 as Acetyl-CoA is also formed from Fatty Acids. If the only contributor is Pyruvate then \ud835\udf31PAc = 1 Figure 17. Acetyl CoA is generated from Pyruvate and Fatty acids Calculate Phi: This allows you to set the parameters to calculate the Phi values based on the formula specified above. You can upload NA corrected file if it has been performed externally. In case, NA correction is performed within the app, there is no need to upload the NA corrected file and you can directly perform Phi calculation. Figure 18. Calculate Phi Select the following options for Phi calculation: Glucose labeled (for TCA cycle): In case the data is 13 C Glucose labeled, you can select this option for calculating the Phi. This will point to the metabolites of the TCA cycle contributing to the generation of that metabolite. Upload your Glucose label identifier file: By default, identifiers file with the expressions present within the app is used. If you want to make use of the identifier file for Glucose with additional expressions, you can upload the Phi expression file. Here, you will be provided with the option to upload Phi Expression File and Intermediate Expression File. Figure 19. Upload Glucose label identifier file Figure 20. Glucose Phi expressions and intermediate expressions identifier file formats Any generic label (eg: Glutamine Labeled): In case you have any other generic label data other than Glucose, you can make use of this option. The app will perform Phi calculation in the same way as for 13 C Glucose labeled elements. Upload your generic label identifier file: The app by default has the identifier expressions file for glutamine label. If at all, you have the identifier file for glutamine with additional expressions or any other generic label, you can select onto this option and further upload the identifier expression file for the same. Figure 21. Upload Generic label identifier file Figure 22. Generic label Phi expressions identifier file format Upload your metadata mq with identifier file: The app by default has the metadata mq identifier loaded, but in case you have made use of any additional expressions for the above parameters, you will have to upload your own metadata mq identifier file. Keeping it unchecked would utilize the metadata mq file within the app. Figure 23. Metadata mq identifier file format Subset by cohorts for glucose labeled (for TCA Cycle) : You can subset the specific cohorts that belong to the Glucose labeled (for TCA Cycle) to proceed with the Phi calculation. You can type out a characteristic string that denotes the particular cohort and select onto the required cohorts from the dropdown. Subset by cohorts for generic labeled : You can subset the specific cohorts that belong to the belong to generic labeled to proceed with the Phi calculation. You can type out a characteristic string that denotes the particular cohort and select onto the required cohorts from the dropdown. After uploading the necessary files, click on Calculate Phi . Results: Glucose Labeled: This tab contains the output to Phi calculation performed on 13 C Glucose samples. It consists of the following sub-tabs namely: Calculated Phis: This sub-tab contains a table of Phi values belonging to each identifier across the different samples. You can download the data as a CSV file as well. Figure 24. Calculated Phis Absent Phis: This sub-tab contains a downloadable list of absent Phi values that could not be calculated possibly because of missing fragments withing the data provided. Identifier Expression: This sub-tab displays the list of identifier formulas used to calculate Phi values for the Glucose labeled data. Figure 25. Identifier Expressions Visualization: The Visualization sub-tab consists of the type of visualizations to view the Phi of identifiers across the cohorts. Select Individual Plot or Multi-select Plot and enter cohorts to view in the visualization space. Further, you can select the identifiers of interest from the dropdown provided under Select the identifier option. You can as well specify the cohorts order. Figure 26. Individual plot visualization Figure 27. Multi-select plot Results: Generic Labeled: In case, you have selected generic labels i.e., other than 13 C Glucose, this tab contains the output to Phi calculation performed on the other samples. It consists of the following sub-tabs namely: Calculated Phis: This sub-tab contains a downloadable list of Phi values that have been calculated for each identifier. Figure 28. Calculated Phis (generic label) Absent Phis: This sub-tab contains a downloadable list of absent Phi values that the app could not match with the identifier table. Identifier Expression: This sub-tab displays the list of identifier formulas used to calculate Phi values for the generic labeled data. Figure 29. Identifier Expression (generic label) Visualization: The Visualization sub-tab consists of the type of visualizations to view the Phi of identifiers across the cohorts. Select Individual Plot or Multi-select Plot and enter cohorts to view in the visualization space. Further, you can select the identifiers of interest from the dropdown provided under Select the identifier option. You can as well specify the cohorts order. Figure 30. Individual Plot (generic label) Figure 31. Multi-select plot (generic label)","title":"PhiBETA Tab"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#details-about-the-app","text":"Autodetection of indistinguishable isotope A new feature in Corna has the ability to auto-detect indistinguishable isotopes in experiments with dual tracers. Some high-resolution mass spectrometer instruments are able to distinguish isotopes like 13 C from 15 N. The packages available for performing natural abundance correction on these data assume infinite resolution, which means that it is assumed that if 13 C can be resolved from 15 N at all masses and it can also be resolved from other isotopes like 2 D. However, this assumption becomes invalid as the resolution of the machine varies with the mass of the metabolite. Corna does not assume ultra-high resolution and corrects for partial indistinguishability in the data. Fractional enrichment Fractional enrichment is calculated for each label corresponding to every metabolite in a sample. Fractional enrichment = corrected intensity of a label / sum of corrected intensities for all isotopologues including PARENT Pool total Sum total of intensities of every isotopologue (label) for each metabolite in a sample. Pool total of metabolite \u2018m\u2019 in sample 1 = Sum (intensities of all labels of \u2018m\u2019 in sample 1)","title":"Details about the app"},{"location":"Apps/Metabolomic Data/Labeled LC-MSMS Workflow.html#references","text":"Lide, D. R., \u201cCRC Handbook of Chemistry and Physics (83rd ed.). Boca Raton\u201d, FL: CRC Press. ISBN 0-8493-0483-0, 2002. Moseley H., \u201cCorrecting for the effects of natural abundance in stable isotope resolved metabolomics experiments involving ultra-high resolution mass spectrometry\u201d,BMC Bioinformatics, 2010, 11:139.","title":"References"},{"location":"Apps/Metabolomic Data/MetScape.html","text":"Introduction Overview Metabolomics is one of the key research areas focusing on small molecules present in a tissue or cell. MetScape Workflow allows you to process unlabeled LC-MS targeted, semi-targeted (without retention time) and untargeted data with insightful visualizations. You can directly upload CSV files obtained after peak detection in El-MAVEN along with the cohort information. The Polly Notebook enables you to use in-built scripts for normalizing, transforming and visualizing data along with the ability to upload and use your custom scripts. The pathway dashboard allows you to track differentially expressed metabolites across cohorts and form hypothesis on not only individual metabolites but on pathways as well. Scope of the app Supports analysis of targeted LC-MS data. Processes high-resolution and low-resolution data of up to 100 samples. Allows preprocessing of data through Polly Notebook. Perform PCA and differential expression analysis. Visualize significant metabolites on a global pathway map. Use the differentially expressed results with the output of RNA Seq Workflow to integrate multi-omics data in IntOmix . Figure 1. MetScape Getting Started User Input MetScape requires the following two files as input: Intensity file The intensity file should be in .csv format as shown in Figure 2. The .csv file exported after peak picking in El-MAVEN is the input file. For untargeted analysis, the output of Untargeted Pipeline serves as the input. Figure 2. Sample intensity file Cohort file The cohort file should be in .csv format as shown in Figure 3. This file should contain two columns, sample containing sample names along with cohort for its cohort information. Figure 3. Sample cohort file Steps involved in data processing Perform custom normalization, transforming and visualization Perform differential expression analysis using limma (R-Bioconductor package) and visualize it using a volcano plot Identify and reject sample outliers, if any from PCA plot Perform sample based visualizations on Polly Notebook Identify significant pathways and metabolites on the pathway dashboard Move back and forth between various components of the workflow Caveats The intensity values should not be NA or NaN for any metabolite. There should not be any blank cell for the intensities. Retention time and m/z information both are required to run the analysis. The metabolites in the input file must have either KEGG IDs or HMDB IDs to map on the pathway dashboard. All the samples present in the intensity files should also be present in the metadata file. Each cohort must have at least three samples to successfully perform differential expression. In the case of dual-mode data, the metabolite name should contain the polarity of the metabolite in the format \"MetaboliteName##polarity\" , ie. Glucose##pos or Fructose##neg. During the identification of significant pathways, if the data consists, where the data consists of duplicates of metabolite features, they are filtered out on the basis of the parameters specified for significance. If at all, there are multiple occurrences of a metabolite, the one with the most significant p -value is mapped on the pathway dashboard. You can always revisit the duplicates from the View Duplicates option. Differential expression will be performed on all possible cohort comparisons. An analysis can not be restored simultaneously using two different tabs. Tutorial Upload files Select MetScape Workflow from the dashboard under the Metabolomics Data Tab as shown in Figure 4. Create a New Workspace or select an existing one from the drop-down and provide the Name of the Session to be redirected to MetScape's upload page. Figure 4. Polly Dashboard and Workspace selection After the files have been successfully uploaded as shown in Figure 5, click on Proceed to move to Polly Notebook. Figure 5. Upload page Polly Notebook The Polly Notebook consists of a list of in-built scripts which can be selected from the toolbox present on the left-hand side as we can see in Figure 6. Figure 6. Polly Notebook showing the visualization of sample distribution The function of these in-built scripts is mentioned below: Go To Dashboard: Clicking on this will take you to the next step Control Normalization: This method is used to perform normalization with respect to Control cohort. df < -control_normalisation(mavenData,metaData,'Control') Cyclic Loess Normalization: To incorporate for negative intensity values post normalization, all intensity values are scaled so the least value is 1. df < -cyclic_loess(mavenData) Log 2 Transformation: To incorporate for 0 intensity values, 1 is added to all intensity values pre-normalization. df < -log_transform(df) Sample View Visualization: This function is used to get the visualization of the intensity file for both pre- and post-normalization. sample_view(mavenData) Click here for a detailed documentation about Polly Notebook. Note: The last data saved after transformation or normalization in df is taken forward. After viewing the overall sample distribution and selecting the appropriate normalization technique, click on Go to Dashboard and hit Proceed to go to Pathway Dashboard. On the Parameters page, select the differential expression algorithm of interest and click on Get Started as shown in Figure 7. In case no normalization is performed, you can choose to log 2 transform your data just for the purpose of differential expression by checking the Perform Log Transformation before Limma . Figure 7. Polly Notebook Figure 8. Select algorithm and proceed to dashboard Pathway Dashboard After data processing, a pathway map will be generated based on the default p -value and absolute log 2 fold-change values as illustrated in Figure 8. Significantly differentially expressed metabolites are displayed as nodes on the KEGG pathway map. Metabolites that fall under the range of these default parameters have color code ranging from red-green, with red representing upregulation and green representing downregulation. The size of the node depends on the level of significance i.e, lower the p -value larger the node. Hovering on any node will display a dialog box within formation about the selected metabolite. Upon clicking a node, a bar plot opens comparing intensities of the particular metabolite across cohorts. You can also zoom in/out, select nodes on the interactive map or download in several formats available. In case there are metabolites that could not be mapped on the pathway dashboard, a pop-up is shown. A list of these metabolites can be downloaded by clicking on Download List . Figure 9. Pathway Dashboard PCA Plot PCA is a common analysis technique to understand the clustering pattern between biologically grouped and ungrouped samples. Typically, an abnormal cluster or an outlier is representative of an issue in sample preparation. In case a sample displays an unusual behavior in the cluster belonging to a defined group, the sample should be dropped or reprocessed. The PCA plot also provides you with the option to exclude samples from the dataset which could be blanks, unwanted samples or samples that didn\u2019t have a good run during MS processing. Clicking on any sample will open a dialog box with the option Remove Outlier . Clicking on this will allow you to decide whether to remove this sample from the PCA Plot ( Realculate PCA ) or from the entire analysis ( Remove from Dataset ). Recalculate PCA will update the PCA plot whereas Remove from Dataset will update the entire analysis including the PCA plot, Volcano plot and the Pathway Dashboard. Excluded samples can be added back by wither clicking on the \"+\" sign beside the sample name or selecting the relevant samples and clicking on Add Back as we can see in Figure 9. Figure 10. PCA Plot Volcano Plot Volcano plot helps you to observe metabolites significantly changing between two cohorts. In Figure. 10, log 2 fold change and log 10 p -values are evaluated based on default parameters set to (log 2 fold change > 1 and log 2 fold change < -1) and p < 0.05 respectively. You can hover over a particular point on the plot to view p -value and fold change. In case there are more than one cohort comparison in your data, you can use the drop downs present in Differential Expression to choose which specific comparison to visualize and click on Apply . Any change in the cohort comparisons will update the Volcano Plot and the Pathway Dashboard accordingly. Figure 11. Volcano Plot View Duplicates There might be instances where you would have considered all the putative peak groups reported for a metabolite and goes ahead with the thought to filter them out in the down-stream analysis once you are certain. The duplicates of the metabolite are found on the basis of the compoundId column in the intensity file. The first filtering is done by making use of the cut-off specified for the significance where the app takes into consideration only the significant metabolic features, there is a possibility that one might filter out the duplicates here itself. If at all, there are multiple occurrences of a metabolite (based on compoundId ), then the one with more significant adj. p -value is represented on the KEGG map. However, if you want to revisit the metabolic features to select the desired representative feature, the View Duplicates option allows you to select onto the required feature from the table as shown in Figure 12. You can select the metabolite feature(s) from the table by clicking onto the check-box and further, clicking on Update . This shall update the pathway dashboard with the selected feature. Figure 12. View Duplicates Table One Click Report BETA One Click Report (OCR) is a functionality that allows you to generate reports in a standard format. It is extremely helpful when standard processes have been decided upon to generate results. Presence of a standardized reporting functionality also ensures reproducibility within an organization over a period of years. It is generated with parameters as specified by you. It can be saved in PDF and HTML formats and will be accessible from the Report section inside the workspace. The following information is mandatory to generate a report: Title of the Report: To provide a name to the report Description of the Report: To describe the experiment performed Analysis Insights: To list out the insights that are generated from the analysis Adj. pval cutoff: To specify the cut-off for the adj. p -value used to generate the volcano plot and pathway dashboard log2FC cutoff: To specify the cut-off for the log 2 fold change used to generate the volcano plot and the pathway dashboard Create Comparisons: To specify the cohorts of interest from the data provided. Once, a cohort comparison is specified, you can click on the '+' sign to add another comparison Generate: Click on Generate to create the report Figure 13. One Click Report Details about the app MetScape broadly works on the concept of differential expression analysis where the aim is to identify those metabolites whose expression differs under different cohort conditions. These differentially expressed metabolites are mapped onto the KEGG map to enable the global pathway analysis. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the disease and control condition and an 'adjusted' p -value that rates the significance of the difference. Frequently Asked Questions (FAQs) S.No Questions Answers 1 What is MetScape? MetScape is an end-to-end workflow that enables you to process unlabeled LC-MS targeted, semi-targeted or untargeted data and generate meaningful visualizations. 2 How can I test MetScape if I don't have my own data? You can download the demo data set by clicking on the Demo Files button on MetScape Upload page. 3 What are the required input files? There are two input files required. The intensity file is the .csv file generated after peak detection in El-MAVEN. It contains a list of metabolites along with their m/z, rt and intensity values across different samples. The cohort mapping file consists of the cohort information of every sample file that is uploaded. 4 Can I use MetScape if I don't have the retention time information? No. Retention time is mandatory as it is useful for separating duplicates. 5 Can I use MetScape if I don't have the HMDB IDs or KEGG IDs for my metabolites? To use MetScape you need HMDB IDs or KEGG IDs using which metabolites are mapped on the pathway dashboard. 6 Can I use public data sets on MetScape? Yes, public data sets can be processed using the following procedure: Convert the instrument generated raw files to .mzML or .mzXML using MSConvert . Process the files in El-MAVEN. For targeted analysis, perform peak annotation against a compound database. For untargeted analysis, perform feature detection and then run Untargeted Pipeline .Prepare the cohort file. Run the intensity and cohort files in MetScape. 7 Can I check the quality of my samples in MetScape? Yes, for quality check, MetScape prepares interactive PCA Plot from your data. The interactive plot enables you to remove any outliers from your analyses. 8 What should I do if l get an error on MetScape? You can directly message us on Intercom (icon on the bottom right of your screen) or you can e-mail to the Polly Support Team.","title":"MetScape"},{"location":"Apps/Metabolomic Data/MetScape.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/MetScape.html#overview","text":"Metabolomics is one of the key research areas focusing on small molecules present in a tissue or cell. MetScape Workflow allows you to process unlabeled LC-MS targeted, semi-targeted (without retention time) and untargeted data with insightful visualizations. You can directly upload CSV files obtained after peak detection in El-MAVEN along with the cohort information. The Polly Notebook enables you to use in-built scripts for normalizing, transforming and visualizing data along with the ability to upload and use your custom scripts. The pathway dashboard allows you to track differentially expressed metabolites across cohorts and form hypothesis on not only individual metabolites but on pathways as well.","title":"Overview"},{"location":"Apps/Metabolomic Data/MetScape.html#scope-of-the-app","text":"Supports analysis of targeted LC-MS data. Processes high-resolution and low-resolution data of up to 100 samples. Allows preprocessing of data through Polly Notebook. Perform PCA and differential expression analysis. Visualize significant metabolites on a global pathway map. Use the differentially expressed results with the output of RNA Seq Workflow to integrate multi-omics data in IntOmix . Figure 1. MetScape","title":"Scope of the app"},{"location":"Apps/Metabolomic Data/MetScape.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/MetScape.html#user-input","text":"MetScape requires the following two files as input: Intensity file The intensity file should be in .csv format as shown in Figure 2. The .csv file exported after peak picking in El-MAVEN is the input file. For untargeted analysis, the output of Untargeted Pipeline serves as the input. Figure 2. Sample intensity file Cohort file The cohort file should be in .csv format as shown in Figure 3. This file should contain two columns, sample containing sample names along with cohort for its cohort information. Figure 3. Sample cohort file","title":"User Input"},{"location":"Apps/Metabolomic Data/MetScape.html#steps-involved-in-data-processing","text":"Perform custom normalization, transforming and visualization Perform differential expression analysis using limma (R-Bioconductor package) and visualize it using a volcano plot Identify and reject sample outliers, if any from PCA plot Perform sample based visualizations on Polly Notebook Identify significant pathways and metabolites on the pathway dashboard Move back and forth between various components of the workflow","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/MetScape.html#caveats","text":"The intensity values should not be NA or NaN for any metabolite. There should not be any blank cell for the intensities. Retention time and m/z information both are required to run the analysis. The metabolites in the input file must have either KEGG IDs or HMDB IDs to map on the pathway dashboard. All the samples present in the intensity files should also be present in the metadata file. Each cohort must have at least three samples to successfully perform differential expression. In the case of dual-mode data, the metabolite name should contain the polarity of the metabolite in the format \"MetaboliteName##polarity\" , ie. Glucose##pos or Fructose##neg. During the identification of significant pathways, if the data consists, where the data consists of duplicates of metabolite features, they are filtered out on the basis of the parameters specified for significance. If at all, there are multiple occurrences of a metabolite, the one with the most significant p -value is mapped on the pathway dashboard. You can always revisit the duplicates from the View Duplicates option. Differential expression will be performed on all possible cohort comparisons. An analysis can not be restored simultaneously using two different tabs.","title":"Caveats"},{"location":"Apps/Metabolomic Data/MetScape.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Metabolomic Data/MetScape.html#upload-files","text":"Select MetScape Workflow from the dashboard under the Metabolomics Data Tab as shown in Figure 4. Create a New Workspace or select an existing one from the drop-down and provide the Name of the Session to be redirected to MetScape's upload page. Figure 4. Polly Dashboard and Workspace selection After the files have been successfully uploaded as shown in Figure 5, click on Proceed to move to Polly Notebook. Figure 5. Upload page","title":"Upload files"},{"location":"Apps/Metabolomic Data/MetScape.html#polly-notebook","text":"The Polly Notebook consists of a list of in-built scripts which can be selected from the toolbox present on the left-hand side as we can see in Figure 6. Figure 6. Polly Notebook showing the visualization of sample distribution The function of these in-built scripts is mentioned below: Go To Dashboard: Clicking on this will take you to the next step Control Normalization: This method is used to perform normalization with respect to Control cohort. df < -control_normalisation(mavenData,metaData,'Control') Cyclic Loess Normalization: To incorporate for negative intensity values post normalization, all intensity values are scaled so the least value is 1. df < -cyclic_loess(mavenData) Log 2 Transformation: To incorporate for 0 intensity values, 1 is added to all intensity values pre-normalization. df < -log_transform(df) Sample View Visualization: This function is used to get the visualization of the intensity file for both pre- and post-normalization. sample_view(mavenData) Click here for a detailed documentation about Polly Notebook. Note: The last data saved after transformation or normalization in df is taken forward. After viewing the overall sample distribution and selecting the appropriate normalization technique, click on Go to Dashboard and hit Proceed to go to Pathway Dashboard. On the Parameters page, select the differential expression algorithm of interest and click on Get Started as shown in Figure 7. In case no normalization is performed, you can choose to log 2 transform your data just for the purpose of differential expression by checking the Perform Log Transformation before Limma . Figure 7. Polly Notebook Figure 8. Select algorithm and proceed to dashboard","title":"Polly Notebook"},{"location":"Apps/Metabolomic Data/MetScape.html#pathway-dashboard","text":"After data processing, a pathway map will be generated based on the default p -value and absolute log 2 fold-change values as illustrated in Figure 8. Significantly differentially expressed metabolites are displayed as nodes on the KEGG pathway map. Metabolites that fall under the range of these default parameters have color code ranging from red-green, with red representing upregulation and green representing downregulation. The size of the node depends on the level of significance i.e, lower the p -value larger the node. Hovering on any node will display a dialog box within formation about the selected metabolite. Upon clicking a node, a bar plot opens comparing intensities of the particular metabolite across cohorts. You can also zoom in/out, select nodes on the interactive map or download in several formats available. In case there are metabolites that could not be mapped on the pathway dashboard, a pop-up is shown. A list of these metabolites can be downloaded by clicking on Download List . Figure 9. Pathway Dashboard","title":"Pathway Dashboard"},{"location":"Apps/Metabolomic Data/MetScape.html#pca-plot","text":"PCA is a common analysis technique to understand the clustering pattern between biologically grouped and ungrouped samples. Typically, an abnormal cluster or an outlier is representative of an issue in sample preparation. In case a sample displays an unusual behavior in the cluster belonging to a defined group, the sample should be dropped or reprocessed. The PCA plot also provides you with the option to exclude samples from the dataset which could be blanks, unwanted samples or samples that didn\u2019t have a good run during MS processing. Clicking on any sample will open a dialog box with the option Remove Outlier . Clicking on this will allow you to decide whether to remove this sample from the PCA Plot ( Realculate PCA ) or from the entire analysis ( Remove from Dataset ). Recalculate PCA will update the PCA plot whereas Remove from Dataset will update the entire analysis including the PCA plot, Volcano plot and the Pathway Dashboard. Excluded samples can be added back by wither clicking on the \"+\" sign beside the sample name or selecting the relevant samples and clicking on Add Back as we can see in Figure 9. Figure 10. PCA Plot","title":"PCA Plot"},{"location":"Apps/Metabolomic Data/MetScape.html#volcano-plot","text":"Volcano plot helps you to observe metabolites significantly changing between two cohorts. In Figure. 10, log 2 fold change and log 10 p -values are evaluated based on default parameters set to (log 2 fold change > 1 and log 2 fold change < -1) and p < 0.05 respectively. You can hover over a particular point on the plot to view p -value and fold change. In case there are more than one cohort comparison in your data, you can use the drop downs present in Differential Expression to choose which specific comparison to visualize and click on Apply . Any change in the cohort comparisons will update the Volcano Plot and the Pathway Dashboard accordingly. Figure 11. Volcano Plot","title":"Volcano Plot"},{"location":"Apps/Metabolomic Data/MetScape.html#view-duplicates","text":"There might be instances where you would have considered all the putative peak groups reported for a metabolite and goes ahead with the thought to filter them out in the down-stream analysis once you are certain. The duplicates of the metabolite are found on the basis of the compoundId column in the intensity file. The first filtering is done by making use of the cut-off specified for the significance where the app takes into consideration only the significant metabolic features, there is a possibility that one might filter out the duplicates here itself. If at all, there are multiple occurrences of a metabolite (based on compoundId ), then the one with more significant adj. p -value is represented on the KEGG map. However, if you want to revisit the metabolic features to select the desired representative feature, the View Duplicates option allows you to select onto the required feature from the table as shown in Figure 12. You can select the metabolite feature(s) from the table by clicking onto the check-box and further, clicking on Update . This shall update the pathway dashboard with the selected feature. Figure 12. View Duplicates Table","title":"View Duplicates"},{"location":"Apps/Metabolomic Data/MetScape.html#one-click-reportbeta","text":"One Click Report (OCR) is a functionality that allows you to generate reports in a standard format. It is extremely helpful when standard processes have been decided upon to generate results. Presence of a standardized reporting functionality also ensures reproducibility within an organization over a period of years. It is generated with parameters as specified by you. It can be saved in PDF and HTML formats and will be accessible from the Report section inside the workspace. The following information is mandatory to generate a report: Title of the Report: To provide a name to the report Description of the Report: To describe the experiment performed Analysis Insights: To list out the insights that are generated from the analysis Adj. pval cutoff: To specify the cut-off for the adj. p -value used to generate the volcano plot and pathway dashboard log2FC cutoff: To specify the cut-off for the log 2 fold change used to generate the volcano plot and the pathway dashboard Create Comparisons: To specify the cohorts of interest from the data provided. Once, a cohort comparison is specified, you can click on the '+' sign to add another comparison Generate: Click on Generate to create the report Figure 13. One Click Report","title":"One Click ReportBETA"},{"location":"Apps/Metabolomic Data/MetScape.html#details-about-the-app","text":"MetScape broadly works on the concept of differential expression analysis where the aim is to identify those metabolites whose expression differs under different cohort conditions. These differentially expressed metabolites are mapped onto the KEGG map to enable the global pathway analysis. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the disease and control condition and an 'adjusted' p -value that rates the significance of the difference.","title":"Details about the app"},{"location":"Apps/Metabolomic Data/MetScape.html#frequently-asked-questions-faqs","text":"S.No Questions Answers 1 What is MetScape? MetScape is an end-to-end workflow that enables you to process unlabeled LC-MS targeted, semi-targeted or untargeted data and generate meaningful visualizations. 2 How can I test MetScape if I don't have my own data? You can download the demo data set by clicking on the Demo Files button on MetScape Upload page. 3 What are the required input files? There are two input files required. The intensity file is the .csv file generated after peak detection in El-MAVEN. It contains a list of metabolites along with their m/z, rt and intensity values across different samples. The cohort mapping file consists of the cohort information of every sample file that is uploaded. 4 Can I use MetScape if I don't have the retention time information? No. Retention time is mandatory as it is useful for separating duplicates. 5 Can I use MetScape if I don't have the HMDB IDs or KEGG IDs for my metabolites? To use MetScape you need HMDB IDs or KEGG IDs using which metabolites are mapped on the pathway dashboard. 6 Can I use public data sets on MetScape? Yes, public data sets can be processed using the following procedure: Convert the instrument generated raw files to .mzML or .mzXML using MSConvert . Process the files in El-MAVEN. For targeted analysis, perform peak annotation against a compound database. For untargeted analysis, perform feature detection and then run Untargeted Pipeline .Prepare the cohort file. Run the intensity and cohort files in MetScape. 7 Can I check the quality of my samples in MetScape? Yes, for quality check, MetScape prepares interactive PCA Plot from your data. The interactive plot enables you to remove any outliers from your analyses. 8 What should I do if l get an error on MetScape? You can directly message us on Intercom (icon on the bottom right of your screen) or you can e-mail to the Polly Support Team.","title":"Frequently Asked Questions (FAQs)"},{"location":"Apps/Metabolomic Data/QuantFit.html","text":"Introduction Overview Calibration refers to the process of quantifying samples of unknown concentrations with known (standard) samples. Mass spec experiments are a crucial component of metabolomics experiments but the output obtained from software like El-MAVEN and MultiQuant are intensities which are unitless. From these intensities, we can calculate fractional enrichment and do relative comparisons. But for some analysis like kinetic flux analysis, we need absolute concentrations of the metabolites. These concentrations have units like M, \u03bcM, mM like any other chemical concentration. In order to absolutely quantify these metabolites, experiments are done with standard samples of known concentrations. QuantFit uses these data points to get mathematical mappings from intensities to concentrations which are subsequently used to calculate concentrations of experimental intensities. Scope of the app Quantifies intensities obtained from El-MAVEN and MultiQuant using internal standards. Supports quantification of metabolites and labeled fragments of metabolites. Provides various curve fitting options like linear, exponential, log-log, polynomial and power. Provides an option to correct the injection efficiency of samples using total ion count normalization. Provides the biosample concentration plot to visualize the concentration of all biosamples in a barplot. Incorporates Grubbs' test to detect outlier samples. Provides various options to optimize the curve fit with e.g.: Slope-intercept equation for quantification Rejecting outlier samples Normalizing concentration values with an internal standard Assigning weights to the standard samples Figure 1. QuantFit Getting Started User Input QuantFit requires the following three files as input: Intensity file The intensity file is mandatory for both LC-MS and LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. The intensity file can be either in the long format. Figure 2. Sample intensity file in long format where, Original Filename is the Sample name Sample Name is the Cohort name Component Name is the Name of the metabolite Mass Info is the Mass information of fragments Area is the Intensities Or it can be the .csv file exported after peak picking in El-MAVEN. Figure 3. Sample intensity file exported from El-MAVEN Metadata standard sample file The metadata standard file is mandatory for both LC-MS and LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. This file contains concentration information of the standard samples using which quantification is performed. Figure 4. Sample metadata standard file where, Original Filename is the Standard sample name Sample Name is the Standard cohort name Concentration is the Concentration of standard samples Unit Info is the Unit of concentration Unlabeled Fragment is the Unlabeled fragment information Metadata mapping file The metadata mapping file is optional LC-MS but mandatory for LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. This file is required to map daughter fragments to their parent fragments. Figure 5. Sample metadata mapping file where, Component Name is the Name of the daughter fragments Formula is the Formula of the particular daughter fragment Unlabeled Fragment is the Name of the metabolite Isotopic Tracer is the Element that was labeled Parent Formula is the Formula of the parent fragment Steps involved in data processing Process MS files using El-MAVEN or MultiQuant Upload intensity files Create/Upload metadata files Adjust quantification parameters and Quantify Download quantification files in CSV format Caveats We use the entire standard and negate any background subtraction. All isotopologue intensities are added to the parent intensity in case of labeled data. Standard samples should contain \u201cstd\u201d in their cohort name. Mass Info column in the metabolite intensity file should be 0 throughout for LC-MS data In case there are no cohorts in your data, the contents of Sample Name and Original Filename in the standard metadata file should be the same. Unlabeled Fragment in the standard metadata file is the metabolite name for LC-MS data. Tutorial Upload Files Select QuantFIt from the dashboard under the Metabolomics Data tab. Create a New Workspace or select an existing one from the drop-down and provide the Name of the Session to be redirected to QuantFit's upload page. Figure 6. Polly Dashboard and Workspace selection Click on Upload File and upload the metabolite intensity file. Figure 7. Upload page Metadata Interface After the file have been successfully uploaded you will be redirected to the Metadata interface. This interface provides you the ability to either upload or create your metadata. Figure 8. Upload metadata by clicking on Upload Files Figure 9. Create metadata by selecting the standards within the data Figure 10. Enter concentration values for all the standard samples across all metabolites in a few clicks Watch the demo video below for more information. Injection Efficiency Correction Injection Efficiency Correction is used to normalize the biosamples with respect to internal standards in the data on the basis of total ion count. We recommend the highest intensity non-outlier sample when correcting for injection efficiency. You can also select a sample from the outliers detected with Grubbs' test for this normalization. To normalize intensities of metabolites present in the sample, by default we calculate a normalizing factor for each sample. This is calculated by dividing each intensity value by the maximum intensity value among the non-outlier samples. Then the intensity values of all metabolites in each sample is divided by the respective normalizing factor. You can also select this highest intensity sample from the list of outlier samples. QuantFit uses Grubb's test to detect outliers from among the samples. Figure 11. Select internal standard metabolite and a sample for injection efficiency correction. Quantification Dashboard By default, linear quantification will be performed by the tool after performing the above-mentioned steps. However, you can optimize quantification by changing the curve fit to Polynomial, Exponential, LogLog or Power depending upon your data. In case you feel there is an outlier in your data, click on the sample and then on the \u201c-\u201d sign in the pop-up box as shown in FIgure. 13. Click on Quantify to recalculate the curve fit at the new parameters. Figure 12. Quantification Dashboard QuantFit plots quantified values and standard values on the calibration curve. You can optimize the curve fit from this interface or view this curve on linear and log axes for better analysis. Figure 13. Calibration curve The Concentration Plot gives the concentrations across all biosamples as a barplot. The Residue Plot gives the residual error for the curve fit as a barplot. You can toggle between these plots by clicking on their names as shown in Figure 14. Figure 14. Residue and Concentration plot Clicking on Standard Data on the Calibration Curv e shown in Figure 12. gives information about the standard mappings in the input file. Figure 15. Standard data Suggested Fits The suggested fits option makes it possible for you to save time spent on manually curating data by removing outliers and selecting the best fit. The samples furthest from the default linear curve are removed individually and the curve fit is calculated. Subsequently the fits calcualted with the best r 2 apart from the default linear fit ( Custom FIt ) are displayed as Suggested Fit 1 , Suggested Fit 2 and Suggested Fit 3 . You can select any fit and mark it as the default fit by clicking on Set Fit as Default . Figure 16. Suggested Fits Normalization You can also normalize the quantified concentration values with respect to an internal standard's concentration values. You can select an internal standard from the list of metabolites for normalization. To normalize the concentration of a given metabolite, concentrations from individual samples are divided by the respective concentration of the internal standard. Figure 17. Normalization Logs It summarizes all the information of each curve fitting performed for every metabolite. Missing metabolite data: Metabolites for which standard sample is absent Name of metabolite Curve fit information: Type of fit Standard Point Information: Standard data information for the metabolite Curve Fit Performed: Curve fitting information and Goodness of fit Figure 18. Logs Processed Data It shows the quantified values of every metabolite in a table. This can be downloaded as a .csv file by clicking on the download icon. Figure 19. Processed data Details about the app Goodness of fit (GOF) parameters The following section describes the formula used for calculating the curve's goodness of fit Sum of squares about the mean where: y : list of observed values y_av : the average of observed values weight : weights given to the sample points (by default 1 is given to each sample point) Note: For Gaussian uncertainties, sigma instead of 1/sigma 2 is used Sum of squares of residuals where: ob : list of observed values pr : list of predicted values weight : weights given to the sample points Note: For Gaussian uncertainties, sigma instead of 1/sigma 2 is used R 2 where: sst : the sum of squares of residuals sse : the sum of squares about the mean Root mean square error where: sse : the sum of squares of residuals dof : the model degrees of freedom, is the length of y minus the number of parameters calculated in the model Errors and warnings Errors raised while selecting files for uploading Number of files error: Three files are required for quantification . If you try to upload more than two files then this error is thrown by the tool. File type error: Please select the correct file type . If the selected file and file type do not match, then this error is thrown by the tool. File validation errors and warnings Missing Value warning: It is raised when any column in the uploaded files has missing values. It also specifies the action taken by the tool if you choose to continue. Example: Warning - Row Number 8192 : column Area has missing value Action Taken - Missing value of columns replaced with 0 Videos References Gorrochategui E. et al, \"Data analysis strategies for targeted and untargeted LC-MS metabolomic studies: Overview and workflow\", TrAC Trends in Analytical Chemistry 2016 82, 425-442","title":"QuantFit"},{"location":"Apps/Metabolomic Data/QuantFit.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/QuantFit.html#overview","text":"Calibration refers to the process of quantifying samples of unknown concentrations with known (standard) samples. Mass spec experiments are a crucial component of metabolomics experiments but the output obtained from software like El-MAVEN and MultiQuant are intensities which are unitless. From these intensities, we can calculate fractional enrichment and do relative comparisons. But for some analysis like kinetic flux analysis, we need absolute concentrations of the metabolites. These concentrations have units like M, \u03bcM, mM like any other chemical concentration. In order to absolutely quantify these metabolites, experiments are done with standard samples of known concentrations. QuantFit uses these data points to get mathematical mappings from intensities to concentrations which are subsequently used to calculate concentrations of experimental intensities.","title":"Overview"},{"location":"Apps/Metabolomic Data/QuantFit.html#scope-of-the-app","text":"Quantifies intensities obtained from El-MAVEN and MultiQuant using internal standards. Supports quantification of metabolites and labeled fragments of metabolites. Provides various curve fitting options like linear, exponential, log-log, polynomial and power. Provides an option to correct the injection efficiency of samples using total ion count normalization. Provides the biosample concentration plot to visualize the concentration of all biosamples in a barplot. Incorporates Grubbs' test to detect outlier samples. Provides various options to optimize the curve fit with e.g.: Slope-intercept equation for quantification Rejecting outlier samples Normalizing concentration values with an internal standard Assigning weights to the standard samples Figure 1. QuantFit","title":"Scope of the app"},{"location":"Apps/Metabolomic Data/QuantFit.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/QuantFit.html#user-input","text":"QuantFit requires the following three files as input: Intensity file The intensity file is mandatory for both LC-MS and LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. The intensity file can be either in the long format. Figure 2. Sample intensity file in long format where, Original Filename is the Sample name Sample Name is the Cohort name Component Name is the Name of the metabolite Mass Info is the Mass information of fragments Area is the Intensities Or it can be the .csv file exported after peak picking in El-MAVEN. Figure 3. Sample intensity file exported from El-MAVEN Metadata standard sample file The metadata standard file is mandatory for both LC-MS and LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. This file contains concentration information of the standard samples using which quantification is performed. Figure 4. Sample metadata standard file where, Original Filename is the Standard sample name Sample Name is the Standard cohort name Concentration is the Concentration of standard samples Unit Info is the Unit of concentration Unlabeled Fragment is the Unlabeled fragment information Metadata mapping file The metadata mapping file is optional LC-MS but mandatory for LC-MS/MS data and should be in either .csv, .xls, .xlsx, .txt format. This file is required to map daughter fragments to their parent fragments. Figure 5. Sample metadata mapping file where, Component Name is the Name of the daughter fragments Formula is the Formula of the particular daughter fragment Unlabeled Fragment is the Name of the metabolite Isotopic Tracer is the Element that was labeled Parent Formula is the Formula of the parent fragment","title":"User Input"},{"location":"Apps/Metabolomic Data/QuantFit.html#steps-involved-in-data-processing","text":"Process MS files using El-MAVEN or MultiQuant Upload intensity files Create/Upload metadata files Adjust quantification parameters and Quantify Download quantification files in CSV format","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/QuantFit.html#caveats","text":"We use the entire standard and negate any background subtraction. All isotopologue intensities are added to the parent intensity in case of labeled data. Standard samples should contain \u201cstd\u201d in their cohort name. Mass Info column in the metabolite intensity file should be 0 throughout for LC-MS data In case there are no cohorts in your data, the contents of Sample Name and Original Filename in the standard metadata file should be the same. Unlabeled Fragment in the standard metadata file is the metabolite name for LC-MS data.","title":"Caveats"},{"location":"Apps/Metabolomic Data/QuantFit.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Metabolomic Data/QuantFit.html#upload-files","text":"Select QuantFIt from the dashboard under the Metabolomics Data tab. Create a New Workspace or select an existing one from the drop-down and provide the Name of the Session to be redirected to QuantFit's upload page. Figure 6. Polly Dashboard and Workspace selection Click on Upload File and upload the metabolite intensity file. Figure 7. Upload page","title":"Upload Files"},{"location":"Apps/Metabolomic Data/QuantFit.html#metadata-interface","text":"After the file have been successfully uploaded you will be redirected to the Metadata interface. This interface provides you the ability to either upload or create your metadata. Figure 8. Upload metadata by clicking on Upload Files Figure 9. Create metadata by selecting the standards within the data Figure 10. Enter concentration values for all the standard samples across all metabolites in a few clicks Watch the demo video below for more information.","title":"Metadata Interface"},{"location":"Apps/Metabolomic Data/QuantFit.html#injection-efficiency-correction","text":"Injection Efficiency Correction is used to normalize the biosamples with respect to internal standards in the data on the basis of total ion count. We recommend the highest intensity non-outlier sample when correcting for injection efficiency. You can also select a sample from the outliers detected with Grubbs' test for this normalization. To normalize intensities of metabolites present in the sample, by default we calculate a normalizing factor for each sample. This is calculated by dividing each intensity value by the maximum intensity value among the non-outlier samples. Then the intensity values of all metabolites in each sample is divided by the respective normalizing factor. You can also select this highest intensity sample from the list of outlier samples. QuantFit uses Grubb's test to detect outliers from among the samples. Figure 11. Select internal standard metabolite and a sample for injection efficiency correction.","title":"Injection Efficiency Correction"},{"location":"Apps/Metabolomic Data/QuantFit.html#quantification-dashboard","text":"By default, linear quantification will be performed by the tool after performing the above-mentioned steps. However, you can optimize quantification by changing the curve fit to Polynomial, Exponential, LogLog or Power depending upon your data. In case you feel there is an outlier in your data, click on the sample and then on the \u201c-\u201d sign in the pop-up box as shown in FIgure. 13. Click on Quantify to recalculate the curve fit at the new parameters. Figure 12. Quantification Dashboard QuantFit plots quantified values and standard values on the calibration curve. You can optimize the curve fit from this interface or view this curve on linear and log axes for better analysis. Figure 13. Calibration curve The Concentration Plot gives the concentrations across all biosamples as a barplot. The Residue Plot gives the residual error for the curve fit as a barplot. You can toggle between these plots by clicking on their names as shown in Figure 14. Figure 14. Residue and Concentration plot Clicking on Standard Data on the Calibration Curv e shown in Figure 12. gives information about the standard mappings in the input file. Figure 15. Standard data","title":"Quantification Dashboard"},{"location":"Apps/Metabolomic Data/QuantFit.html#suggested-fits","text":"The suggested fits option makes it possible for you to save time spent on manually curating data by removing outliers and selecting the best fit. The samples furthest from the default linear curve are removed individually and the curve fit is calculated. Subsequently the fits calcualted with the best r 2 apart from the default linear fit ( Custom FIt ) are displayed as Suggested Fit 1 , Suggested Fit 2 and Suggested Fit 3 . You can select any fit and mark it as the default fit by clicking on Set Fit as Default . Figure 16. Suggested Fits","title":"Suggested Fits"},{"location":"Apps/Metabolomic Data/QuantFit.html#normalization","text":"You can also normalize the quantified concentration values with respect to an internal standard's concentration values. You can select an internal standard from the list of metabolites for normalization. To normalize the concentration of a given metabolite, concentrations from individual samples are divided by the respective concentration of the internal standard. Figure 17. Normalization","title":"Normalization"},{"location":"Apps/Metabolomic Data/QuantFit.html#logs","text":"It summarizes all the information of each curve fitting performed for every metabolite. Missing metabolite data: Metabolites for which standard sample is absent Name of metabolite Curve fit information: Type of fit Standard Point Information: Standard data information for the metabolite Curve Fit Performed: Curve fitting information and Goodness of fit Figure 18. Logs","title":"Logs"},{"location":"Apps/Metabolomic Data/QuantFit.html#processed-data","text":"It shows the quantified values of every metabolite in a table. This can be downloaded as a .csv file by clicking on the download icon. Figure 19. Processed data","title":"Processed Data"},{"location":"Apps/Metabolomic Data/QuantFit.html#details-about-the-app","text":"","title":"Details about the app"},{"location":"Apps/Metabolomic Data/QuantFit.html#goodness-of-fit-gof-parameters","text":"The following section describes the formula used for calculating the curve's goodness of fit Sum of squares about the mean where: y : list of observed values y_av : the average of observed values weight : weights given to the sample points (by default 1 is given to each sample point) Note: For Gaussian uncertainties, sigma instead of 1/sigma 2 is used Sum of squares of residuals where: ob : list of observed values pr : list of predicted values weight : weights given to the sample points Note: For Gaussian uncertainties, sigma instead of 1/sigma 2 is used R 2 where: sst : the sum of squares of residuals sse : the sum of squares about the mean Root mean square error where: sse : the sum of squares of residuals dof : the model degrees of freedom, is the length of y minus the number of parameters calculated in the model","title":"Goodness of fit (GOF) parameters"},{"location":"Apps/Metabolomic Data/QuantFit.html#errors-and-warnings","text":"Errors raised while selecting files for uploading Number of files error: Three files are required for quantification . If you try to upload more than two files then this error is thrown by the tool. File type error: Please select the correct file type . If the selected file and file type do not match, then this error is thrown by the tool. File validation errors and warnings Missing Value warning: It is raised when any column in the uploaded files has missing values. It also specifies the action taken by the tool if you choose to continue. Example: Warning - Row Number 8192 : column Area has missing value Action Taken - Missing value of columns replaced with 0","title":"Errors and warnings"},{"location":"Apps/Metabolomic Data/QuantFit.html#videos","text":"","title":"Videos"},{"location":"Apps/Metabolomic Data/QuantFit.html#references","text":"Gorrochategui E. et al, \"Data analysis strategies for targeted and untargeted LC-MS metabolomic studies: Overview and workflow\", TrAC Trends in Analytical Chemistry 2016 82, 425-442","title":"References"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html","text":"Introduction Overview Untargeted Metabolomics, otherwise known as discovery metabolomics, analyzes the metabolomic profile globally from each sample thus producing voluminous and complex data. This needs robust bioinformatics tools to help meaningfully interpret this data. The Untargeted Pipeline enables you to perform the annotation and identification of the metabolites. It uses CAMERA , a package built for annotation of the adducts, isotopes, fragments and then maps features to a reference compound database (KEGG, HMDB or a custom database). The workflow begins with automated peak curation on El-MAVEN using the Untargeted algorithm and the peak table derived from this is used as input for PollyTM Untargeted Pipeline. Scope of the app Annotate adducts, isotopes and fragments in the data and identify metabolites Perform downstream analysis such as differential expression, anova test and pathway enrichment. Figure 1. Untargeted Pipeline Getting Started User Input Untargeted Pipeline can take a csv file are well as a emdb file as input. Emdb File The emdb file used here is a RSQLite database file generated when an El-MAVEN session is saved. It has all the unannotated features along with other features of the peaks. Intensity File The intensity file used here is the El-MAVEN output in peak detailed format. This output contains unannotated features along with their retention time and m/z information. Figure 2. El-MAVEN intensity file Metadata File The metadata file contains the sample to cohort mapping information that will be used in the downstream processing of the data. Figure 3. Metadata file Steps involved in data processing Process raw data on El-MAVEN using automated feature detection. Export intensity file in peak detailed format. Annotate adducts, isotopes and fragments in the data. Perform identification of metabolites. Perform downstream analysis such as differential expression, anova test and pathway enrichment. Caveats The input file should be the peak detailed output of El-MAVEN. The emdb file should be the one generated from El-MAVEN. Tutorial Go to the dashboard and select Untargeted Pipeline under the Metabolomcis Data tab. Create a New workspace or choose from the existing ones and provide a Name of the Session to be redirected to the upload page. Figure 4. Polly Dashboard Figure 5. Untargeted Pipeline Upload Files The upload data tab allows you to upload El-MAVEN output in a csv format or an emdb file containing peak information along with the cohort file up to 300MB. Upload either the intensity file or an emdb file along with cohort file using the drop downs shown below, select the polarity of the data and then click on Load Data to proceed. Figure 6. Upload files Annotation For annotation, we use the R package, CAMERA. It takes the output in peak detailed format from El-MAVEN which contains. The file should contain mzmin and mzmax details in the file. CAMERA CAMERA operates in the following steps: First it groups the features by their retention time It then forms groups by correlation inside samples (EIC) or correlation across samples or both After grouping these features, it annotates the possible isotopes and adducts. Figure 7. Annotation Advanced parameters The following parameters need to be set before running CAMERA: cor_exp_th: Correlation threshold for EIC correlation (Range: 0-1) pval: p -value threshold for testing correlation of significance (Range: 0-1) perfwhm: percentage of FWHM (Full Width at Half Maximum) width used in \" groupFWHM \" function for grouping features sigma: multiplier of the standard deviation used in \" groupFWHM \" function for grouping features calccis: Use correlation inside samples for peak grouping (TRUE/FALSE) calccas: Use correlation across samples for peak grouping (TRUE/FALSE) max_iso: maximum number of expected isotopes (0-8) minfrac: The percentage number of samples, which must satisfy the C12/C13 rule for isotope annotation ppm: General ppm error mzabs: General absolut error in m/z multiplier: If no ruleset is provided, calculate ruleset with max. number n of [nM+x] clusterions max_peaks: How much peaks will be calculated in every thread using the parallel mode maxcharge: maximum ion charge Figure 8. Advanced parameters Select adducts for annotation You can select adducts that are needed for annotation. Available Adducts Rules: The default adduct rules file is present which can be used for annotation. Upload Custom Adducts Rules: You can upload the custom adducts rules file otherwise. The adducts rules file has the following columns: name: adduct name nmol: Number of molecules (xM) included in the molecule charge: charge of the molecule massdiff: mass difference without calculation of charge and nmol (CAMERA will do this automatically) oidscore: This score is the adduct index. Molecules with the same cations (anions) configuration and different nmol values have the same oidscore, such as [M+H] and [2M+H] quasi: Every annotation which belongs to one molecule is called annotation group. Examples for these are [M+H] and [M+Na], where M is the same molecule. An annotation group must include at least one ion with quasi set to 1 for this adduct. If an annotation group only includes optional adducts (rule set to 0) then this group is excluded. To disable this reduction, set all rules to 1 or 0. ips: This is the rule score. If a peak is related to more than one annotation group, then the group having a higher score (sum of all annotations) gets picked. This effectively reduces the number of false positives. Figure 9. Select adducts for annotation CAMERA output table After annotation CAMERA adds three columns i.e. isotopes, adducts and pcgroup. The isotopes column contains the annotation for the isotopes where annotation is in the format of \"[id][isotope]charge\" for example [1][M]+, [1][M+1]+, [2][M+3]+. The adduct column contains the annotation for the adducts where annotation is in the format of \"[adduct] charge basemass\" for example [M+H]+ 161.105, [M+K]+ 123.15 etc. The pcgroup column contains the \u2018pseudospectra\u2019 which means features are grouped based on rt and correlation (inside and across samples). Figure 10. CAMERA output table Restructured CAMERA output table The features in the CAMERA output are not grouped together according to the pcgroup because it only appends the new columns in the existing feature table without changing the order of the features and also features which are different molecules may have the same pcgroup . So to interpret the results better it is necessary to separate the features which belong to different molecules within the same pcgroup . To overcome the above problem, there is another label of grouping based on the features belonging to the same molecule. The other label of grouping is done by assuming that the features which are representing the same molecule should have the same basemass. The following operations are performed to make the restructured CAMERA output: The adduct column is split into two new columns i.e. adduct_type and basemass . If any feature has more than one combination of adduct_type and basemass for example \"[M+K]+ 123.15 [M+Na]+ 139.124 [M+H]+ 161.105\" then they are split into separate rows having other information same. The isotopes column is split into two new columns i.e. isotope_id and isotope_type . A new column feature_group is added in the existing table where each value represents a different molecule. The feature_group column is filled based on the following assumptions: Features having basemass will have the same feature_group id. Those features which do not have the adducts (which CAMERA could not annotate) will be filled by [M+H]+ (in positive mode) or [M-H]- (in negative mode) as adduct and based on this adduct information the adduct_type and basemass columns will be filled. Features having the same isotope_id will be grouped in the same feature_group id. The same pcgroup may have more than one feature_group ids if it has more than one molecule. Figure 11. Restructured CAMERA ouput table Representatiove output table After restructuring the CAMERA output it is necessary to define the representative feature from the features belonging to the same feature_group id because since these features belong to the same molecule so there is no need to include all features in the identification step. The representative feature is defined based on the following assumptions: If the feature group has only one feature then that feature will be considered as the representative feature. The feature group has more than one feature then the representative feature will be determined by the following criteria: If the feature group has [M+H] (in positive mode) or [M-H] (in negative mode) then that feature will be considered as the representative feature. If the feature group does not have [M+H] (in positive mode) or [M-H] (in negative mode) then that feature will be considered as representative whose sum of intensity across all samples is maximum. Figure 12. Representative CAMERA ouput table Summary of annotation The data is summarized on the basis of the number of features within pcgroup and feature group: Number of features vs counts of pcgroup Figure 13. Number of features vs counts of pcgroup Number of features vs counts of feature groups Figure 14. Number of features vs counts of feature groups Identification The identification is performed on the representative table only. The representative features are searched against the compound database uploaded. It uses the basemass instead mZ for mass searching because adducts and isotopes are already filtered in the above steps. Figure 15. Advanced parameters Figure 16. Identification Representative metabolite identification table The representative table is appended by the compound database columns after identification. Figure 17. Representative metabolite identification table Overall metabolite identification table The representative metabolite identification table is again merged to the restructured camera output. Figure 18. Overall metabolite identification table El-Maven format The results are generated by converting the representative and overall tables in the group summary format. Figure 19. El-Maven output(Group Summary format) GCT-Preparation This interface allows you to select the data to be used in the downstream pipeline. The user can use any data to the downstream analysis by selecting the data from the previous steps. Figure 20. GCT-Preparation Tab Uploaded data: This allows you to take the uploaded data to the downsteam pipeline. Annotated data: This allows you to take the annotated data to the downsteam pipeline. You can either take the representative data or the overall data to the downsteam pipeline. Identified data: This allows you to take the identified data to the downsteam pipeline. You can either take the representative data or the overall data to the downsteam pipeline. Figure 21. Data downstream Representative: This allows you to select the representative data from either the annotated data or identified data depending on the above selection. Overall: This allows you to select the overall data from either the annotated data or identified data depending on the above selection. Figure 22. Data type Pre-processing The Pre-processing interface allows you to perform a multitude of functions on the data such as: Figure 23. Preprocessing Tab Select Internal Standards: This allows you to select the internal standard(s) from within the El-MAVEN output file when a separate internal standards file is not provided as input. Note: In case, the internal standard(s) are not in the El-MAVEN output file but in the separate internal standards file, they will not show up in the drop down menu. To select the desired internal standards, select them in Normalize by individual internal standards option under Normalize by Internal standards in the Perform Normalization > Normalization . Figure 24. Selecting internal standards from the data Drop Samples: This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during MS processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 24 after selecting the sample(s) from the drop down menu. Figure 25. Drop Samples option Normalize by Internal standards performs normalization using the internal standards. Figure 26. Normalize by internal standards options Normalize by sum of internal standards normalizes by the sum of the standards provided. Normalize by average of internal standards normalizes by the average of the standards provided. Normalize by individual internal standards normalizes by the internal standards selected previously. Normalize by metabolites normalizes by any particular metabolite selected. Figure 27. Normalize by metabolites option Normalize by sum of metabolites normalizes by the sum of metabolites. Here, the user can select the metabolites from the dropdown option. Normalize by metadata column normalizes by any additional column specified in the metadata file. such as cell number etc. Normalize by control normalizes by control samples present in the data. Figure 28. Normalize by metadata options log2 y + log2(x) [where data is shifted by max value of data plus one] Figure 29. Scaling options Note: If internal standard(s) have already been selected in the Select Internal Standards option, they would be present in the drop down. Clicking on Run will perform the normalization and scaling based on the parameters selected. Table: This displays the data table and visualizations for both pre- and post- normalization. Metadata: This displays the metadata uploaded. This data can be downloaded in the .csv format as shown in Figure 23. Metabolite Mapping data: This displays the metabolite data uploaded. This data can be downloaded in the .csv format as shown in Figure 30. Raw data: This displays the raw El-MAVEN data uploaded. This data can be downloaded in the .gct format as shown in Figure 31. Processed data: This displays the normalized El-MAVEN data based on the parameters selected. This data can be downloaded in .gct format as shown in Figure 32. Pre-Processing Results: This allows you to have a look at the sample distribution with the help of density plot and box-plot before normalization as shown in Figure 33. Post-Processing Results: This allows you to have a look at the sample distribution with the help of the density plot and box-plot after normalization. This provides you with the ability to check the effect of the normalization parameters on the data as shown in Figure 34. Figure 30. Metadata table Figure 31. Raw data table Figure 31. Processed data table Figure 33. Pre-Processing Results Figure 34. Post-Processing Results Quality Checks This tab allows you to perform quality checks for the internal standards, metabolites and across samples with the help of interactive visualizations. Internal Standards It allows you to have a look at the quality of the internal standards used in the data with the help of the different visualizations for any individual as well as for all internal standards. Internal Standards (Individual): You can visualize the quality checks for any internal standard specifically. This allows you to select the internal standard by name, followed by another drop down to select by uniqueId of the feature. It\u2019s also possible to specify the cohort order for the plots. For dual mode data, you can specify the internal standard of the particular mode from the Select uniqueIds drop down. Figure 35. Internal Standards (Individual) options Figure 36. CV Distribution across cohorts Figure 37. CV Distribution across samples Metabolites: It allows you to have a look at the quality of the metabolites present in the data with the help of the Coefficient of Variation plots Metabolites CoV Boxplot visualizes the Coefficient of Variation across different cohorts in the data in the form of the boxplot. It\u2019s also possible to specify the cohort order for the plots as shown in Figure 38. Metabolites CoV Barplot visualizes the Coefficient of Variation as a quality check for any specific metabolite. To use this, select the metabolite followed by the unique id of the feature using the drop downs shown in Figure 39. It\u2019s also possible to specify the cohort order for the plots as shown in FIgure 40. Figure 38. Metabolites CoV Boxplot option and CV Distribution across Cohorts boxplot Figure 39. Metabolites CoV Barplot Figure 40. CV Distribution across cohorts for selected metabolite Figure 41. CV Distribution across samples for the selected metabolite PCA This allows you to understand the clustering pattern between biologically grouped and ungrouped samples. Figure 42. PCA option PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 43. Two-dimensional PCA plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 44. Three-dimensional PCA plot Loadings table displays the individual PC conponents across the uploaded features. Figure 45. Three-dimensional PCA plot Statistical Analysis This interface allows you to perform differential expression limma analysis with the aim to identify metabolites whose expression differs between any specified cohort conditions. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the two experimental conditions and an 'adjusted' p -value that rates the significance of the difference. Figure 46. Statistical Analysis interface The following parameters are available for selection: Select Cohort A and Cohort B: Default values are filled automatically for a selected cohort condition, which can be changed as per the cohorts of interest. Filter and categorize data: Select this to categorize the data by a column or filter data before performing differential expression. Categorize data by a column: Select this to categorize data by a column. The points will be grouped based on this column and will be assigned the same shape on the volcano plot. Select column for hover info: Select this to change the hoverinfo of each point on volcano plot. The info in this column will be displayed on when hovering over a point in volcano plot. Select text_label to display all information on the hoverinfo. Filter and categorize data: Select this to categorize the data by a column or filter data before performing differential expression. Add condition: Add a filtering condition by selecting a column from feature metadata and then selecting a value a from a row. Merge conditions: Merge two conditions created using the Add condition interface. This enables dynamic filtering by allowing you to merge any of the existing conditions. Select condition: Select a filtering condition created using the Add condition or Merge conditions interface. The condition selected here will be used to filter data before performing differential expression. Figure 47. Filtering Interface Select p-val or adj. p-val : Select either p -value or adj. p -value for significance. p-val or adj. p-val cutoff: By default, the value is 0.05 but can be changed if required. log2FC: Specify the cut-off for log 2 fold change with the help of the slider. Once the parameters are specified, click on the Update button to plot the volcano plot. Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 48. Volcano plot Filtered Metabolites Visualization provides the visualization of cohort-based distribution of the metabolites that are significant based on the parameters specified. Figure 49. Filtered Metabolites Visualization Filtered Normalized Table contains the normalized data of the metabolites that are significant based on the parameters specified. Figure 50. Filtered Normalized Table Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 51. Filtered Differential Expression Table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 52. Differential Expression Table Pathway Enrichment Analysis performs the pathway enrichment analysis for the significant metabolites based on the parameters specified for the particular cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 53. Metabolite Set Enrichment Analysis Plot Figure 54. Pathway Topology Analysis Plot Figure 55. Metabolite Set Enrichment Analysis Table Figure 56. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolites they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolism selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 57. Pathway View Plot Visualization This interface allows you to visualize the cohort-based distribution of a specific metabolite or a group of metabolites on the basis on its normalized intensity values. Figure 58. Visualization tab options Enter metabolite: Select the metabolite(s) of interest from the drop down option. Select uniqueIds: You can specifically select the metabolic feature of interest for the metabolite from the drop down option. Select order of cohort: You can also specify the particular order of the cohort to visualize the bar plot. Once the parameters are selected, click on Load Plots to plot the bar plot for the metabolite. Figure 59. Cohort-wise bar plot with the normalized intensity of selected metabolite Heatmap This tab allows you to produce a heatmap of the processed data, so that you can observe the level of expression in a visual form. Click on Load Heatmap button to generate the heatmap. Figure 60. Heatmap IntOmix Input This tab allows you to generate the input for IntOmix where you can visualize the significantly altered metabolic network modules between any two experimental conditions. Figure 61. IntOmix Input Tab options Specify two or more cohorts from the Select cohorts drop down option for which you want to generate the IntOmix input. Once the required cohorts are selected, click on Generate to generate the IntOmix input. Figure 62. IntOmix Table for the cohort conditions specified NOTE: At least two cohorts are required to create the input file. Comparative Analysis This tab allows you to perform comparative analysis between a set of cohorts in your data. As a result of which you can visualize the UpSet plot of the unique and overlapping metabolites for the selected cohort comparisons. Further, you can also perform pathway analysis on the metabolites for the set intersections of interest. Comparison Parameters tab allows you to select the cohorts of interest for which you would want to get the set intersections. You can select the cohorts from the Select cohorts drop-down and click on Run button. Further, you can also specify the p -value cut-off and log 2 FC threshold. Figure 63. Comparison Parameters Tab options You will get a table as a result of the parameters specified which will have the significant metabolites for the different cohort comparisons along with their corresponding p -values and log 2 FC values. You can also download this table as a .CSV file. Figure 64. Comparison Table UpSet Plot tab allows you to visualize the set intersections for the cohort comparisons selected where every comparison consists of the significant metabolites associated with the same. You can select the cohort comparisons of interest from the Select Cohort Comparison drop-down which represents all the possible comparisons for the cohorts specified in the previous tab. Click on Plot to get the UpSet plot for the specified comparisons. Figure 65. UpSet Plot options Figure 66. UpSet Plot Along with the plot, you can also get all the constituent metabolites for the respective comparisons in a tabular format that can be downloaded as a .CSV file. Figure 67. UpSet Plot Table Pathway Enrichment Analysis tab allows you to perform the pathway enrichment analysis for the significant metabolites that show up based on the parameters specified in the Comparison Parameters tab for the particular set of cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 68. Metabolite Set Enrichment Analysis Plot Figure 69. Pathway Topology Analysis Plot Figure 70. Metabolite Set Enrichment Analysis Table Figure 71. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolisms they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolite selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 72. Pathway View options Figure 73. Pathway View Plot References CAMERA: An Integrated Strategy for Compound Spectra Extraction and Annotation of Liquid Chromatography/Mass Spectrometry Data Sets Anal. Chem. 2012, 84, 1, 283-289","title":"Untargeted Pipeline"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#overview","text":"Untargeted Metabolomics, otherwise known as discovery metabolomics, analyzes the metabolomic profile globally from each sample thus producing voluminous and complex data. This needs robust bioinformatics tools to help meaningfully interpret this data. The Untargeted Pipeline enables you to perform the annotation and identification of the metabolites. It uses CAMERA , a package built for annotation of the adducts, isotopes, fragments and then maps features to a reference compound database (KEGG, HMDB or a custom database). The workflow begins with automated peak curation on El-MAVEN using the Untargeted algorithm and the peak table derived from this is used as input for PollyTM Untargeted Pipeline.","title":"Overview"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#scope-of-the-app","text":"Annotate adducts, isotopes and fragments in the data and identify metabolites Perform downstream analysis such as differential expression, anova test and pathway enrichment. Figure 1. Untargeted Pipeline","title":"Scope of the app"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#user-input","text":"Untargeted Pipeline can take a csv file are well as a emdb file as input. Emdb File The emdb file used here is a RSQLite database file generated when an El-MAVEN session is saved. It has all the unannotated features along with other features of the peaks. Intensity File The intensity file used here is the El-MAVEN output in peak detailed format. This output contains unannotated features along with their retention time and m/z information. Figure 2. El-MAVEN intensity file Metadata File The metadata file contains the sample to cohort mapping information that will be used in the downstream processing of the data. Figure 3. Metadata file","title":"User Input"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#steps-involved-in-data-processing","text":"Process raw data on El-MAVEN using automated feature detection. Export intensity file in peak detailed format. Annotate adducts, isotopes and fragments in the data. Perform identification of metabolites. Perform downstream analysis such as differential expression, anova test and pathway enrichment.","title":"Steps involved in data processing"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#caveats","text":"The input file should be the peak detailed output of El-MAVEN. The emdb file should be the one generated from El-MAVEN.","title":"Caveats"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#tutorial","text":"Go to the dashboard and select Untargeted Pipeline under the Metabolomcis Data tab. Create a New workspace or choose from the existing ones and provide a Name of the Session to be redirected to the upload page. Figure 4. Polly Dashboard Figure 5. Untargeted Pipeline","title":"Tutorial"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#upload-files","text":"The upload data tab allows you to upload El-MAVEN output in a csv format or an emdb file containing peak information along with the cohort file up to 300MB. Upload either the intensity file or an emdb file along with cohort file using the drop downs shown below, select the polarity of the data and then click on Load Data to proceed. Figure 6. Upload files","title":"Upload Files"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#annotation","text":"For annotation, we use the R package, CAMERA. It takes the output in peak detailed format from El-MAVEN which contains. The file should contain mzmin and mzmax details in the file. CAMERA CAMERA operates in the following steps: First it groups the features by their retention time It then forms groups by correlation inside samples (EIC) or correlation across samples or both After grouping these features, it annotates the possible isotopes and adducts. Figure 7. Annotation Advanced parameters The following parameters need to be set before running CAMERA: cor_exp_th: Correlation threshold for EIC correlation (Range: 0-1) pval: p -value threshold for testing correlation of significance (Range: 0-1) perfwhm: percentage of FWHM (Full Width at Half Maximum) width used in \" groupFWHM \" function for grouping features sigma: multiplier of the standard deviation used in \" groupFWHM \" function for grouping features calccis: Use correlation inside samples for peak grouping (TRUE/FALSE) calccas: Use correlation across samples for peak grouping (TRUE/FALSE) max_iso: maximum number of expected isotopes (0-8) minfrac: The percentage number of samples, which must satisfy the C12/C13 rule for isotope annotation ppm: General ppm error mzabs: General absolut error in m/z multiplier: If no ruleset is provided, calculate ruleset with max. number n of [nM+x] clusterions max_peaks: How much peaks will be calculated in every thread using the parallel mode maxcharge: maximum ion charge Figure 8. Advanced parameters Select adducts for annotation You can select adducts that are needed for annotation. Available Adducts Rules: The default adduct rules file is present which can be used for annotation. Upload Custom Adducts Rules: You can upload the custom adducts rules file otherwise. The adducts rules file has the following columns: name: adduct name nmol: Number of molecules (xM) included in the molecule charge: charge of the molecule massdiff: mass difference without calculation of charge and nmol (CAMERA will do this automatically) oidscore: This score is the adduct index. Molecules with the same cations (anions) configuration and different nmol values have the same oidscore, such as [M+H] and [2M+H] quasi: Every annotation which belongs to one molecule is called annotation group. Examples for these are [M+H] and [M+Na], where M is the same molecule. An annotation group must include at least one ion with quasi set to 1 for this adduct. If an annotation group only includes optional adducts (rule set to 0) then this group is excluded. To disable this reduction, set all rules to 1 or 0. ips: This is the rule score. If a peak is related to more than one annotation group, then the group having a higher score (sum of all annotations) gets picked. This effectively reduces the number of false positives. Figure 9. Select adducts for annotation CAMERA output table After annotation CAMERA adds three columns i.e. isotopes, adducts and pcgroup. The isotopes column contains the annotation for the isotopes where annotation is in the format of \"[id][isotope]charge\" for example [1][M]+, [1][M+1]+, [2][M+3]+. The adduct column contains the annotation for the adducts where annotation is in the format of \"[adduct] charge basemass\" for example [M+H]+ 161.105, [M+K]+ 123.15 etc. The pcgroup column contains the \u2018pseudospectra\u2019 which means features are grouped based on rt and correlation (inside and across samples). Figure 10. CAMERA output table Restructured CAMERA output table The features in the CAMERA output are not grouped together according to the pcgroup because it only appends the new columns in the existing feature table without changing the order of the features and also features which are different molecules may have the same pcgroup . So to interpret the results better it is necessary to separate the features which belong to different molecules within the same pcgroup . To overcome the above problem, there is another label of grouping based on the features belonging to the same molecule. The other label of grouping is done by assuming that the features which are representing the same molecule should have the same basemass. The following operations are performed to make the restructured CAMERA output: The adduct column is split into two new columns i.e. adduct_type and basemass . If any feature has more than one combination of adduct_type and basemass for example \"[M+K]+ 123.15 [M+Na]+ 139.124 [M+H]+ 161.105\" then they are split into separate rows having other information same. The isotopes column is split into two new columns i.e. isotope_id and isotope_type . A new column feature_group is added in the existing table where each value represents a different molecule. The feature_group column is filled based on the following assumptions: Features having basemass will have the same feature_group id. Those features which do not have the adducts (which CAMERA could not annotate) will be filled by [M+H]+ (in positive mode) or [M-H]- (in negative mode) as adduct and based on this adduct information the adduct_type and basemass columns will be filled. Features having the same isotope_id will be grouped in the same feature_group id. The same pcgroup may have more than one feature_group ids if it has more than one molecule. Figure 11. Restructured CAMERA ouput table Representatiove output table After restructuring the CAMERA output it is necessary to define the representative feature from the features belonging to the same feature_group id because since these features belong to the same molecule so there is no need to include all features in the identification step. The representative feature is defined based on the following assumptions: If the feature group has only one feature then that feature will be considered as the representative feature. The feature group has more than one feature then the representative feature will be determined by the following criteria: If the feature group has [M+H] (in positive mode) or [M-H] (in negative mode) then that feature will be considered as the representative feature. If the feature group does not have [M+H] (in positive mode) or [M-H] (in negative mode) then that feature will be considered as representative whose sum of intensity across all samples is maximum. Figure 12. Representative CAMERA ouput table Summary of annotation The data is summarized on the basis of the number of features within pcgroup and feature group: Number of features vs counts of pcgroup Figure 13. Number of features vs counts of pcgroup Number of features vs counts of feature groups Figure 14. Number of features vs counts of feature groups","title":"Annotation"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#identification","text":"The identification is performed on the representative table only. The representative features are searched against the compound database uploaded. It uses the basemass instead mZ for mass searching because adducts and isotopes are already filtered in the above steps. Figure 15. Advanced parameters Figure 16. Identification Representative metabolite identification table The representative table is appended by the compound database columns after identification. Figure 17. Representative metabolite identification table Overall metabolite identification table The representative metabolite identification table is again merged to the restructured camera output. Figure 18. Overall metabolite identification table El-Maven format The results are generated by converting the representative and overall tables in the group summary format. Figure 19. El-Maven output(Group Summary format)","title":"Identification"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#gct-preparation","text":"This interface allows you to select the data to be used in the downstream pipeline. The user can use any data to the downstream analysis by selecting the data from the previous steps. Figure 20. GCT-Preparation Tab Uploaded data: This allows you to take the uploaded data to the downsteam pipeline. Annotated data: This allows you to take the annotated data to the downsteam pipeline. You can either take the representative data or the overall data to the downsteam pipeline. Identified data: This allows you to take the identified data to the downsteam pipeline. You can either take the representative data or the overall data to the downsteam pipeline. Figure 21. Data downstream Representative: This allows you to select the representative data from either the annotated data or identified data depending on the above selection. Overall: This allows you to select the overall data from either the annotated data or identified data depending on the above selection. Figure 22. Data type","title":"GCT-Preparation"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#pre-processing","text":"The Pre-processing interface allows you to perform a multitude of functions on the data such as: Figure 23. Preprocessing Tab Select Internal Standards: This allows you to select the internal standard(s) from within the El-MAVEN output file when a separate internal standards file is not provided as input. Note: In case, the internal standard(s) are not in the El-MAVEN output file but in the separate internal standards file, they will not show up in the drop down menu. To select the desired internal standards, select them in Normalize by individual internal standards option under Normalize by Internal standards in the Perform Normalization > Normalization . Figure 24. Selecting internal standards from the data Drop Samples: This allows you to drop/remove certain samples from further analysis which could be blank samples or any samples that didn\u2019t have a good run during MS processing. Samples can be dropped by clicking on Drop Samples as shown in Figure 24 after selecting the sample(s) from the drop down menu. Figure 25. Drop Samples option Normalize by Internal standards performs normalization using the internal standards. Figure 26. Normalize by internal standards options Normalize by sum of internal standards normalizes by the sum of the standards provided. Normalize by average of internal standards normalizes by the average of the standards provided. Normalize by individual internal standards normalizes by the internal standards selected previously. Normalize by metabolites normalizes by any particular metabolite selected. Figure 27. Normalize by metabolites option Normalize by sum of metabolites normalizes by the sum of metabolites. Here, the user can select the metabolites from the dropdown option. Normalize by metadata column normalizes by any additional column specified in the metadata file. such as cell number etc. Normalize by control normalizes by control samples present in the data. Figure 28. Normalize by metadata options log2 y + log2(x) [where data is shifted by max value of data plus one] Figure 29. Scaling options Note: If internal standard(s) have already been selected in the Select Internal Standards option, they would be present in the drop down. Clicking on Run will perform the normalization and scaling based on the parameters selected. Table: This displays the data table and visualizations for both pre- and post- normalization. Metadata: This displays the metadata uploaded. This data can be downloaded in the .csv format as shown in Figure 23. Metabolite Mapping data: This displays the metabolite data uploaded. This data can be downloaded in the .csv format as shown in Figure 30. Raw data: This displays the raw El-MAVEN data uploaded. This data can be downloaded in the .gct format as shown in Figure 31. Processed data: This displays the normalized El-MAVEN data based on the parameters selected. This data can be downloaded in .gct format as shown in Figure 32. Pre-Processing Results: This allows you to have a look at the sample distribution with the help of density plot and box-plot before normalization as shown in Figure 33. Post-Processing Results: This allows you to have a look at the sample distribution with the help of the density plot and box-plot after normalization. This provides you with the ability to check the effect of the normalization parameters on the data as shown in Figure 34. Figure 30. Metadata table Figure 31. Raw data table Figure 31. Processed data table Figure 33. Pre-Processing Results Figure 34. Post-Processing Results","title":"Pre-processing"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#quality-checks","text":"This tab allows you to perform quality checks for the internal standards, metabolites and across samples with the help of interactive visualizations. Internal Standards It allows you to have a look at the quality of the internal standards used in the data with the help of the different visualizations for any individual as well as for all internal standards. Internal Standards (Individual): You can visualize the quality checks for any internal standard specifically. This allows you to select the internal standard by name, followed by another drop down to select by uniqueId of the feature. It\u2019s also possible to specify the cohort order for the plots. For dual mode data, you can specify the internal standard of the particular mode from the Select uniqueIds drop down. Figure 35. Internal Standards (Individual) options Figure 36. CV Distribution across cohorts Figure 37. CV Distribution across samples Metabolites: It allows you to have a look at the quality of the metabolites present in the data with the help of the Coefficient of Variation plots Metabolites CoV Boxplot visualizes the Coefficient of Variation across different cohorts in the data in the form of the boxplot. It\u2019s also possible to specify the cohort order for the plots as shown in Figure 38. Metabolites CoV Barplot visualizes the Coefficient of Variation as a quality check for any specific metabolite. To use this, select the metabolite followed by the unique id of the feature using the drop downs shown in Figure 39. It\u2019s also possible to specify the cohort order for the plots as shown in FIgure 40. Figure 38. Metabolites CoV Boxplot option and CV Distribution across Cohorts boxplot Figure 39. Metabolites CoV Barplot Figure 40. CV Distribution across cohorts for selected metabolite Figure 41. CV Distribution across samples for the selected metabolite PCA This allows you to understand the clustering pattern between biologically grouped and ungrouped samples. Figure 42. PCA option PCA (2D) provides PCA visualization in a two-dimensional manner by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 43. Two-dimensional PCA plot PCA (3D) provides PCA visualization in a three-dimensional manner by selecting the PC values for x- , y- and z- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 44. Three-dimensional PCA plot Loadings table displays the individual PC conponents across the uploaded features. Figure 45. Three-dimensional PCA plot","title":"Quality Checks"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#statistical-analysis","text":"This interface allows you to perform differential expression limma analysis with the aim to identify metabolites whose expression differs between any specified cohort conditions. The 'limma' R package is used to identify the differentially expressed metabolites. This method creates a log 2 fold change ratio between the two experimental conditions and an 'adjusted' p -value that rates the significance of the difference. Figure 46. Statistical Analysis interface The following parameters are available for selection: Select Cohort A and Cohort B: Default values are filled automatically for a selected cohort condition, which can be changed as per the cohorts of interest. Filter and categorize data: Select this to categorize the data by a column or filter data before performing differential expression. Categorize data by a column: Select this to categorize data by a column. The points will be grouped based on this column and will be assigned the same shape on the volcano plot. Select column for hover info: Select this to change the hoverinfo of each point on volcano plot. The info in this column will be displayed on when hovering over a point in volcano plot. Select text_label to display all information on the hoverinfo. Filter and categorize data: Select this to categorize the data by a column or filter data before performing differential expression. Add condition: Add a filtering condition by selecting a column from feature metadata and then selecting a value a from a row. Merge conditions: Merge two conditions created using the Add condition interface. This enables dynamic filtering by allowing you to merge any of the existing conditions. Select condition: Select a filtering condition created using the Add condition or Merge conditions interface. The condition selected here will be used to filter data before performing differential expression. Figure 47. Filtering Interface Select p-val or adj. p-val : Select either p -value or adj. p -value for significance. p-val or adj. p-val cutoff: By default, the value is 0.05 but can be changed if required. log2FC: Specify the cut-off for log 2 fold change with the help of the slider. Once the parameters are specified, click on the Update button to plot the volcano plot. Based on the parameters specified, a volcano plot is displayed. The volcano plot helps in visualizing metabolites that are significantly dysregulated between two cohorts. Figure 48. Volcano plot Filtered Metabolites Visualization provides the visualization of cohort-based distribution of the metabolites that are significant based on the parameters specified. Figure 49. Filtered Metabolites Visualization Filtered Normalized Table contains the normalized data of the metabolites that are significant based on the parameters specified. Figure 50. Filtered Normalized Table Filtered Differential Expression Table contains only the metabolites that have significant p -values as specified. Figure 51. Filtered Differential Expression Table Differential Expression Table contains all the differentially expressed metabolites without any filtering. Figure 52. Differential Expression Table Pathway Enrichment Analysis performs the pathway enrichment analysis for the significant metabolites based on the parameters specified for the particular cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 53. Metabolite Set Enrichment Analysis Plot Figure 54. Pathway Topology Analysis Plot Figure 55. Metabolite Set Enrichment Analysis Table Figure 56. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolites they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolism selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 57. Pathway View Plot","title":"Statistical Analysis"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#visualization","text":"This interface allows you to visualize the cohort-based distribution of a specific metabolite or a group of metabolites on the basis on its normalized intensity values. Figure 58. Visualization tab options Enter metabolite: Select the metabolite(s) of interest from the drop down option. Select uniqueIds: You can specifically select the metabolic feature of interest for the metabolite from the drop down option. Select order of cohort: You can also specify the particular order of the cohort to visualize the bar plot. Once the parameters are selected, click on Load Plots to plot the bar plot for the metabolite. Figure 59. Cohort-wise bar plot with the normalized intensity of selected metabolite","title":"Visualization"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#heatmap","text":"This tab allows you to produce a heatmap of the processed data, so that you can observe the level of expression in a visual form. Click on Load Heatmap button to generate the heatmap. Figure 60. Heatmap","title":"Heatmap"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#intomix-input","text":"This tab allows you to generate the input for IntOmix where you can visualize the significantly altered metabolic network modules between any two experimental conditions. Figure 61. IntOmix Input Tab options Specify two or more cohorts from the Select cohorts drop down option for which you want to generate the IntOmix input. Once the required cohorts are selected, click on Generate to generate the IntOmix input. Figure 62. IntOmix Table for the cohort conditions specified NOTE: At least two cohorts are required to create the input file.","title":"IntOmix Input"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#comparative-analysis","text":"This tab allows you to perform comparative analysis between a set of cohorts in your data. As a result of which you can visualize the UpSet plot of the unique and overlapping metabolites for the selected cohort comparisons. Further, you can also perform pathway analysis on the metabolites for the set intersections of interest. Comparison Parameters tab allows you to select the cohorts of interest for which you would want to get the set intersections. You can select the cohorts from the Select cohorts drop-down and click on Run button. Further, you can also specify the p -value cut-off and log 2 FC threshold. Figure 63. Comparison Parameters Tab options You will get a table as a result of the parameters specified which will have the significant metabolites for the different cohort comparisons along with their corresponding p -values and log 2 FC values. You can also download this table as a .CSV file. Figure 64. Comparison Table UpSet Plot tab allows you to visualize the set intersections for the cohort comparisons selected where every comparison consists of the significant metabolites associated with the same. You can select the cohort comparisons of interest from the Select Cohort Comparison drop-down which represents all the possible comparisons for the cohorts specified in the previous tab. Click on Plot to get the UpSet plot for the specified comparisons. Figure 65. UpSet Plot options Figure 66. UpSet Plot Along with the plot, you can also get all the constituent metabolites for the respective comparisons in a tabular format that can be downloaded as a .CSV file. Figure 67. UpSet Plot Table Pathway Enrichment Analysis tab allows you to perform the pathway enrichment analysis for the significant metabolites that show up based on the parameters specified in the Comparison Parameters tab for the particular set of cohort comparison. Click on the Perform Pathway Analysis button. As a result, you get Metabolite Set Enrichment Analysis and Pathway Topology Analysis plots that can be downloaded under the Plot panel. You can also obtain the tablular representation of the plots by selecting onto the Table panel. Figure 68. Metabolite Set Enrichment Analysis Plot Figure 69. Pathway Topology Analysis Plot Figure 70. Metabolite Set Enrichment Analysis Table Figure 71. Pathway Topology Analysis Table Pathway View plots the pathway view of the metabolites that show up in the Metabolite Set Enrichment Analysis. It maps and renders the metabolite hits on relevant pathway graphs. This enables you to visualize the significant metabolites on pathway graphs of the respective metabolisms they belong to. You can select your metabolite of interest from the drop-down and click on Plot . This will plot the pathway view of the metabolite selected. You can also download the plot as a .png file by clicking onto the Download Pathview Plot button. Figure 72. Pathway View options Figure 73. Pathway View Plot","title":"Comparative Analysis"},{"location":"Apps/Metabolomic Data/Untargeted Pipeline.html#references","text":"CAMERA: An Integrated Strategy for Compound Spectra Extraction and Annotation of Liquid Chromatography/Mass Spectrometry Data Sets Anal. Chem. 2012, 84, 1, 283-289","title":"References"},{"location":"Apps/Multi-omic Data/IntOmix.html","text":"Introduction Overview The recent development of various omics technologies in biology has increased the systems level analytical methods. It includes complete genome, transcriptome, proteome and metabolome analysis with various high-throughput technologies, including microarrays, Next-Generation Sequencing (NGS) and mass spectrometry. Multi-omics data integration provides an opportunity to understand the cellular or tissue level environment as a whole. IntOmix is an application specifically geared towards integrated network analysis of multi-omics with a focus on the identification of significantly changing metabolic network modules between two experimental conditions. The integration of the data generated by these two approaches can give simultaneous information about metabolomic changes along with the transcriptional changes. Scope of the App Go from differentially expressed data to visualisations of pathway-level changes in less than 5 minutes. Process single or multi-omics data. Search for publications based on keywords added and significantly perturbed genes or metabolites by adding/editing keywords before or after optimization. Compare insights obtained from the data with existing literature. Compare results obtained from the same data at different parameters. Download results as publication-quality figures in various formats such as .png, .jpeg, .pdf or .xgmml. Figure 1. IntOmix Getting Started User input IntOmix requires two types of .csv input files: Metabolite differential expression file Gene differential expression file Both the input files consist of information about log 2 fold change and the p -value of each gene or metabolite. In case of comparisons between multiple cohorts, you can also provide the name of the cohorts in the columns state1 and state2 as shown in Figure 2. You can also use files without the cohort information as shown in Figure 3. Figure 2. Sample input files Figure 3. Files without cohort information Steps in data processing Upload differentially expressed metabolomics/transcriptomics/genomics files. Set parameters to perform the optimization of the network. Perform Pathway Enrichment analysis. Perform cross-cohort comparison to find common genes and metabolites across the various analysis. Note: You can run the app using any single file as input if required. Caveats KEGG or HMDB ID for metabolites and gene id for genes is required to map these on the pathway map. If state 1 and state 2 columns are not present in the input files, the entire data will be treated as if from one comparison with the default cohort names, Cohort1 and Cohort2 . In case there are blank cells for p -value or log 2 fold change in the input files, the corresponding metabolites or genes are dropped before optimization and a notice is shown to that effect. Currently, we only support reactions from Homo sapiens , Mus musculus , Drosophila melanogaster and Chinese hamster ovary. At a time only one keyword can be annotated as the primary keyword. Publication Search module requires at least one keyword to be annotated as the primary keyword. To utilize the Cross Cohort Comparison functionality, run your analysis at least twice at different parameters. Tutorial Select IntOmix from the dashboard under the Multi-omic Data Tab as shown in Figure 4. Create a New Workspace or choose an existing one from thr drop-down and provide a Name of the Session to be redirected to IntOmix's upload page. Figure 4. Polly Dashboard and Workspace selection Upload files The file upload interface allows you to upload input files to be processed in IntOmix as shown in Figure 5. Figure 5. File Upload interface Click on Upload .csv Files to upload the differential expression files in the format mentioned above and then click on Continue to Dashboard. Parameter Interface This interface allows you to adjust parameters to be used to generate the optimized network. You can select cohorts as per your requirements. The organism will be detected automatically based on data input if the gene file has been uploaded. You can either proceed with the default parameters or specify the values. To know more about the significance of the parameters, refer to the Details of the App section. To isolate keywords from a paragraph, copy it on the left side of the Add Keywords section and click on Filter Keywords . The isolated keywords will be displayed below. To add specific keywords, write them on the right side of the Add Keywords section separated by a comma and click on Add Keywords. Click on any particular keyword to annotate it as the primary keyword. The primary keyword selection can be changed on the next interface as well as we can see in Figure 6. After specifying all the required parameters, click on Get Started to process your data. Figure 6. Set parameter interface Note: Processing of data requires as much time as specified to optimize the significantly perturbed networks present in the data. Figure 7. Optimization and map generation Pathway Dashboard After data processing, a KEGG map is generated based on the input parameters. The most significantly changing genes and metabolites between the two conditions are captured in the form of a network module. This network module is overlaid on the KEGG pathway to effectively visualize the overall topology of the variations between the two conditions. Hovering on any node or edge will display a dialog box with information about the selected gene or metabolite as shown in Figure 8. You can also zoom in, zoom out, select nodes or edges on the interactive map or download it in one of the several formats available. Figure 8. Hovering on metabolites and genes provides further information. You can add all metabolites and genes which do not satisfy the parameters specified for the optimized network. Option and Information In the Option & Information pop-up window as shown in Figure 9, you can select All Others option to add all genes and metabolites in the data to the generated map. The genes and metabolites have color ranging from red-green scale . All upregulated metabolites or genes are represented by a shade of red and downregulated metabolites or genes as a shade of green. The missing metabolites and genes are represented by shade of blue. Figure 9. Pathway map containing all metabolites and genes in the input files. Analyze your data After the KEGG network is created, the predefined set of queries can be selected from Analyze your Data option which gives you more insights on the results derived. What are the key pathways? This option enables you to find the key pathways that are perturbed in the data. The results of this query are based on two scoring methods; pathway enrichment and topology analysis. Pathway enrichment analysis refers to the quantitative approach based on the concentration of metabolites present. Gene set enrichment analysis is one of the famous algorithms used for enrichment analysis. In Polly TM IntOmix, R packages such as globaltest and GlobalAncova are used to analyze the pathway enrichment. Similarly, for topology analysis, betweenness centrality and degree centrality are considered as key parameters to find the most influential and important pathways in the whole subnetwork. Figure 10. Pathway Enrichment module How do publicly available results correlate with my results? This question enables you to select any gene or metabolite that is significantly differentially expressed in their data along with the specified primary keyword. Using both of these as the input, IntOmix will display a list of relevant publications that you can read more to gain insights about their data and generate hypotheses. Figure 11. Publication Search module. How do my other comparisons compare with my results? Using this question you can compare the analysis of the same data using different parameters. The output of this question is displayed as a Venn diagram wherein the intersection contains the common genes and metabolites found in both the set of analyses. This helps you understand how a single parameter or a group of parameters affect their analysis. Figure 12. Cross Cohort Comparison Watch the demo video below for more information. Details about the app IntOmix is based on CoMBI-T i.e., Concordant Metabolomics Integration with Transcription pipeline to utilize the multiple omics techniques (metabolomics and transcriptomics) to focus on the identification of significantly altering metabolic network modules between two experimental conditions under investigation. This network includes both metabolites and enzymes. The most co-ordinately responsive sub-network is identified using the integrated network analysis and also, the novel points of metabolic rewiring are revealed. Finding a module After making a network, one can find a connected module that contains the most significantly changed genes and metabolites. Internally, this is done by first scoring nodes and edges based on their p -values in such a way that positive scores correspond to significant p -values and negative scores correspond to insignificant changes. Then the problem of finding a connected subgraph with maximum summary weight is solved. The FDR values and Score for absent metabolites options control the size of the module. Increasing/decreasing the FDR for reactions ( FDR for metabolites ) value makes adding reactions (metabolites) to a module easier/harder. It is recommended to start from the default values and then gradually change them depending on the results. Default FDR values are generated so that the module will be the size of ~100 reactions. Videos References Sergushichev, Alexey A., et al. \"GAM: a web-service for integrated transcriptional and metabolic network analysis.\" Nucleic acids research 44.W1 (2016): W194-W200. Goeman, Jelle J., and Peter B\u00fchlmann. \"Analyzing gene expression data in terms of gene sets: methodological issues.\" Bioinformatics 23.8 (2007): 980-987. Hummel, Manuela, Reinhard Meister, and Ulrich Mansmann. \"GlobalANCOVA: exploration and assessment of gene group effects.\" Bioinformatics 24.1 (2007): 78-85. Jha, Abhishek K., et al. \"Network integration of parallel metabolic and transcriptional data reveals metabolic modules that regulate macrophage polarization.\" Immunity 42.3 (2015): 419-430. Frequently Asked Questions (FAQs) S. No. Question Answer 1. What is Polly TM IntOmix? Polly TM IntOmix is an application specifically geared towards integrated network analysis of multi-omics with a focus on the identification of significantly changing metabolic network modules between two experimental conditions. 2. How can I test Polly TM IntOmix if I don't have my own data? You can download the demo data set by clicking on the Download Demo Files button on Polly TM IntOmix Upload page. 3. Can I run Polly TM IntOmix if I only have the metabolomics or transcriptomics data? Yes. Even though Polly TM IntOmix is designed for integrating multi-omics data, you can run the metabolomics or transcriptomics data by itself. 4. What is the function of the Filter Keywords tab? Filter Keywords is a function that filters keywords from any text or paragraph input. You can copy and paste text and then click on Filter Keywords. 5. Can I add keywords manually? Yes, you can add keywords manually and then click on Add Keywords. 6. How many keywords can be selected at one time? There is no limit to adding keywords however, at a time only one keyword can be selected as the primary keyword. The primary keyword can be changed by clicking on Edit without optimizing the pathway map. 7. How can I get more information about any gene or metabolite once the map is generated? You can hover over a particular node or edge to get more details about the component of interest. 8. How can I visualize all of the genes or metabolites that are present in my data but not in the optimized network? Clicking on the pop-up window on the right side of the screen opens up a menu named Option & Information in which selecting All Others will display all the genes and metabolites present in the data on the pathway map. The genes and metabolites that satisfy the parameters selected in the previous menu have a color palette of red-green and the ones that do not are displayed as shades of purple. 9. How can I see the key pathways present in my data? Clicking on the pop-up window on the left side of the screen opens up a menu named Analyze your data which contains a list of questions that you can ask regarding their data. Clicking on What are the key pathways will open the Pathway Enrichment module that assigns a score to perturbed pathways based on enrichment and topology. 10. How can I effectively use the query \"How do publicly available results correlate with my results?\" to get insights into my data? To use this feature effectively, you need to select one keyword as mentioned on the parameter page and one gene or metabolite of interest from the target list of the generated map. This combination (keyword and gene/metabolite) will be searched for in public databases to get the most relevant publications.","title":"IntOmix"},{"location":"Apps/Multi-omic Data/IntOmix.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Multi-omic Data/IntOmix.html#overview","text":"The recent development of various omics technologies in biology has increased the systems level analytical methods. It includes complete genome, transcriptome, proteome and metabolome analysis with various high-throughput technologies, including microarrays, Next-Generation Sequencing (NGS) and mass spectrometry. Multi-omics data integration provides an opportunity to understand the cellular or tissue level environment as a whole. IntOmix is an application specifically geared towards integrated network analysis of multi-omics with a focus on the identification of significantly changing metabolic network modules between two experimental conditions. The integration of the data generated by these two approaches can give simultaneous information about metabolomic changes along with the transcriptional changes.","title":"Overview"},{"location":"Apps/Multi-omic Data/IntOmix.html#scope-of-the-app","text":"Go from differentially expressed data to visualisations of pathway-level changes in less than 5 minutes. Process single or multi-omics data. Search for publications based on keywords added and significantly perturbed genes or metabolites by adding/editing keywords before or after optimization. Compare insights obtained from the data with existing literature. Compare results obtained from the same data at different parameters. Download results as publication-quality figures in various formats such as .png, .jpeg, .pdf or .xgmml. Figure 1. IntOmix","title":"Scope of the App"},{"location":"Apps/Multi-omic Data/IntOmix.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Multi-omic Data/IntOmix.html#user-input","text":"IntOmix requires two types of .csv input files: Metabolite differential expression file Gene differential expression file Both the input files consist of information about log 2 fold change and the p -value of each gene or metabolite. In case of comparisons between multiple cohorts, you can also provide the name of the cohorts in the columns state1 and state2 as shown in Figure 2. You can also use files without the cohort information as shown in Figure 3. Figure 2. Sample input files Figure 3. Files without cohort information","title":"User input"},{"location":"Apps/Multi-omic Data/IntOmix.html#steps-in-data-processing","text":"Upload differentially expressed metabolomics/transcriptomics/genomics files. Set parameters to perform the optimization of the network. Perform Pathway Enrichment analysis. Perform cross-cohort comparison to find common genes and metabolites across the various analysis. Note: You can run the app using any single file as input if required.","title":"Steps in data processing"},{"location":"Apps/Multi-omic Data/IntOmix.html#caveats","text":"KEGG or HMDB ID for metabolites and gene id for genes is required to map these on the pathway map. If state 1 and state 2 columns are not present in the input files, the entire data will be treated as if from one comparison with the default cohort names, Cohort1 and Cohort2 . In case there are blank cells for p -value or log 2 fold change in the input files, the corresponding metabolites or genes are dropped before optimization and a notice is shown to that effect. Currently, we only support reactions from Homo sapiens , Mus musculus , Drosophila melanogaster and Chinese hamster ovary. At a time only one keyword can be annotated as the primary keyword. Publication Search module requires at least one keyword to be annotated as the primary keyword. To utilize the Cross Cohort Comparison functionality, run your analysis at least twice at different parameters.","title":"Caveats"},{"location":"Apps/Multi-omic Data/IntOmix.html#tutorial","text":"Select IntOmix from the dashboard under the Multi-omic Data Tab as shown in Figure 4. Create a New Workspace or choose an existing one from thr drop-down and provide a Name of the Session to be redirected to IntOmix's upload page. Figure 4. Polly Dashboard and Workspace selection","title":"Tutorial"},{"location":"Apps/Multi-omic Data/IntOmix.html#upload-files","text":"The file upload interface allows you to upload input files to be processed in IntOmix as shown in Figure 5. Figure 5. File Upload interface Click on Upload .csv Files to upload the differential expression files in the format mentioned above and then click on Continue to Dashboard.","title":"Upload files"},{"location":"Apps/Multi-omic Data/IntOmix.html#parameter-interface","text":"This interface allows you to adjust parameters to be used to generate the optimized network. You can select cohorts as per your requirements. The organism will be detected automatically based on data input if the gene file has been uploaded. You can either proceed with the default parameters or specify the values. To know more about the significance of the parameters, refer to the Details of the App section. To isolate keywords from a paragraph, copy it on the left side of the Add Keywords section and click on Filter Keywords . The isolated keywords will be displayed below. To add specific keywords, write them on the right side of the Add Keywords section separated by a comma and click on Add Keywords. Click on any particular keyword to annotate it as the primary keyword. The primary keyword selection can be changed on the next interface as well as we can see in Figure 6. After specifying all the required parameters, click on Get Started to process your data. Figure 6. Set parameter interface Note: Processing of data requires as much time as specified to optimize the significantly perturbed networks present in the data. Figure 7. Optimization and map generation","title":"Parameter Interface"},{"location":"Apps/Multi-omic Data/IntOmix.html#pathway-dashboard","text":"After data processing, a KEGG map is generated based on the input parameters. The most significantly changing genes and metabolites between the two conditions are captured in the form of a network module. This network module is overlaid on the KEGG pathway to effectively visualize the overall topology of the variations between the two conditions. Hovering on any node or edge will display a dialog box with information about the selected gene or metabolite as shown in Figure 8. You can also zoom in, zoom out, select nodes or edges on the interactive map or download it in one of the several formats available. Figure 8. Hovering on metabolites and genes provides further information. You can add all metabolites and genes which do not satisfy the parameters specified for the optimized network.","title":"Pathway Dashboard"},{"location":"Apps/Multi-omic Data/IntOmix.html#option-and-information","text":"In the Option & Information pop-up window as shown in Figure 9, you can select All Others option to add all genes and metabolites in the data to the generated map. The genes and metabolites have color ranging from red-green scale . All upregulated metabolites or genes are represented by a shade of red and downregulated metabolites or genes as a shade of green. The missing metabolites and genes are represented by shade of blue. Figure 9. Pathway map containing all metabolites and genes in the input files.","title":"Option and Information"},{"location":"Apps/Multi-omic Data/IntOmix.html#analyze-your-data","text":"After the KEGG network is created, the predefined set of queries can be selected from Analyze your Data option which gives you more insights on the results derived. What are the key pathways? This option enables you to find the key pathways that are perturbed in the data. The results of this query are based on two scoring methods; pathway enrichment and topology analysis. Pathway enrichment analysis refers to the quantitative approach based on the concentration of metabolites present. Gene set enrichment analysis is one of the famous algorithms used for enrichment analysis. In Polly TM IntOmix, R packages such as globaltest and GlobalAncova are used to analyze the pathway enrichment. Similarly, for topology analysis, betweenness centrality and degree centrality are considered as key parameters to find the most influential and important pathways in the whole subnetwork. Figure 10. Pathway Enrichment module How do publicly available results correlate with my results? This question enables you to select any gene or metabolite that is significantly differentially expressed in their data along with the specified primary keyword. Using both of these as the input, IntOmix will display a list of relevant publications that you can read more to gain insights about their data and generate hypotheses. Figure 11. Publication Search module. How do my other comparisons compare with my results? Using this question you can compare the analysis of the same data using different parameters. The output of this question is displayed as a Venn diagram wherein the intersection contains the common genes and metabolites found in both the set of analyses. This helps you understand how a single parameter or a group of parameters affect their analysis. Figure 12. Cross Cohort Comparison Watch the demo video below for more information.","title":"Analyze your data"},{"location":"Apps/Multi-omic Data/IntOmix.html#details-about-the-app","text":"IntOmix is based on CoMBI-T i.e., Concordant Metabolomics Integration with Transcription pipeline to utilize the multiple omics techniques (metabolomics and transcriptomics) to focus on the identification of significantly altering metabolic network modules between two experimental conditions under investigation. This network includes both metabolites and enzymes. The most co-ordinately responsive sub-network is identified using the integrated network analysis and also, the novel points of metabolic rewiring are revealed.","title":"Details about the app"},{"location":"Apps/Multi-omic Data/IntOmix.html#finding-a-module","text":"After making a network, one can find a connected module that contains the most significantly changed genes and metabolites. Internally, this is done by first scoring nodes and edges based on their p -values in such a way that positive scores correspond to significant p -values and negative scores correspond to insignificant changes. Then the problem of finding a connected subgraph with maximum summary weight is solved. The FDR values and Score for absent metabolites options control the size of the module. Increasing/decreasing the FDR for reactions ( FDR for metabolites ) value makes adding reactions (metabolites) to a module easier/harder. It is recommended to start from the default values and then gradually change them depending on the results. Default FDR values are generated so that the module will be the size of ~100 reactions.","title":"Finding a module"},{"location":"Apps/Multi-omic Data/IntOmix.html#videos","text":"","title":"Videos"},{"location":"Apps/Multi-omic Data/IntOmix.html#references","text":"Sergushichev, Alexey A., et al. \"GAM: a web-service for integrated transcriptional and metabolic network analysis.\" Nucleic acids research 44.W1 (2016): W194-W200. Goeman, Jelle J., and Peter B\u00fchlmann. \"Analyzing gene expression data in terms of gene sets: methodological issues.\" Bioinformatics 23.8 (2007): 980-987. Hummel, Manuela, Reinhard Meister, and Ulrich Mansmann. \"GlobalANCOVA: exploration and assessment of gene group effects.\" Bioinformatics 24.1 (2007): 78-85. Jha, Abhishek K., et al. \"Network integration of parallel metabolic and transcriptional data reveals metabolic modules that regulate macrophage polarization.\" Immunity 42.3 (2015): 419-430.","title":"References"},{"location":"Apps/Multi-omic Data/IntOmix.html#frequently-asked-questions-faqs","text":"S. No. Question Answer 1. What is Polly TM IntOmix? Polly TM IntOmix is an application specifically geared towards integrated network analysis of multi-omics with a focus on the identification of significantly changing metabolic network modules between two experimental conditions. 2. How can I test Polly TM IntOmix if I don't have my own data? You can download the demo data set by clicking on the Download Demo Files button on Polly TM IntOmix Upload page. 3. Can I run Polly TM IntOmix if I only have the metabolomics or transcriptomics data? Yes. Even though Polly TM IntOmix is designed for integrating multi-omics data, you can run the metabolomics or transcriptomics data by itself. 4. What is the function of the Filter Keywords tab? Filter Keywords is a function that filters keywords from any text or paragraph input. You can copy and paste text and then click on Filter Keywords. 5. Can I add keywords manually? Yes, you can add keywords manually and then click on Add Keywords. 6. How many keywords can be selected at one time? There is no limit to adding keywords however, at a time only one keyword can be selected as the primary keyword. The primary keyword can be changed by clicking on Edit without optimizing the pathway map. 7. How can I get more information about any gene or metabolite once the map is generated? You can hover over a particular node or edge to get more details about the component of interest. 8. How can I visualize all of the genes or metabolites that are present in my data but not in the optimized network? Clicking on the pop-up window on the right side of the screen opens up a menu named Option & Information in which selecting All Others will display all the genes and metabolites present in the data on the pathway map. The genes and metabolites that satisfy the parameters selected in the previous menu have a color palette of red-green and the ones that do not are displayed as shades of purple. 9. How can I see the key pathways present in my data? Clicking on the pop-up window on the left side of the screen opens up a menu named Analyze your data which contains a list of questions that you can ask regarding their data. Clicking on What are the key pathways will open the Pathway Enrichment module that assigns a score to perturbed pathways based on enrichment and topology. 10. How can I effectively use the query \"How do publicly available results correlate with my results?\" to get insights into my data? To use this feature effectively, you need to select one keyword as mentioned on the parameter page and one gene or metabolite of interest from the target list of the generated map. This combination (keyword and gene/metabolite) will be searched for in public databases to get the most relevant publications.","title":"Frequently Asked Questions (FAQs)"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html","text":"Introduction Overview Proteomics is a methodical approach used to identify and understand protein expression patterns at a given time in response to a specific stimulus coupled with functional protein networks that exist at the level of the cell, tissue, or whole organism. This has grown into a popular and promising field for the identification and characterization of cellular gene products ( i.e. proteins) that are present, absent, or altered under certain environmental, physiological and pathophysiological conditions. With the onset of robust and reliable mass spectrometers which help provide methodical analysis and quantification of complex protein mixtures, it is also important to standardize methods to process this data and perform in-depth analysis resulting in a meaningful outcome. Proteomics Workflow provides a platform to analyze any proteomics data states ranging from pre-processing to in-depth pathway analysis. Scope of the app Systematic downstream analysis of Proteomics data with ease of switching interfaces. Visualize abundance plots for gene(s) against predefined or custom pathway databases. Perform PCA for quality check. Perform differential expression using different statistical methods and identify most differentially expressed proteins. Perform global pathway analysis using X2K (Expression to Kinase) with adjustable parameters. Figure 1. Proteomics Workflow Getting Started User input Proteomics Workflow requires two files: Abundance File This file should contain normalized abundance values, protein names, and their corresponding accessions along with the gene symbols. The input abundance file should have Accession , Gene Symbol and Abundances column. Figure 2. Abundance file Metadata File The metadata file should contain sample cohort mapping for the samples present in the abundance file. It consists of two columns, SampleName which contains the samples present in the abundance file and Cohort which contains the cohort information for each sample. This file should be in .csv format. Figure 3. Cohort file Steps involved in data processing Upload abundance and the metadata files. Perform normalization of data. Perform pathway analysis using in-house KEGG, HMDB and Reactome databases or upload a custom database. Perform X2K analysis and visualize enrichment plots Caveats The input file format has to be exactly same as the demo data. Tutorial Upload files Select Proteomics Workflow from the dashboard under the Proteomics Data tab. You can either Add New Workspace or Select a Workspace which is an already existing workspace as shown in Figure 4. After entering workspace details, you will be redirected to the app. Figure 4. Polly Dashboard and Proteomics Workflow Upload the abundance and cohort file in the upload space and click on Go . Figure 5. Upload interface Pre-processing The pre-processing section extracts and displays only the protein abundances column for all samples. To perform control normalization, select the cohort using the drop down and click on Normalize as shown in Figure 6. Control normalization normalizes every cohort with respect to the cohort selected in the Control Cohort section. Figure 6. Pre-processing PCA Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features. High-dimensional data are very common in biology and arise when multiple features, such as expression of many genes, are measured for each sample. PCA is an unsupervised learning method similar to clustering wherein it finds patterns without reference to prior knowledge about whether the samples come from different treatment groups or have phenotypic differences. PCA reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs. The first PC is chosen to minimize the total distance between the data and their projection onto the PC. The second (and subsequent) PCs are selected similarly, with the additional requirement that they be uncorrelated with all previous PCs. The PCA Plot interface allows visualizing PC1 to PC11 using the drop-down menu's labeled PC on x axis and PC on y axis . The input data for the PCA Plot is the Log 2 Control Normalized Abundances. Figure 7. PCA Pathway Search The Pathway Search interface helps in visualizing the abundance of proteins across different cohorts belonging to a particular pathway. The following customization are possible in the Pathway Search interface: Choose Pathway Database: KEGG, HMDB, Reactome pathways are available by default and can be selected using the drop down under Choose Pathway Database. Alternatively, custom pathways can be added using the upload option on the right side. The custom pathway file should be in .csv format and should contain the pathway names as well as the proteins involved in the pathway. A sample pathway file can be downloaded by clicking on Download a sample pathway file . Pathway: The pathway to be visualized can be selected from the list of pathways present in the selected database. Multiple pathways can be selected simultaneously. Protein Name: The protein to be visualized can be selected from the drop down menu. Only the proteins involved in the pathway selected will show up in this menu. Selecting one protein will add all its phosphosites present in the data to the plot as well. Multiple proteins can be selected simultaneously. Figure 8. Pathway Search Differential Expression Analysis The differential analysis supports three methods to perform differential expression; t-test, limma, and One-Way ANOVA. You can select this from the Statistical test drop down menu. There are two methods to perform p -value correction; Benjamini-Hochberg and Bonferroni correction. By default Benjamini-Hochberg correction procedure is used however, it is possible to perform either Bonferroni correction procedure or both the methods simultaneously or remove them altogether. The cohorts to be used can be selected from the drop down menu's labeled Cohort A and Cohort B . The input data for the differential expression analysis is the Log 2 Control Normalized Abundances. The p -value and log 2 fold change cutoff parameters can be changed either before or after the plot has been prepared. Clicking on Go! will display a volcano plot prepared between the two selected cohorts using the cutoff parameters defined. You can specify the cohorts for comparison and adjust the parameters of p- value and log 2 fold change using the drop downs and seek bar as shown in Figure 9. Figure 9. Differential Expression Analysis X2K analysis An X2K analysis involves measuring transcription factors regulating differentially expressed genes which further associates it to PPIs or Protein-Protein interactions thereby creating a subnetwork. A Kinase Enrichment analysis is done on the nodes of this subnetwork. The X2K analysis is done after the differential expression is carried out. The differentially expressed data is used as an input for X2K analysis. Here, differential expression is performed where significant genes ( p -value < 0.05) are selected. These significant genes are ordered on the basis of their log 2 FC value. You can select top 'n' of the ordered values based on up and downregulation of genes. Finally, on the selected number of genes, X2K is performed. Figure 10. X2K analysis Figure 11. TFEA plots KSEA KSEA (Kinase\u2013Substrate Enrichment Analysis) is one of the several methods used to study biological signaling processes by understanding kinase regulation. This is of increasing interest due to the potential of developing kinase-altering therapies as biological signaling processes have been observed to form the molecular pathogenesis of many diseases. KSEA works by scoring each kinase based on the relative hyper-phosphorylation or dephosphorylation of the majority of its substrates, as identified from phosphosite-specific Kinase\u2013Substrate (K\u2013S) databases. The negative or positive value of the score, in turn, implies a decrease or increase in the kinase\u2019s overall activity relative to the control. The KSEA interface allows identification and visualization of kinase-level annotations from their quantitative phosphoproteomics data sets. The bars in the KSEA bar plot are red for kinases which are significantly enriched. KSEA is performed after a method is chosen for differential expression in the drop-down menu labeled Statistical Test . It is possible to choose either t-test or limma. The results of the differential expression analysis is then used as the input for KSEA. The input is formed in the following manner: One-way ANOVA or other statistical test as selected is performed and significant phosphosites are chosen Differential expression analysis is performed and fold changes and p -values are calculated Protein and phosphosites are separated into multiple rows Figure 12. KEA plot Figure 13. X2K plot References Clarke DJB, Kuleshov MV, Schilder BM, Torre D, Duffy ME, Keenan AB, Lachmann A, Feldmann AS, Gundersen GW, Silverstein MC, Wang Z, Ma'ayan A. eXpression2Kinases (X2K) Web: linking expression signatures to upstream cell signaling networks. Nucleic Acids Res. 2018 Jul 2;46(W1):W171-W179 Chen EY, Xu H, Gordonov S, Lim MP, Perkins MH, Ma'ayan A. Expression2Kinases: mRNA profiling linked to multiple upstream regulatory layers. Bioinformatics. 28:105 (2012)","title":"Proteomics Workflow"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#overview","text":"Proteomics is a methodical approach used to identify and understand protein expression patterns at a given time in response to a specific stimulus coupled with functional protein networks that exist at the level of the cell, tissue, or whole organism. This has grown into a popular and promising field for the identification and characterization of cellular gene products ( i.e. proteins) that are present, absent, or altered under certain environmental, physiological and pathophysiological conditions. With the onset of robust and reliable mass spectrometers which help provide methodical analysis and quantification of complex protein mixtures, it is also important to standardize methods to process this data and perform in-depth analysis resulting in a meaningful outcome. Proteomics Workflow provides a platform to analyze any proteomics data states ranging from pre-processing to in-depth pathway analysis.","title":"Overview"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#scope-of-the-app","text":"Systematic downstream analysis of Proteomics data with ease of switching interfaces. Visualize abundance plots for gene(s) against predefined or custom pathway databases. Perform PCA for quality check. Perform differential expression using different statistical methods and identify most differentially expressed proteins. Perform global pathway analysis using X2K (Expression to Kinase) with adjustable parameters. Figure 1. Proteomics Workflow","title":"Scope of the app"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#user-input","text":"Proteomics Workflow requires two files: Abundance File This file should contain normalized abundance values, protein names, and their corresponding accessions along with the gene symbols. The input abundance file should have Accession , Gene Symbol and Abundances column. Figure 2. Abundance file Metadata File The metadata file should contain sample cohort mapping for the samples present in the abundance file. It consists of two columns, SampleName which contains the samples present in the abundance file and Cohort which contains the cohort information for each sample. This file should be in .csv format. Figure 3. Cohort file","title":"User input"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#steps-involved-in-data-processing","text":"Upload abundance and the metadata files. Perform normalization of data. Perform pathway analysis using in-house KEGG, HMDB and Reactome databases or upload a custom database. Perform X2K analysis and visualize enrichment plots","title":"Steps involved in data processing"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#caveats","text":"The input file format has to be exactly same as the demo data.","title":"Caveats"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#upload-files","text":"Select Proteomics Workflow from the dashboard under the Proteomics Data tab. You can either Add New Workspace or Select a Workspace which is an already existing workspace as shown in Figure 4. After entering workspace details, you will be redirected to the app. Figure 4. Polly Dashboard and Proteomics Workflow Upload the abundance and cohort file in the upload space and click on Go . Figure 5. Upload interface","title":"Upload files"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#pre-processing","text":"The pre-processing section extracts and displays only the protein abundances column for all samples. To perform control normalization, select the cohort using the drop down and click on Normalize as shown in Figure 6. Control normalization normalizes every cohort with respect to the cohort selected in the Control Cohort section. Figure 6. Pre-processing","title":"Pre-processing"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#pca","text":"Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features. High-dimensional data are very common in biology and arise when multiple features, such as expression of many genes, are measured for each sample. PCA is an unsupervised learning method similar to clustering wherein it finds patterns without reference to prior knowledge about whether the samples come from different treatment groups or have phenotypic differences. PCA reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs. The first PC is chosen to minimize the total distance between the data and their projection onto the PC. The second (and subsequent) PCs are selected similarly, with the additional requirement that they be uncorrelated with all previous PCs. The PCA Plot interface allows visualizing PC1 to PC11 using the drop-down menu's labeled PC on x axis and PC on y axis . The input data for the PCA Plot is the Log 2 Control Normalized Abundances. Figure 7. PCA","title":"PCA"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#pathway-search","text":"The Pathway Search interface helps in visualizing the abundance of proteins across different cohorts belonging to a particular pathway. The following customization are possible in the Pathway Search interface: Choose Pathway Database: KEGG, HMDB, Reactome pathways are available by default and can be selected using the drop down under Choose Pathway Database. Alternatively, custom pathways can be added using the upload option on the right side. The custom pathway file should be in .csv format and should contain the pathway names as well as the proteins involved in the pathway. A sample pathway file can be downloaded by clicking on Download a sample pathway file . Pathway: The pathway to be visualized can be selected from the list of pathways present in the selected database. Multiple pathways can be selected simultaneously. Protein Name: The protein to be visualized can be selected from the drop down menu. Only the proteins involved in the pathway selected will show up in this menu. Selecting one protein will add all its phosphosites present in the data to the plot as well. Multiple proteins can be selected simultaneously. Figure 8. Pathway Search","title":"Pathway Search"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#differential-expression-analysis","text":"The differential analysis supports three methods to perform differential expression; t-test, limma, and One-Way ANOVA. You can select this from the Statistical test drop down menu. There are two methods to perform p -value correction; Benjamini-Hochberg and Bonferroni correction. By default Benjamini-Hochberg correction procedure is used however, it is possible to perform either Bonferroni correction procedure or both the methods simultaneously or remove them altogether. The cohorts to be used can be selected from the drop down menu's labeled Cohort A and Cohort B . The input data for the differential expression analysis is the Log 2 Control Normalized Abundances. The p -value and log 2 fold change cutoff parameters can be changed either before or after the plot has been prepared. Clicking on Go! will display a volcano plot prepared between the two selected cohorts using the cutoff parameters defined. You can specify the cohorts for comparison and adjust the parameters of p- value and log 2 fold change using the drop downs and seek bar as shown in Figure 9. Figure 9. Differential Expression Analysis","title":"Differential Expression Analysis"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#x2k-analysis","text":"An X2K analysis involves measuring transcription factors regulating differentially expressed genes which further associates it to PPIs or Protein-Protein interactions thereby creating a subnetwork. A Kinase Enrichment analysis is done on the nodes of this subnetwork. The X2K analysis is done after the differential expression is carried out. The differentially expressed data is used as an input for X2K analysis. Here, differential expression is performed where significant genes ( p -value < 0.05) are selected. These significant genes are ordered on the basis of their log 2 FC value. You can select top 'n' of the ordered values based on up and downregulation of genes. Finally, on the selected number of genes, X2K is performed. Figure 10. X2K analysis Figure 11. TFEA plots KSEA KSEA (Kinase\u2013Substrate Enrichment Analysis) is one of the several methods used to study biological signaling processes by understanding kinase regulation. This is of increasing interest due to the potential of developing kinase-altering therapies as biological signaling processes have been observed to form the molecular pathogenesis of many diseases. KSEA works by scoring each kinase based on the relative hyper-phosphorylation or dephosphorylation of the majority of its substrates, as identified from phosphosite-specific Kinase\u2013Substrate (K\u2013S) databases. The negative or positive value of the score, in turn, implies a decrease or increase in the kinase\u2019s overall activity relative to the control. The KSEA interface allows identification and visualization of kinase-level annotations from their quantitative phosphoproteomics data sets. The bars in the KSEA bar plot are red for kinases which are significantly enriched. KSEA is performed after a method is chosen for differential expression in the drop-down menu labeled Statistical Test . It is possible to choose either t-test or limma. The results of the differential expression analysis is then used as the input for KSEA. The input is formed in the following manner: One-way ANOVA or other statistical test as selected is performed and significant phosphosites are chosen Differential expression analysis is performed and fold changes and p -values are calculated Protein and phosphosites are separated into multiple rows Figure 12. KEA plot Figure 13. X2K plot","title":"X2K analysis"},{"location":"Apps/Proteomic Data/Proteomics Workflow.html#references","text":"Clarke DJB, Kuleshov MV, Schilder BM, Torre D, Duffy ME, Keenan AB, Lachmann A, Feldmann AS, Gundersen GW, Silverstein MC, Wang Z, Ma'ayan A. eXpression2Kinases (X2K) Web: linking expression signatures to upstream cell signaling networks. Nucleic Acids Res. 2018 Jul 2;46(W1):W171-W179 Chen EY, Xu H, Gordonov S, Lim MP, Perkins MH, Ma'ayan A. Expression2Kinases: mRNA profiling linked to multiple upstream regulatory layers. Bioinformatics. 28:105 (2012)","title":"References"},{"location":"Apps/Screening Data/CRISPR Screening.html","text":"Introduction Overview CRISPR-Cas9 (Clustered Regularly Interspaced Short Palindromic Repeats) is one of the most popular gene-editing techniques to modify gene loci at the desired position in a model organism. The gene-editing CRISPR/cas9 method cuts at specific locations of a DNA with the help of a small stretch of guide RNAs that takes the Cas9 endonuclease to a specific site. Increasingly the scientific groups are applying this technology to create genetic screens in order to identify mutations that drive treatment resistance to cancer or rapidly access the drug targets. The aim of the genome-wide CRISPR screening experiment is to screen mutant cells to identify genes involved in a particular phenotype. The beauty of the pooled screen is to integrate many genetic perturbations in one experiment. The CRISPR/cas9 system begins by generating a library of perturbed cells using the library of gRNAs. Lentivirus or Retrovirus is used to deliver gRNAs i.e. integrated to the genome of the cell and serve as a molecular tag. After that cells are separated according to the phenotype of interest and the genes causing the phenotype change can be read out by first isolating the genomic DNA from the population of the cell using PCR followed by sequencing(NGS) across gRNA encoding regions. Then computationally sequencing read count is mapped to a precompiled list of the designed gRNA library. How does CRISPR screening help scientists find gene targets? CRISPR-Cas9 (Clustered Regularly Interspaced Short Palindromic Repeats) is one of the most popular gene-editing techniques to modify gene loci at the desired position in a model organism. CRISPR technique is extremely helpful in the identification of genes involved in a particular biological phenomenon. The two main components of CRISPR screening are Cas9 (CRISPR-associated protein 9) and specific guide RNAs that either disrupt host gene or insert gene fragments of interest. Bacteria use this system as a part of their adaptive immune response. Cas9 is an enzyme involved in this mechanism and sgRNA is the complementary sequence of foreign DNA. The popularity of CRISPR-Cas9 is also influenced by its simplicity and cost-effectiveness. CRISPR Screening provides an interactive way to explore and visualize the quality and analysis results of CRISPR screens. Presently, the tool analyzes the data using\u200b MAGeCK-VISPR algorithm\u200b and display the results in an interactive and intuitive manner. Scope of the app Swiftly upload large data sets and analyze multiple comparisons in one go. Track the processing status of each comparison. Visualize and interpret the quality check and analysis results effectively. Compare the results of different analyses on the same dashboard. Save and share analyses with other collaborators. Revisit previously performed analyses at any time. Figure 1. CRISPR Screening Getting Started User Input CRISPR Screening requires the following two files as input: Fastq files Fastq files stores short read data from high throughput sequencing experiments. The sequence and quality scores are usually put into a single line each. These are files in fast.gz format. A FASTQ record has the following format: A line starting with @, containing the sequence ID. One or more lines that contain the sequence. A new line starting with the character +, and being either empty or repeating the sequence ID. One or more lines that contain the quality scores. Figure 2. Fastq File The ideal CRISPR sequence length is only ~20 bp nucleotides. If you find your data contains more than ~ 20 nucleotides, there is a high chance there are adapters in the sequence. These experimental adapters need to be trimmed. Figure 3. Fastq File showing adapters The highlighted sequence is of adapters. You can specify the length of nucleotides to be trimmed in the Trim-5 given under the pipeline and parameter tab. Figure 4. Trim-5 Library file The library file of CRISPR method stores which sgRNA targets which gene. The file format should be CSV. Each gene associated with multiple sgRNA. The file contains three columns: Id : contains Unique sgRNA id gRNA sequence : Consists of gRNA sequence. The sequences are searched in the fastq files by the CRISPR screening algorithm to calculate the read counts. Gene : Gene column consists of gene symbol associated with each gRNA sequence Note : The sequence of these columns should be as given in the figure above. It should be maintained as it is before uploading on the Polly CRISPR application. Figure 5. Library File Steps involved in data processing Plan a run: To obtain insights from your data generated from a CRISPR screening experiment, open the CRISPR Screening app on Polly. Create/Choose a workspace: Before running your data in CRISPR Screening, you need to either create or choose an existing workspace on Polly. Uploading data: Polly CRISPR Screening requires two types of input files; fastq.gz files generated from the experiment and the sgRNA library. Select pipeline and parameters: You need to select parameters such as Pipeline , Species , Assembly , Trim , and sgRNA length . Rename samples: You can rename the samples if required. Add comparisons: After filling all parameters you need to create comparisons and define baseline samples as well as various conditions. Start processing: Once the comparisons are set you can start processing. Log files: You can also look into the progress of run by analyzing log files. Go to the dashboard: Once the run has completed you can click on go to the dashboard and analyze data. Set base comparison: Select the base comparison to see the table of genes with the p -value and beta score. Quality control: You can check the quality of analysis at the sequence level as well as the sample level. In sequence level quality check, you can find graphs of sequencing reads, GC content, Base quality, Mapped reads, Gini index, and zero counts. Similarly at the sample level there are PCA clustering, Read count CDF and normalization graphs. MAGeCK VISPR: MAGeCK VISPR output consists of sgRNA Read count plot and any gene of interest can be selected in various samples. Similarly, Hierarchical clustering over normalized beta scores (using Ward variance minimization and Euclidean distance) on 500 genes is also provided. You can also analyze the various comparisons and distribution plots. Cross-analysis comparison: In Cross-analysis comparison feature you can select any relevant comparison from previous analysis and compare with the results present analysis. Tutorial CRISPR Screening provides an interactive way to explore and visualize the quality and analysis results of CRISPR screens. Presently, the tool analyzes the data using\u200b MAGeCK-VISPR algorithm\u200b and display the results in an interactive and intuitive manner. NOTE: The public dataset used for the tutorial is extracted from the paper \"A CRISPR Dropout Screen Identifies Genetic Vulnerabilities and Therapeutic Targets in Acute Myeloid Leukemia(Tzelepis et al)\". Genome-wide drop-out screens were performed in 5 Acute Myeloid Leukemia (AML) cell lines and 2 Non-AML cell lines (HT-29 and HT-1080). For our study, we have considered three AML cell lines i.e. OCIAML3, MV411, and MOLM. Select CRISPR Screening from the dashboard under the Screening and Sequencing Data Tab as shown in Figure 6. Create a New Workspace or select from the existing one from the drop-down and provide a Name of the Session to be redirected to CRISPR Screening's upload page. Figure 6. Dashboard Figure 7. CRISPR Screening Upload Files Upload the files in their respective tabs\u200b as indicated by the screenshots below. Click on Upload Files on the Upload Fastq Files box to upload the fastq.gz files. Click on Upload Files button on the Upload sgRNA Files box to upload the library file in .csv format. Figure 8. Upload interface Track the status of the upload real-time. Figure 9. Real-time tracking Set parameters and process comparisons Upon successful upload of the files, the next step is to set parameters. All the parameters indicated in the screenshot below are required for processing the data. Note: Parameters for Demo Dataset are: Species: Homo sapiens Assembly: hg38 Trim-5': 23 sgRNA length: 19 Figure 10. Select Pipeline and Parameters Interface Give identifier names to the uploaded .fastq samples Figure 11. Sample Name Editor Renaming of comparisons and selection of conditions and baseline Rename the default comparison and select baseline and condition samples for this comparison as indicated in the screenshot. After selecting the baseline & condition samples, select Start Processing option. Figure 12. Rename comparisons Add another comparison by clicking on the button Add New button at the right corner of the screen. Note: Once you click on start processing, editing of all the existing comparison is disabled. You can add multiple comparisons in one go without waiting for the analysis completion of the previous comparisons using the Add New option. Restore analysis After the coffee break, you can go to the working project by following small steps: Login to Polly Go to Manage Tab. Click on the workspace you were working on. Figure 13. Select workspace Track the progress of each comparison with the help of the icons depicted below. Figure 14. Track Progress of each comparison Dashboard After clicking on Go To Dashboard , the following screenshot is displayed. Figure 15. CRISPR Dashboard Analysis Table & sgRNA Plot The table displays the beta score, p -value and FDR values for each gene. sgRNA Plot: The sgRNA plot navigates you to visualise the normalised counts of each gene. By clicking the gene name in the table the sgRNA plots changes accordingly. You can download the plots, normalised counts and result table of the analysis by clicking on Download button on the top right corner of the page. By default, all sgRNA and Samples are selected and displayed in the sgRNA Read Count Plot. sgRNA Plot Setting: Under setting, there is an option of select and deselect of sgRNAs or samples of interest. Figure 16. sgRNA Plot Distribution Plot: CDF Plot: The cumulative distribution function of p -values. Smallest FDR greater or equal to 5% and 25% are displayed. Figure 17. CDF Plot Count Plot: Histogram of p -values. Small p -values should be enriched. If a one-sided test is performed, p -values toward 1.0 are expected to be enriched as well. Figure 18. Count Plot Note: To download the distribution plots, the CDF Plot is selected or Count plot is opened first and then select Distribution in the drop-down menu of Download . The respective image will be downloaded. The table representing beta score , FDR and p -values for the genes is sorted based on the beta score by default. You can change the setting in either ascending or descending direction by clicking on the headers of each column. There is a functionality to filter the genes by adjusting the range in each column. Also, you can search for gene of interest just by typing on the search white box. Additionally, you can click on any of the genes in the table to view a detailed information about that gene in a new tab. Beta score: Beta score is a measurement of gene selection. It is equivalent to log fold change in differential expression analysis. It reflects the extent of selection in each condition. Beta score of a gene > 0, it reflects gene is positively selected. Beta score of a gene < 0, it reflects gene is negatively selected. p -value: p -value helps to determine the significance of the result. A small p -value (typically \u2264 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis. A large p -value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis. p -values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always report the p -value so your readers can draw their own conclusions. FDR: FDR determines the adjusted p -value for each test. It controls the number of false discoveries in those results that result in discovery (i.e. significant p -value). A p -value of 0.05 implies that 5% of all tests will result in false positives. An FDR adjusted p -value (or q -value) of 0.05 implies that 5% of significant tests will result in false positives. The latter will result in fewer false positives. Switch Selection By default, the table get displayed is of Negative Selection . The selection can be changed by clicking on the toggle button. Switch Comparison By clicking on the tab Switch Comparison , you can change the comparison that has been processed in the current analysis. Access QC plots and analysis visualizations using the module section Click on button present at the left of the screen. Following options will pop up: Figure 19. Analyze your Data module Have a look at the QC module You can have a look at the QC module by clicking on the button as indicated on the screenshot below. Sequence Level QC measurements aim to detect problems with the sequencing, similar as in another next-generation sequencing (NGS) experiments. Glance through the sequence level QC summary from here. QC TERM DESCRIPTION EXPECTED SCREENSHOT GC Content Distribution Measures GC content across the whole length of each sequencing reads Similar distribution for all samples from the same library Base Quality Measures the base quality distribution of sequencing reads. Sequencing Reads should have reasonable median value > 25. Sequencing Reads Gives the total number of sequencing reads. May vary depending on the sequencing platform. Mapped Reads Total Number of mapped reads to the sgRNA library. Good Statistical power of downstream analysis relies on sufficient reads preferred to be over 300. Gini Index Measures the evenness of the sgRNA read counts. It is the log scaled read count distribution. Gini Index: 1. Plasmid or Control Samples ~ 0.1 2. Negative Selection Samples ~ 0.2 High Gini index in plasmid library, in early time points, or in negative selection experiments may indicate CRISPR oligo-nucleotide synthesis unevenness, low viral transfection efficiency, and over the selection, respectively. Zero Counts Number of sgRNAs with zero counts. ~ 1% of total sgRNAs. Visualize sample level QC By selecting the indicated categories. Checks the consistency between samples. QC TERM DESCRIPTION SCREENSHOT PCA Clustering Calculates pairwise Pearson correlations of sample log read counts and draw the samples on the first three components of a Principle Component Analysis (PCA). Samples with similar conditions should cluster together Read count CDF Normalization Boxplot shows the normalized read count distribution. Explore MAGeCK-VISPR results By clicking on the indicated module and selecting the visualization of interest. CATEGORIES DESCRIPTION SCREENSHOT Clustering k- means clustering view over normalized beta scores of all conditions (using Ward variance minimization and Euclidean distance) on top 500 genes. Comparisons Generates the Venn diagram displaying the overlap of significant genes in different comparisons. When you hover on the intersected area, you get the list of common genes in the analysis. Multi Comparison In Multi Comparison, you can compare the current analysis with the previously processed analyses irrespective of whether they are in the current workspace or a different workspace. To view multiple comparisons follow these steps : Click on Multi-Comparison. A drop down with a list of all the CRISPR Screening analyses pops up. Select the comparisons you want to compare with your base comparison. Base Comparison is the comparison selected in the current analysis. Figure 20. Multi comparison Once the comparisons are selected, click on the Save button on the top right corner of the page. On doing so, the following table is displayed with all the selected comparisons. By default, the table displays values for the negative selection. It can be switched to positive selection by clicking the positive selection option (as shown earlier for a single comparison). The multi-comparison table can show union or intersection of the genes present in all the comparisons. Here, union will show all the genes present in all the comparisons and intersection will only show genes which are common to all the comparisons. by default, union is selected. This can be changed by selecting the intersection option on the top of the screen. Figure 21. Multi-comparison sgRNA read count plot for multiple comparisons can be seen together one below the other (two at a time). Go to the MAGeCK VISPR option. Click on sgRNA Read Count Plot. Options and Info box opens from the right side of the screen. Select the gene for which the plot is required and click on Apply Changes . Once done, plots for two comparisons appear one below the other as shown in the image. Figure 22. Compare sgRNA Read count plot of a gene of interest in the base and comparisons performed in the previous analysis The comparisons for which the plots are shown can be changed by clicking on the translucent arrows next to the images. sgRNA Removal There are multiple sgRNAs associated with a gene in experiments. If the count of any sgRNA is abruptly high in a sample, it can be removed as an outlier using the sgRNA Removal feature. This feature removes the sgRNAs which have counts greater than to the product of the median multiplier & median count of all the sgRNAs in a sample treating them as outliers. The outlier sgRNAs can be removed by providing any natural number as the median multiplier in the sgRNA Removal box. It will give the estimated number of gRNAs left in the matrix. You can iterate based on the total number of gRNAs left in the count matrix to perform the MaGeCK analysis. Follow these steps to remove these abruptly high sgRNAs Select the srRNA Removal option from the pop up box on the left hand side It will take some time for the sgRNA file to be fetched. Then, a box will open to enter the Median Multiplier . Enter the desired median multiplier in the box and click Run . The number of sgRNAs rejected and remaining will be shown. You can keep changing the multiplier till a satisfied number of sgRNAs are remaining. Once you are satisfied with the number of sgRNAs, click Run MLE. This will start running a comparison with just the remaining sgRNAs. Once the new comparison starts, a notification appears on the top right of the screen. Once the comparison is processed, it will be available in the list of comparisons in this analysis. Videos References Li W, K\u00f6ster J, Xu H, Chen CH, Xiao T, Liu JS, Brown M, Liu XS. Quality control, modeling, and visualization of CRISPR screens with MAGeCK-VISPR. Genome biology. 2015 Dec;16(1):281. Frequently Asked Questions (FAQs) S.No. Questions Answers 1. What is Polly? Polly is a one-stop data analysis platform designed to accelerate the drug discovery process. Polly hosts multiple applications that can be combined and/or customized by you to create their own specific analytical workflows. 2. What is the CRISPR Screening app? CRISPR Screening is an app built to analyze of CRISPR screening experiments. 3. What are the input files required to run an experiment in CRISPR Screening app? Two types of files are required for CRISPR screening analysis; fastq.gz files and sgRNA file. 4. Can I change the name of my samples? Yes, the name of any sample can be changed. 5. How many comparisons can I select in my analysis? There is no limit to add comparisons in any analysis. 6. Can I continue my analysis without selecting any parameters? No, the parameters have to be selected before adding a comparison. 7. Is the selection of baseline and condition mandatory for any comparison? Yes, the baseline and condition have to be defined. 8. What is the base comparison? A base comparison is the main comparison performed during analysis. This comparison can be further compared with other subsequent comparisons. 9. What is the cross-analysis comparison? Cross-analysis comparison is a feature that allows for the comparison between the current and any previous comparisons. It is available in manage comparisons. 10. How I can access the quality control of my analysis? After the base comparison or cross-analysis comparison has been selected, the sequence level (sequencing reads, GC content, base quality, mapped reads, Gini index, and zero counts) and sample level (PCA, read count CDF and normalization) quality control can be accessed.","title":"CRISPR Screening"},{"location":"Apps/Screening Data/CRISPR Screening.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Screening Data/CRISPR Screening.html#overview","text":"CRISPR-Cas9 (Clustered Regularly Interspaced Short Palindromic Repeats) is one of the most popular gene-editing techniques to modify gene loci at the desired position in a model organism. The gene-editing CRISPR/cas9 method cuts at specific locations of a DNA with the help of a small stretch of guide RNAs that takes the Cas9 endonuclease to a specific site. Increasingly the scientific groups are applying this technology to create genetic screens in order to identify mutations that drive treatment resistance to cancer or rapidly access the drug targets. The aim of the genome-wide CRISPR screening experiment is to screen mutant cells to identify genes involved in a particular phenotype. The beauty of the pooled screen is to integrate many genetic perturbations in one experiment. The CRISPR/cas9 system begins by generating a library of perturbed cells using the library of gRNAs. Lentivirus or Retrovirus is used to deliver gRNAs i.e. integrated to the genome of the cell and serve as a molecular tag. After that cells are separated according to the phenotype of interest and the genes causing the phenotype change can be read out by first isolating the genomic DNA from the population of the cell using PCR followed by sequencing(NGS) across gRNA encoding regions. Then computationally sequencing read count is mapped to a precompiled list of the designed gRNA library. How does CRISPR screening help scientists find gene targets? CRISPR-Cas9 (Clustered Regularly Interspaced Short Palindromic Repeats) is one of the most popular gene-editing techniques to modify gene loci at the desired position in a model organism. CRISPR technique is extremely helpful in the identification of genes involved in a particular biological phenomenon. The two main components of CRISPR screening are Cas9 (CRISPR-associated protein 9) and specific guide RNAs that either disrupt host gene or insert gene fragments of interest. Bacteria use this system as a part of their adaptive immune response. Cas9 is an enzyme involved in this mechanism and sgRNA is the complementary sequence of foreign DNA. The popularity of CRISPR-Cas9 is also influenced by its simplicity and cost-effectiveness. CRISPR Screening provides an interactive way to explore and visualize the quality and analysis results of CRISPR screens. Presently, the tool analyzes the data using\u200b MAGeCK-VISPR algorithm\u200b and display the results in an interactive and intuitive manner.","title":"Overview"},{"location":"Apps/Screening Data/CRISPR Screening.html#scope-of-the-app","text":"Swiftly upload large data sets and analyze multiple comparisons in one go. Track the processing status of each comparison. Visualize and interpret the quality check and analysis results effectively. Compare the results of different analyses on the same dashboard. Save and share analyses with other collaborators. Revisit previously performed analyses at any time. Figure 1. CRISPR Screening","title":"Scope of the app"},{"location":"Apps/Screening Data/CRISPR Screening.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Screening Data/CRISPR Screening.html#user-input","text":"CRISPR Screening requires the following two files as input: Fastq files Fastq files stores short read data from high throughput sequencing experiments. The sequence and quality scores are usually put into a single line each. These are files in fast.gz format. A FASTQ record has the following format: A line starting with @, containing the sequence ID. One or more lines that contain the sequence. A new line starting with the character +, and being either empty or repeating the sequence ID. One or more lines that contain the quality scores. Figure 2. Fastq File The ideal CRISPR sequence length is only ~20 bp nucleotides. If you find your data contains more than ~ 20 nucleotides, there is a high chance there are adapters in the sequence. These experimental adapters need to be trimmed. Figure 3. Fastq File showing adapters The highlighted sequence is of adapters. You can specify the length of nucleotides to be trimmed in the Trim-5 given under the pipeline and parameter tab. Figure 4. Trim-5 Library file The library file of CRISPR method stores which sgRNA targets which gene. The file format should be CSV. Each gene associated with multiple sgRNA. The file contains three columns: Id : contains Unique sgRNA id gRNA sequence : Consists of gRNA sequence. The sequences are searched in the fastq files by the CRISPR screening algorithm to calculate the read counts. Gene : Gene column consists of gene symbol associated with each gRNA sequence Note : The sequence of these columns should be as given in the figure above. It should be maintained as it is before uploading on the Polly CRISPR application. Figure 5. Library File","title":"User Input"},{"location":"Apps/Screening Data/CRISPR Screening.html#steps-involved-in-data-processing","text":"Plan a run: To obtain insights from your data generated from a CRISPR screening experiment, open the CRISPR Screening app on Polly. Create/Choose a workspace: Before running your data in CRISPR Screening, you need to either create or choose an existing workspace on Polly. Uploading data: Polly CRISPR Screening requires two types of input files; fastq.gz files generated from the experiment and the sgRNA library. Select pipeline and parameters: You need to select parameters such as Pipeline , Species , Assembly , Trim , and sgRNA length . Rename samples: You can rename the samples if required. Add comparisons: After filling all parameters you need to create comparisons and define baseline samples as well as various conditions. Start processing: Once the comparisons are set you can start processing. Log files: You can also look into the progress of run by analyzing log files. Go to the dashboard: Once the run has completed you can click on go to the dashboard and analyze data. Set base comparison: Select the base comparison to see the table of genes with the p -value and beta score. Quality control: You can check the quality of analysis at the sequence level as well as the sample level. In sequence level quality check, you can find graphs of sequencing reads, GC content, Base quality, Mapped reads, Gini index, and zero counts. Similarly at the sample level there are PCA clustering, Read count CDF and normalization graphs. MAGeCK VISPR: MAGeCK VISPR output consists of sgRNA Read count plot and any gene of interest can be selected in various samples. Similarly, Hierarchical clustering over normalized beta scores (using Ward variance minimization and Euclidean distance) on 500 genes is also provided. You can also analyze the various comparisons and distribution plots. Cross-analysis comparison: In Cross-analysis comparison feature you can select any relevant comparison from previous analysis and compare with the results present analysis.","title":"Steps involved in data processing"},{"location":"Apps/Screening Data/CRISPR Screening.html#tutorial","text":"CRISPR Screening provides an interactive way to explore and visualize the quality and analysis results of CRISPR screens. Presently, the tool analyzes the data using\u200b MAGeCK-VISPR algorithm\u200b and display the results in an interactive and intuitive manner. NOTE: The public dataset used for the tutorial is extracted from the paper \"A CRISPR Dropout Screen Identifies Genetic Vulnerabilities and Therapeutic Targets in Acute Myeloid Leukemia(Tzelepis et al)\". Genome-wide drop-out screens were performed in 5 Acute Myeloid Leukemia (AML) cell lines and 2 Non-AML cell lines (HT-29 and HT-1080). For our study, we have considered three AML cell lines i.e. OCIAML3, MV411, and MOLM. Select CRISPR Screening from the dashboard under the Screening and Sequencing Data Tab as shown in Figure 6. Create a New Workspace or select from the existing one from the drop-down and provide a Name of the Session to be redirected to CRISPR Screening's upload page. Figure 6. Dashboard Figure 7. CRISPR Screening","title":"Tutorial"},{"location":"Apps/Screening Data/CRISPR Screening.html#upload-files","text":"Upload the files in their respective tabs\u200b as indicated by the screenshots below. Click on Upload Files on the Upload Fastq Files box to upload the fastq.gz files. Click on Upload Files button on the Upload sgRNA Files box to upload the library file in .csv format. Figure 8. Upload interface Track the status of the upload real-time. Figure 9. Real-time tracking","title":"Upload Files"},{"location":"Apps/Screening Data/CRISPR Screening.html#set-parameters-and-process-comparisons","text":"Upon successful upload of the files, the next step is to set parameters. All the parameters indicated in the screenshot below are required for processing the data. Note: Parameters for Demo Dataset are: Species: Homo sapiens Assembly: hg38 Trim-5': 23 sgRNA length: 19 Figure 10. Select Pipeline and Parameters Interface","title":"Set parameters and process comparisons"},{"location":"Apps/Screening Data/CRISPR Screening.html#give-identifier-names-to-the-uploaded-fastq-samples","text":"Figure 11. Sample Name Editor","title":"Give identifier names to the uploaded .fastq samples"},{"location":"Apps/Screening Data/CRISPR Screening.html#renaming-of-comparisons-and-selection-of-conditions-and-baseline","text":"Rename the default comparison and select baseline and condition samples for this comparison as indicated in the screenshot. After selecting the baseline & condition samples, select Start Processing option. Figure 12. Rename comparisons Add another comparison by clicking on the button Add New button at the right corner of the screen. Note: Once you click on start processing, editing of all the existing comparison is disabled. You can add multiple comparisons in one go without waiting for the analysis completion of the previous comparisons using the Add New option.","title":"Renaming of comparisons and selection of conditions and baseline"},{"location":"Apps/Screening Data/CRISPR Screening.html#restore-analysis","text":"After the coffee break, you can go to the working project by following small steps: Login to Polly Go to Manage Tab. Click on the workspace you were working on. Figure 13. Select workspace Track the progress of each comparison with the help of the icons depicted below. Figure 14. Track Progress of each comparison","title":"Restore analysis"},{"location":"Apps/Screening Data/CRISPR Screening.html#dashboard","text":"After clicking on Go To Dashboard , the following screenshot is displayed. Figure 15. CRISPR Dashboard","title":"Dashboard"},{"location":"Apps/Screening Data/CRISPR Screening.html#analysis-table-sgrna-plot","text":"The table displays the beta score, p -value and FDR values for each gene. sgRNA Plot: The sgRNA plot navigates you to visualise the normalised counts of each gene. By clicking the gene name in the table the sgRNA plots changes accordingly. You can download the plots, normalised counts and result table of the analysis by clicking on Download button on the top right corner of the page. By default, all sgRNA and Samples are selected and displayed in the sgRNA Read Count Plot. sgRNA Plot Setting: Under setting, there is an option of select and deselect of sgRNAs or samples of interest. Figure 16. sgRNA Plot Distribution Plot: CDF Plot: The cumulative distribution function of p -values. Smallest FDR greater or equal to 5% and 25% are displayed. Figure 17. CDF Plot Count Plot: Histogram of p -values. Small p -values should be enriched. If a one-sided test is performed, p -values toward 1.0 are expected to be enriched as well. Figure 18. Count Plot Note: To download the distribution plots, the CDF Plot is selected or Count plot is opened first and then select Distribution in the drop-down menu of Download . The respective image will be downloaded. The table representing beta score , FDR and p -values for the genes is sorted based on the beta score by default. You can change the setting in either ascending or descending direction by clicking on the headers of each column. There is a functionality to filter the genes by adjusting the range in each column. Also, you can search for gene of interest just by typing on the search white box. Additionally, you can click on any of the genes in the table to view a detailed information about that gene in a new tab. Beta score: Beta score is a measurement of gene selection. It is equivalent to log fold change in differential expression analysis. It reflects the extent of selection in each condition. Beta score of a gene > 0, it reflects gene is positively selected. Beta score of a gene < 0, it reflects gene is negatively selected. p -value: p -value helps to determine the significance of the result. A small p -value (typically \u2264 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis. A large p -value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis. p -values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always report the p -value so your readers can draw their own conclusions. FDR: FDR determines the adjusted p -value for each test. It controls the number of false discoveries in those results that result in discovery (i.e. significant p -value). A p -value of 0.05 implies that 5% of all tests will result in false positives. An FDR adjusted p -value (or q -value) of 0.05 implies that 5% of significant tests will result in false positives. The latter will result in fewer false positives.","title":"Analysis Table &amp; sgRNA Plot"},{"location":"Apps/Screening Data/CRISPR Screening.html#switch-selection","text":"By default, the table get displayed is of Negative Selection . The selection can be changed by clicking on the toggle button.","title":"Switch Selection"},{"location":"Apps/Screening Data/CRISPR Screening.html#switch-comparison","text":"By clicking on the tab Switch Comparison , you can change the comparison that has been processed in the current analysis.","title":"Switch Comparison"},{"location":"Apps/Screening Data/CRISPR Screening.html#access-qc-plots-and-analysis-visualizations-using-the-module-section","text":"Click on button present at the left of the screen. Following options will pop up: Figure 19. Analyze your Data module Have a look at the QC module You can have a look at the QC module by clicking on the button as indicated on the screenshot below. Sequence Level QC measurements aim to detect problems with the sequencing, similar as in another next-generation sequencing (NGS) experiments. Glance through the sequence level QC summary from here. QC TERM DESCRIPTION EXPECTED SCREENSHOT GC Content Distribution Measures GC content across the whole length of each sequencing reads Similar distribution for all samples from the same library Base Quality Measures the base quality distribution of sequencing reads. Sequencing Reads should have reasonable median value > 25. Sequencing Reads Gives the total number of sequencing reads. May vary depending on the sequencing platform. Mapped Reads Total Number of mapped reads to the sgRNA library. Good Statistical power of downstream analysis relies on sufficient reads preferred to be over 300. Gini Index Measures the evenness of the sgRNA read counts. It is the log scaled read count distribution. Gini Index: 1. Plasmid or Control Samples ~ 0.1 2. Negative Selection Samples ~ 0.2 High Gini index in plasmid library, in early time points, or in negative selection experiments may indicate CRISPR oligo-nucleotide synthesis unevenness, low viral transfection efficiency, and over the selection, respectively. Zero Counts Number of sgRNAs with zero counts. ~ 1% of total sgRNAs. Visualize sample level QC By selecting the indicated categories. Checks the consistency between samples. QC TERM DESCRIPTION SCREENSHOT PCA Clustering Calculates pairwise Pearson correlations of sample log read counts and draw the samples on the first three components of a Principle Component Analysis (PCA). Samples with similar conditions should cluster together Read count CDF Normalization Boxplot shows the normalized read count distribution. Explore MAGeCK-VISPR results By clicking on the indicated module and selecting the visualization of interest. CATEGORIES DESCRIPTION SCREENSHOT Clustering k- means clustering view over normalized beta scores of all conditions (using Ward variance minimization and Euclidean distance) on top 500 genes. Comparisons Generates the Venn diagram displaying the overlap of significant genes in different comparisons. When you hover on the intersected area, you get the list of common genes in the analysis.","title":"Access QC plots and analysis visualizations using the module section"},{"location":"Apps/Screening Data/CRISPR Screening.html#multi-comparison","text":"In Multi Comparison, you can compare the current analysis with the previously processed analyses irrespective of whether they are in the current workspace or a different workspace. To view multiple comparisons follow these steps : Click on Multi-Comparison. A drop down with a list of all the CRISPR Screening analyses pops up. Select the comparisons you want to compare with your base comparison. Base Comparison is the comparison selected in the current analysis. Figure 20. Multi comparison Once the comparisons are selected, click on the Save button on the top right corner of the page. On doing so, the following table is displayed with all the selected comparisons. By default, the table displays values for the negative selection. It can be switched to positive selection by clicking the positive selection option (as shown earlier for a single comparison). The multi-comparison table can show union or intersection of the genes present in all the comparisons. Here, union will show all the genes present in all the comparisons and intersection will only show genes which are common to all the comparisons. by default, union is selected. This can be changed by selecting the intersection option on the top of the screen. Figure 21. Multi-comparison sgRNA read count plot for multiple comparisons can be seen together one below the other (two at a time). Go to the MAGeCK VISPR option. Click on sgRNA Read Count Plot. Options and Info box opens from the right side of the screen. Select the gene for which the plot is required and click on Apply Changes . Once done, plots for two comparisons appear one below the other as shown in the image. Figure 22. Compare sgRNA Read count plot of a gene of interest in the base and comparisons performed in the previous analysis The comparisons for which the plots are shown can be changed by clicking on the translucent arrows next to the images.","title":"Multi Comparison"},{"location":"Apps/Screening Data/CRISPR Screening.html#sgrna-removal","text":"There are multiple sgRNAs associated with a gene in experiments. If the count of any sgRNA is abruptly high in a sample, it can be removed as an outlier using the sgRNA Removal feature. This feature removes the sgRNAs which have counts greater than to the product of the median multiplier & median count of all the sgRNAs in a sample treating them as outliers. The outlier sgRNAs can be removed by providing any natural number as the median multiplier in the sgRNA Removal box. It will give the estimated number of gRNAs left in the matrix. You can iterate based on the total number of gRNAs left in the count matrix to perform the MaGeCK analysis. Follow these steps to remove these abruptly high sgRNAs Select the srRNA Removal option from the pop up box on the left hand side It will take some time for the sgRNA file to be fetched. Then, a box will open to enter the Median Multiplier . Enter the desired median multiplier in the box and click Run . The number of sgRNAs rejected and remaining will be shown. You can keep changing the multiplier till a satisfied number of sgRNAs are remaining. Once you are satisfied with the number of sgRNAs, click Run MLE. This will start running a comparison with just the remaining sgRNAs. Once the new comparison starts, a notification appears on the top right of the screen. Once the comparison is processed, it will be available in the list of comparisons in this analysis.","title":"sgRNA Removal"},{"location":"Apps/Screening Data/CRISPR Screening.html#videos","text":"","title":"Videos"},{"location":"Apps/Screening Data/CRISPR Screening.html#references","text":"Li W, K\u00f6ster J, Xu H, Chen CH, Xiao T, Liu JS, Brown M, Liu XS. Quality control, modeling, and visualization of CRISPR screens with MAGeCK-VISPR. Genome biology. 2015 Dec;16(1):281.","title":"References"},{"location":"Apps/Screening Data/CRISPR Screening.html#frequently-asked-questions-faqs","text":"S.No. Questions Answers 1. What is Polly? Polly is a one-stop data analysis platform designed to accelerate the drug discovery process. Polly hosts multiple applications that can be combined and/or customized by you to create their own specific analytical workflows. 2. What is the CRISPR Screening app? CRISPR Screening is an app built to analyze of CRISPR screening experiments. 3. What are the input files required to run an experiment in CRISPR Screening app? Two types of files are required for CRISPR screening analysis; fastq.gz files and sgRNA file. 4. Can I change the name of my samples? Yes, the name of any sample can be changed. 5. How many comparisons can I select in my analysis? There is no limit to add comparisons in any analysis. 6. Can I continue my analysis without selecting any parameters? No, the parameters have to be selected before adding a comparison. 7. Is the selection of baseline and condition mandatory for any comparison? Yes, the baseline and condition have to be defined. 8. What is the base comparison? A base comparison is the main comparison performed during analysis. This comparison can be further compared with other subsequent comparisons. 9. What is the cross-analysis comparison? Cross-analysis comparison is a feature that allows for the comparison between the current and any previous comparisons. It is available in manage comparisons. 10. How I can access the quality control of my analysis? After the base comparison or cross-analysis comparison has been selected, the sequence level (sequencing reads, GC content, base quality, mapped reads, Gini index, and zero counts) and sample level (PCA, read count CDF and normalization) quality control can be accessed.","title":"Frequently Asked Questions (FAQs)"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html","text":"Introduction Overview With the capability of rapid screening, large quantity of data is generated in a considerably short period of time and we subsequently need rapid extraction of biochemical significance from the heaps of data generated. High Throughput Drug Screening allows you to rapidly screen diverse compounds, by conducting chemical, genetic or pharmacological tests and provide key insights on active compounds, genes or antibodies that regulate the bio-molecular pathway of interest. The results obtained provide a deeper understanding and present a starting point for drug design. Scope of the app Automates post-processing of your data output from high throughput sequencing experiments Rapid visualizations of the results giving you insights about various drug-dosage combinations Percentage proliferation vs concentration dosage for all the drugs in the library at a glance Percentage proliferation difference vs concentration dosage selectivity plot for drugs and cell lines IC 50 values Lethality estimation of all the drugs for the provided cell lines Analyze the results for a drug individually Download tabular representation of the post-processed results Figure 1. High Throughput Drug Screening Getting Started User Input High Throughput Drug Screening requires the following two files as input: RLU Sample Files RLU files are machine generated and should not be tampered with. The second sheet of the file contains all the valuable information that the app needs as shown in Figure 2. Figure 2. Second sheet of RLU file Drug Key-Dosage Files Drug key-dosage file contains the information pertaining to the names of the drugs and their concentration values. It should contain only the drugs from one library i.e. for 40 drugs as shown in Figure 3. Figure 3. Drug key-dosage file Steps in data processing Upload input files Performs visualizations for proliferation, IC 50 , lethality and selectivity values Caveats The input file format has to be exactly same as the demo data. Tutorial Upload files Select High Throughput Drug Screening Application from the dashboard under the Screening and Sequencing Data Tab. Figure 4. Polly Dashboard Create a New Workspace or select from the existing one from the drop-down and provide a Name of the Session to be redirected to High Throughput Drug Screening application's upload page. Figure 5. Workspace Selection Choose no. of cell lines from the drop down as shown in Figure 6. Figure 6. Cell line selection For each cell line: Provide a name to your cell line Click on the Browse option to select the input files files. Browse option for replicate files provides multiple file selection option so that all your replicate files can be selected and added here. Click on Browse to add metadata file in the metadata option. It allows only single file selection and only one metadata file would be added for one cell line. Repeat steps 1-4 for other cell lines if selected. Figure 7. File upload for cell line 1 Once all the input files are selected, click on Go . You will be able to see the information of the uploaded cell lines along with their input files at the top right section of the interface. Figure 8. Upload Interface Visualization Interface Once the input files are uploaded in the side panel, the results are generated and the plots, single drug and table tabs are populated. To see the plots click on Plots present at the top. Figure 9. Plot for percentage proliferation vs concentration dosage for all the drugs Under the Plots tab, you can see the percent proliferation vs concentration dosage for all the drugs in the library at a glance. The x-axis represents the dosage concentration values in log 10 of (nM) and the y-axis represents the percent proliferation. Choose the desired cell line by clicking on the selection box present beside the cell lines a shown in Figure 10. You can choose multiple cell lines as well and their Percent proliferation vs concentration dosage information would be overlaid on the prior plots. Figure 10. Cell line selection on Plots interface Within the plot, you can see the name of the drug at the top while different cell lines would be represented with different colors. To know the information of data points within the plot simply hover over the point you want to see. To analyze the results for a single drug, click on the Single Drug present at the top and then select the cell line you want to analyze by clicking on the selection box beside it. Figure 11. Cell line selection at single drug interface For selecting the drug, move to the bottom of the page to the Choose drug option and select the desired drug from the drop down. The plot would get updated as per your drug selection. Figure 12. Single drug selection interface Post-Processing results The results are available in a tabular format in the Table section. Choose your cell line to view the processed file of that cell line as shown in Figure 13. Figure 13. Cell line selection for tabular results Analysis The Analysis section provides further detailed insights in three sections: IC 50 : The half maximal inhibitory concentration (IC 50 ) is a measure of the effectiveness of a substance in inhibiting a specific biological or biochemical function. This quantitative measure indicates how much of a particular drug is needed to inhibit a given biological process (or component of a process, i.e. an enzyme or cell receptor) by half. According to the FDA, IC 50 represents the concentration of a drug that is required for 50% inhibition in vitro. Selecting a cell line from the drop down as shown in Figure 13 will display three different lists pertaining to the drugs that fall under the following categories: Drugs with IC 50 values in the given dosage range Drugs with IC 50 values lesser than the tested minimum dosage Drugs with IC 50 values greater than the tested maximum dosage Figure 14. IC 50 Plot Lethality: It offers an indication of the lethal toxicity for a given drug. It provides the degree to which a drug is harmful to a user or how capable it is of causing death. The Lethality section provides a chart showing relative lethality of drugs for all the cell lines uploaded. Figure 15. Lethality Plot Selectivity: It refers to a drug\u2019s ability to preferentially produce a particular effect and is related to the structural specificity of drug binding to receptors. Drug selectivity is an important aspect and is used for evaluating the ADRs (Adverse Drug Reaction) of drugs. The Selectivity section provides a percent proliferation difference vs concentration dosage selectivity plot. Figure 16. Selectivity Plot Select you desired drug as well as the cell lines to get the visualization as shown in Figure 16. Figure 17. Cell line selection for Selectivity","title":"High Throughput Drug Screening"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#overview","text":"With the capability of rapid screening, large quantity of data is generated in a considerably short period of time and we subsequently need rapid extraction of biochemical significance from the heaps of data generated. High Throughput Drug Screening allows you to rapidly screen diverse compounds, by conducting chemical, genetic or pharmacological tests and provide key insights on active compounds, genes or antibodies that regulate the bio-molecular pathway of interest. The results obtained provide a deeper understanding and present a starting point for drug design.","title":"Overview"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#scope-of-the-app","text":"Automates post-processing of your data output from high throughput sequencing experiments Rapid visualizations of the results giving you insights about various drug-dosage combinations Percentage proliferation vs concentration dosage for all the drugs in the library at a glance Percentage proliferation difference vs concentration dosage selectivity plot for drugs and cell lines IC 50 values Lethality estimation of all the drugs for the provided cell lines Analyze the results for a drug individually Download tabular representation of the post-processed results Figure 1. High Throughput Drug Screening","title":"Scope of the app"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#getting-started","text":"","title":"Getting Started"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#user-input","text":"High Throughput Drug Screening requires the following two files as input: RLU Sample Files RLU files are machine generated and should not be tampered with. The second sheet of the file contains all the valuable information that the app needs as shown in Figure 2. Figure 2. Second sheet of RLU file Drug Key-Dosage Files Drug key-dosage file contains the information pertaining to the names of the drugs and their concentration values. It should contain only the drugs from one library i.e. for 40 drugs as shown in Figure 3. Figure 3. Drug key-dosage file","title":"User Input"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#steps-in-data-processing","text":"Upload input files Performs visualizations for proliferation, IC 50 , lethality and selectivity values","title":"Steps in data processing"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#caveats","text":"The input file format has to be exactly same as the demo data.","title":"Caveats"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#tutorial","text":"","title":"Tutorial"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#upload-files","text":"Select High Throughput Drug Screening Application from the dashboard under the Screening and Sequencing Data Tab. Figure 4. Polly Dashboard Create a New Workspace or select from the existing one from the drop-down and provide a Name of the Session to be redirected to High Throughput Drug Screening application's upload page. Figure 5. Workspace Selection Choose no. of cell lines from the drop down as shown in Figure 6. Figure 6. Cell line selection For each cell line: Provide a name to your cell line Click on the Browse option to select the input files files. Browse option for replicate files provides multiple file selection option so that all your replicate files can be selected and added here. Click on Browse to add metadata file in the metadata option. It allows only single file selection and only one metadata file would be added for one cell line. Repeat steps 1-4 for other cell lines if selected. Figure 7. File upload for cell line 1 Once all the input files are selected, click on Go . You will be able to see the information of the uploaded cell lines along with their input files at the top right section of the interface. Figure 8. Upload Interface","title":"Upload files"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#visualization-interface","text":"Once the input files are uploaded in the side panel, the results are generated and the plots, single drug and table tabs are populated. To see the plots click on Plots present at the top. Figure 9. Plot for percentage proliferation vs concentration dosage for all the drugs Under the Plots tab, you can see the percent proliferation vs concentration dosage for all the drugs in the library at a glance. The x-axis represents the dosage concentration values in log 10 of (nM) and the y-axis represents the percent proliferation. Choose the desired cell line by clicking on the selection box present beside the cell lines a shown in Figure 10. You can choose multiple cell lines as well and their Percent proliferation vs concentration dosage information would be overlaid on the prior plots. Figure 10. Cell line selection on Plots interface Within the plot, you can see the name of the drug at the top while different cell lines would be represented with different colors. To know the information of data points within the plot simply hover over the point you want to see. To analyze the results for a single drug, click on the Single Drug present at the top and then select the cell line you want to analyze by clicking on the selection box beside it. Figure 11. Cell line selection at single drug interface For selecting the drug, move to the bottom of the page to the Choose drug option and select the desired drug from the drop down. The plot would get updated as per your drug selection. Figure 12. Single drug selection interface","title":"Visualization Interface"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#post-processing-results","text":"The results are available in a tabular format in the Table section. Choose your cell line to view the processed file of that cell line as shown in Figure 13. Figure 13. Cell line selection for tabular results","title":"Post-Processing results"},{"location":"Apps/Screening Data/High Throughput Drug Screening.html#analysis","text":"The Analysis section provides further detailed insights in three sections: IC 50 : The half maximal inhibitory concentration (IC 50 ) is a measure of the effectiveness of a substance in inhibiting a specific biological or biochemical function. This quantitative measure indicates how much of a particular drug is needed to inhibit a given biological process (or component of a process, i.e. an enzyme or cell receptor) by half. According to the FDA, IC 50 represents the concentration of a drug that is required for 50% inhibition in vitro. Selecting a cell line from the drop down as shown in Figure 13 will display three different lists pertaining to the drugs that fall under the following categories: Drugs with IC 50 values in the given dosage range Drugs with IC 50 values lesser than the tested minimum dosage Drugs with IC 50 values greater than the tested maximum dosage Figure 14. IC 50 Plot Lethality: It offers an indication of the lethal toxicity for a given drug. It provides the degree to which a drug is harmful to a user or how capable it is of causing death. The Lethality section provides a chart showing relative lethality of drugs for all the cell lines uploaded. Figure 15. Lethality Plot Selectivity: It refers to a drug\u2019s ability to preferentially produce a particular effect and is related to the structural specificity of drug binding to receptors. Drug selectivity is an important aspect and is used for evaluating the ADRs (Adverse Drug Reaction) of drugs. The Selectivity section provides a percent proliferation difference vs concentration dosage selectivity plot. Figure 16. Selectivity Plot Select you desired drug as well as the cell lines to get the visualization as shown in Figure 16. Figure 17. Cell line selection for Selectivity","title":"Analysis"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html","text":"Introduction Overview RNA sequencing (RNA-seq) uses next generation sequencing (NGS) to estimate the presence and quantity of RNA in the experimental sample. With increase in popularity of a technique comes the challenge to extract maximum relevant information from the results. RNA Seq app takes the sequenced counts file as an input and allows you to perform downstream analysis on it. Since analysis of RNA sequenced data is a complex and extensive process, it helps the user to correctly analyze and process large datasets. It provides the users the ability to pre-process their data and allow them to fully assess the quality of their data, determine the accuracy of their own analysis, and tailor the analysis as per their biological question. Scope of the app Clean data and filter protein coding genes as per the selected species. Perform quantification and normalization on the data. Perform batch correction. Provides PCA plot as per the selected cohort conditions. Presents the data in the form of Heatmap, depicting the expression values of each genes corresponding to the number of samples in the data. Perform differential expression analysis. Perform enrichment analysis of significant genes using Enrichr and X2K anlysis of significant gene set using X2K. Use the differentially expressed results with the output of MetScape to integrate multi-omics data in IntOmix . Figure 1. RNA Seq Workflow Getting Started RNA Seq Workflow requires the following two files as input: User Input Feature counts file This is a .csv file wherein the first column presents the gene ids of the genes you are analyzing your data against followed by subsequent columns containing counts information from each sample Figure 2. Feature counts file Metadata File This is a .csv file wherein the first column presents the samples and would include all the samples used under study. The column Cohort represents your cohort information in which the samples are segregated. Other columns can be added to the metadata file as per your use case but are optional. Figure 3. Metadata file Steps involved in data processing Upload input files Clean data and perform quantification, normalization and batch correction on the data Perform differential expression followed by X2K, Enrichr and PGSEA Caveats The presence of both Samples and Cohort columns in a metadata file is mandatory. Tutorial Select RNA Seq Workflow from the dashboard under the Screening and Sequencing Data tab. Figure 4. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the Session to be redirected to RNA Seq Workflow 's upload page Figure 5. Workspace selection Upload Files Upload the feature countsa and metadata file using the drop downs as shown in Figure 6. Figure 6. Upload interface Pre-processing Pre-processing tab contains four sub-tabs which allow you to specify parameters at which to analyze the uploaded data. The sub tabs are as follows: Data Cleaning Data cleaning interface presents a visual summary of the counts file which includes the number of genes, number of duplicates in the data, and number of NA and NaN values. It allows you to choose from a set of predefined operations to clean and filter data. These include: Species selection: Human Mouse Data Cleaning methods: Removing NA and NaN values: Rows having even a single NA or NaN values are removed. Removing duplicates: Duplicates are removed on the basis of a row's mean value, i.e. The one with greater row mean value is retained. Filter Protein Coding Genes: Only the Protein coding genes are filtered from the complete dataset based on the specified species. Replace Ensemble ID with HGNC Symbol: In case the input data has ENSEMBL IDs, they can be replaced with the HGNC symbols (mandatory for further analysis). Select the desired cleaning method (multiple methods can be selected) and specify the species under study. Once the selection is done, click on Clean Data . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 7. Data cleaning Quantification Quantification interface provides the following options: Reads Per Kilobase of transcript, per Million mapped reads (RPKM): It is a normalized unit of transcript expression. It scales by transcript length to compensate for the fact that most RNA-seq protocols will generate more sequencing reads from longer RNA molecules. RPKM is calculated as mentioned below: Count up the total reads in a sample and divide that number by 1,000,000 \u2013 this is our \u201cper million\u201d scaling factor. Divide the read counts by the \u201cper million\u201d scaling factor. This normalizes for sequencing depth, giving you reads per million (RPM). Divide the RPM values by the length of the gene, in kilobases. This gives you RPKM. Transcripts Per Million (TPM): TPM is very similar to RPKM. The only difference is the order of operation. TPM is calculated as mentioned below: Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK). Count up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor. Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM. No Quantification: This is to undo any quantification that you may have done. It is equivalent to skipping quantification altogether. Select the desired quantification (only a single method can be selected at a time). Once the selection is done, click on Quantify . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 8. Quantification interface Normalization The Normalization tab allows you to adjust raw data to account for factors that prevent direct comparison of expression measures. It allows you to choose from a set of predefined operations to normalize. This will be performed on top of quantified data if quantified. Quantile: It is a technique for making two distributions identical in statistical properties. To quantile normalize two or more distributions to each other without a reference distribution, sort as before, then set to the average (usually, arithmetical mean) of the distributions. The highest value in all cases becomes the mean of the highest values, the second highest value becomes the mean of the second highest values, and so on. In the app, this is done using normalize.quantiles function in R. R-Log: This function transforms the count data to the log 2 scale in a way which minimizes differences between samples for rows with small counts, and which normalizes with respect to library size. In the app, this is performed using library DESeq. Since, this operation is only performed on counts data (no quantification/normalization/log transformation), trying to run this function after quantification will throw an error. To supersede that you will need to go back to Quantification tab and select Mo Quantification , and then select rlog . Cyclic Loess: This function is intended to normalize single channel or A-value microarray intensities between arrays. Cyclic loess normalization has similar effect and intention as quantile normalization, but with some advantages, in particular the ability to incorporate probe weights. In the app, this is performed using normalizeCyclicLoess function of the limma package. Select the desired normalization method (only a single method can be selected at a time). Once the selection is done, click on Normalize . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 9. Normalization interface Batch Correction The Batch Correction tab allows you to select a condition from metadata on which you want to run batch effect correction. The app uses sva package to remove the batch effect. ComBat allows you to adjust for batch effects in datasets where the batch covariate is known, using the methodology described in Johnson et al. 2007. It uses either parametric or non-parametric empirical Bayes frameworks for adjusting data for batch effects. You are returned an expression matrix that has been corrected for batch effects. The input data are assumed to be cleaned and normalized before batch effect removal. Histogram of the data is shown by taking log of the data (after Batch Correction). Data Table, showing how the data looks like after performing the above mentioned operations. Select the cohort condition from the drop down list (single selection). Once the selection is done, click on Run Batch Correction . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 10. Batch correction PCA Plot PCA plot allows you to understand the clustering pattern between biologically grouped and ungrouped samples. It provides viusalization by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 11. PCA Plot Heat map A heat map is a graphical representation of data where the individual values contained in a matrix are represented on a color scale. Here the heat map represents expression value of each gene corresponding to the number of samples in the data. The filter icon can be used to filter out the desired samples and genes. Once the filter is applied, the heat map containing the selected genes and sample would b generated. Figure 12. Heat map Differential expression The goal of differential expression analysis is to identify genes whose expression differs under different conditions. An important consideration for differential expression analysis is correction for multiple testing. This is a statistical phenomenon that occurs when thousands of comparisons (e.g. the comparison of expression of multiple genes in multiple conditions) are performed for a small number of samples. Following are the components in this tab: Three types of statistical tests are provided in the app, namely, t-test,limma and One-Way ANOVA. t-test: To perform t-test, the app uses ttest function in R. limma: To perform limma, limma function of the limma library is used. One-Way ANOVA: To perform One-Way ANOVA, the app uses aov function in R. For p -value correction, following two methods are provided: Bonferroni: In this method the p -values are multiplied by the number of comparisons. BH: The BH method helps you to avoid Type I errors (false positives). Adjusting the rate helps to control for the fact that sometimes small p -values (less than 5%) happen by chance, which could lead you to incorrectly reject the true null hypotheses. p.adjust function takes care of the correction. Select the statistical test, p -val correction method, cohort condition and cohort combination. Once the selection is done click on Go . A volcano plot along with the data table which can be used to quickly identify changes in large data sets composed of replicate data is shown. It plots significance versus fold-change on the y and x axes, respectively. This plot colours the significant and insignificant genes based on p -value or FDR and log 2 FC values. Based on positive or negative log 2 FC values, you can choose to further filter the significant genes and create a gene set of up-regulated or down-regulated genes which can then be relayed to Enrichr. By default it takes into consideration all the significant genes. Figure 13. Differential expression X2K analysis (eXpression2Kinases) The input for X2K analysis is a gene set consisting significant genes which is formed in the same manner as is for Enrichr in the previous step. Both the steps are autonomous and are not dependent on each other. Once we have the gene set handy, X2K analysis is performed on it, the only other input required from the user is TF-target gene background database to be used for enrichment You can choose from the following databases to perform the analysis. ChEA 2015 ENCODE 2015 ChEA & ENCODE Consensus Transfac and Jaspar ChEA 2016 ARCHS4 TFs Coexp CREEDS Enrichr Submissions TF-Gene co-occurrence On selecting all the options along with the target database, click on the option Go . The enrichment results from the Transcription Factor Enrichment Analysis (TFEA), kinase enrichment analysis (KEA) and Expression2Kinase (X2K) can be viewed from the TFEA,KEA and X2K tabs respectively. Figure 14. X2K Analysis (TFEA) Figure 15. X2K Analysis (KEA) Figure 16. X2K Analysis (X2K) Enrichr (Enrichment analysis) Enrichr performs the enrichment analysis on the gene set relayed, either in the Differential Expression tab or X2K analysis tab. Enrichment analysis is a computational method for inferring knowledge about an input gene set by comparing it to annotated gene sets representing prior biological knowledge. Enrichment analysis checks whether an input set of genes significantly overlaps with annotated gene sets. To perfomr the enrichment analysis, select the desired Enrichr database. Figure 17. Enrichr PGSEA (Parametric Gene Set Enrichment Analysis) It avails you to do exploratory parametric analysis of gene expression data. This type of analysis can assist in determining of lists of genes, such as those deregulated in defined experimental systems, are similarly deregulated in other data sets. It further subsets the data based on lists of genes, computes a summary statistic for each gene list, and returns the results in the form of Heat map. Click on Pathway Database option to select the database of interest from the drop down. Clicking on Run PGSEA will generate a data table containing the list of genes along with the heat map. Figure 18. PGSEA References Johnson, W. Evan, Cheng Li, and Ariel Rabinovic. \"Adjusting batch effects in microarray expression data using empirical Bayes methods.\" Biostatistics 8.1 (2007): 118-127.","title":"RNA Seq Workflow"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#introduction","text":"","title":"Introduction"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#overview","text":"RNA sequencing (RNA-seq) uses next generation sequencing (NGS) to estimate the presence and quantity of RNA in the experimental sample. With increase in popularity of a technique comes the challenge to extract maximum relevant information from the results. RNA Seq app takes the sequenced counts file as an input and allows you to perform downstream analysis on it. Since analysis of RNA sequenced data is a complex and extensive process, it helps the user to correctly analyze and process large datasets. It provides the users the ability to pre-process their data and allow them to fully assess the quality of their data, determine the accuracy of their own analysis, and tailor the analysis as per their biological question.","title":"Overview"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#scope-of-the-app","text":"Clean data and filter protein coding genes as per the selected species. Perform quantification and normalization on the data. Perform batch correction. Provides PCA plot as per the selected cohort conditions. Presents the data in the form of Heatmap, depicting the expression values of each genes corresponding to the number of samples in the data. Perform differential expression analysis. Perform enrichment analysis of significant genes using Enrichr and X2K anlysis of significant gene set using X2K. Use the differentially expressed results with the output of MetScape to integrate multi-omics data in IntOmix . Figure 1. RNA Seq Workflow","title":"Scope of the app"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#getting-started","text":"RNA Seq Workflow requires the following two files as input:","title":"Getting Started"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#user-input","text":"Feature counts file This is a .csv file wherein the first column presents the gene ids of the genes you are analyzing your data against followed by subsequent columns containing counts information from each sample Figure 2. Feature counts file Metadata File This is a .csv file wherein the first column presents the samples and would include all the samples used under study. The column Cohort represents your cohort information in which the samples are segregated. Other columns can be added to the metadata file as per your use case but are optional. Figure 3. Metadata file","title":"User Input"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#steps-involved-in-data-processing","text":"Upload input files Clean data and perform quantification, normalization and batch correction on the data Perform differential expression followed by X2K, Enrichr and PGSEA","title":"Steps involved in data processing"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#caveats","text":"The presence of both Samples and Cohort columns in a metadata file is mandatory.","title":"Caveats"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#tutorial","text":"Select RNA Seq Workflow from the dashboard under the Screening and Sequencing Data tab. Figure 4. Polly Dashboard Create a New Workspace or choose the existing one from the drop down and provide the Name of the Session to be redirected to RNA Seq Workflow 's upload page Figure 5. Workspace selection","title":"Tutorial"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#upload-files","text":"Upload the feature countsa and metadata file using the drop downs as shown in Figure 6. Figure 6. Upload interface","title":"Upload Files"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#pre-processing","text":"Pre-processing tab contains four sub-tabs which allow you to specify parameters at which to analyze the uploaded data. The sub tabs are as follows: Data Cleaning Data cleaning interface presents a visual summary of the counts file which includes the number of genes, number of duplicates in the data, and number of NA and NaN values. It allows you to choose from a set of predefined operations to clean and filter data. These include: Species selection: Human Mouse Data Cleaning methods: Removing NA and NaN values: Rows having even a single NA or NaN values are removed. Removing duplicates: Duplicates are removed on the basis of a row's mean value, i.e. The one with greater row mean value is retained. Filter Protein Coding Genes: Only the Protein coding genes are filtered from the complete dataset based on the specified species. Replace Ensemble ID with HGNC Symbol: In case the input data has ENSEMBL IDs, they can be replaced with the HGNC symbols (mandatory for further analysis). Select the desired cleaning method (multiple methods can be selected) and specify the species under study. Once the selection is done, click on Clean Data . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 7. Data cleaning Quantification Quantification interface provides the following options: Reads Per Kilobase of transcript, per Million mapped reads (RPKM): It is a normalized unit of transcript expression. It scales by transcript length to compensate for the fact that most RNA-seq protocols will generate more sequencing reads from longer RNA molecules. RPKM is calculated as mentioned below: Count up the total reads in a sample and divide that number by 1,000,000 \u2013 this is our \u201cper million\u201d scaling factor. Divide the read counts by the \u201cper million\u201d scaling factor. This normalizes for sequencing depth, giving you reads per million (RPM). Divide the RPM values by the length of the gene, in kilobases. This gives you RPKM. Transcripts Per Million (TPM): TPM is very similar to RPKM. The only difference is the order of operation. TPM is calculated as mentioned below: Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK). Count up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor. Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM. No Quantification: This is to undo any quantification that you may have done. It is equivalent to skipping quantification altogether. Select the desired quantification (only a single method can be selected at a time). Once the selection is done, click on Quantify . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 8. Quantification interface Normalization The Normalization tab allows you to adjust raw data to account for factors that prevent direct comparison of expression measures. It allows you to choose from a set of predefined operations to normalize. This will be performed on top of quantified data if quantified. Quantile: It is a technique for making two distributions identical in statistical properties. To quantile normalize two or more distributions to each other without a reference distribution, sort as before, then set to the average (usually, arithmetical mean) of the distributions. The highest value in all cases becomes the mean of the highest values, the second highest value becomes the mean of the second highest values, and so on. In the app, this is done using normalize.quantiles function in R. R-Log: This function transforms the count data to the log 2 scale in a way which minimizes differences between samples for rows with small counts, and which normalizes with respect to library size. In the app, this is performed using library DESeq. Since, this operation is only performed on counts data (no quantification/normalization/log transformation), trying to run this function after quantification will throw an error. To supersede that you will need to go back to Quantification tab and select Mo Quantification , and then select rlog . Cyclic Loess: This function is intended to normalize single channel or A-value microarray intensities between arrays. Cyclic loess normalization has similar effect and intention as quantile normalization, but with some advantages, in particular the ability to incorporate probe weights. In the app, this is performed using normalizeCyclicLoess function of the limma package. Select the desired normalization method (only a single method can be selected at a time). Once the selection is done, click on Normalize . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 9. Normalization interface Batch Correction The Batch Correction tab allows you to select a condition from metadata on which you want to run batch effect correction. The app uses sva package to remove the batch effect. ComBat allows you to adjust for batch effects in datasets where the batch covariate is known, using the methodology described in Johnson et al. 2007. It uses either parametric or non-parametric empirical Bayes frameworks for adjusting data for batch effects. You are returned an expression matrix that has been corrected for batch effects. The input data are assumed to be cleaned and normalized before batch effect removal. Histogram of the data is shown by taking log of the data (after Batch Correction). Data Table, showing how the data looks like after performing the above mentioned operations. Select the cohort condition from the drop down list (single selection). Once the selection is done, click on Run Batch Correction . The visual summary along with the boxplot and data table will get updated once the processing is done. Figure 10. Batch correction","title":"Pre-processing"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#pca-plot","text":"PCA plot allows you to understand the clustering pattern between biologically grouped and ungrouped samples. It provides viusalization by selecting the PC values for x- and y- axes. It\u2019s also possible to specify the cohort order for the plots. Figure 11. PCA Plot","title":"PCA Plot"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#heat-map","text":"A heat map is a graphical representation of data where the individual values contained in a matrix are represented on a color scale. Here the heat map represents expression value of each gene corresponding to the number of samples in the data. The filter icon can be used to filter out the desired samples and genes. Once the filter is applied, the heat map containing the selected genes and sample would b generated. Figure 12. Heat map","title":"Heat map"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#differential-expression","text":"The goal of differential expression analysis is to identify genes whose expression differs under different conditions. An important consideration for differential expression analysis is correction for multiple testing. This is a statistical phenomenon that occurs when thousands of comparisons (e.g. the comparison of expression of multiple genes in multiple conditions) are performed for a small number of samples. Following are the components in this tab: Three types of statistical tests are provided in the app, namely, t-test,limma and One-Way ANOVA. t-test: To perform t-test, the app uses ttest function in R. limma: To perform limma, limma function of the limma library is used. One-Way ANOVA: To perform One-Way ANOVA, the app uses aov function in R. For p -value correction, following two methods are provided: Bonferroni: In this method the p -values are multiplied by the number of comparisons. BH: The BH method helps you to avoid Type I errors (false positives). Adjusting the rate helps to control for the fact that sometimes small p -values (less than 5%) happen by chance, which could lead you to incorrectly reject the true null hypotheses. p.adjust function takes care of the correction. Select the statistical test, p -val correction method, cohort condition and cohort combination. Once the selection is done click on Go . A volcano plot along with the data table which can be used to quickly identify changes in large data sets composed of replicate data is shown. It plots significance versus fold-change on the y and x axes, respectively. This plot colours the significant and insignificant genes based on p -value or FDR and log 2 FC values. Based on positive or negative log 2 FC values, you can choose to further filter the significant genes and create a gene set of up-regulated or down-regulated genes which can then be relayed to Enrichr. By default it takes into consideration all the significant genes. Figure 13. Differential expression","title":"Differential expression"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#x2k-analysis-expression2kinases","text":"The input for X2K analysis is a gene set consisting significant genes which is formed in the same manner as is for Enrichr in the previous step. Both the steps are autonomous and are not dependent on each other. Once we have the gene set handy, X2K analysis is performed on it, the only other input required from the user is TF-target gene background database to be used for enrichment You can choose from the following databases to perform the analysis. ChEA 2015 ENCODE 2015 ChEA & ENCODE Consensus Transfac and Jaspar ChEA 2016 ARCHS4 TFs Coexp CREEDS Enrichr Submissions TF-Gene co-occurrence On selecting all the options along with the target database, click on the option Go . The enrichment results from the Transcription Factor Enrichment Analysis (TFEA), kinase enrichment analysis (KEA) and Expression2Kinase (X2K) can be viewed from the TFEA,KEA and X2K tabs respectively. Figure 14. X2K Analysis (TFEA) Figure 15. X2K Analysis (KEA) Figure 16. X2K Analysis (X2K)","title":"X2K analysis (eXpression2Kinases)"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#enrichr-enrichment-analysis","text":"Enrichr performs the enrichment analysis on the gene set relayed, either in the Differential Expression tab or X2K analysis tab. Enrichment analysis is a computational method for inferring knowledge about an input gene set by comparing it to annotated gene sets representing prior biological knowledge. Enrichment analysis checks whether an input set of genes significantly overlaps with annotated gene sets. To perfomr the enrichment analysis, select the desired Enrichr database. Figure 17. Enrichr","title":"Enrichr (Enrichment analysis)"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#pgsea-parametric-gene-set-enrichment-analysis","text":"It avails you to do exploratory parametric analysis of gene expression data. This type of analysis can assist in determining of lists of genes, such as those deregulated in defined experimental systems, are similarly deregulated in other data sets. It further subsets the data based on lists of genes, computes a summary statistic for each gene list, and returns the results in the form of Heat map. Click on Pathway Database option to select the database of interest from the drop down. Clicking on Run PGSEA will generate a data table containing the list of genes along with the heat map. Figure 18. PGSEA","title":"PGSEA (Parametric Gene Set Enrichment Analysis)"},{"location":"Apps/Sequencing Data/RNA Seq Workflow.html#references","text":"Johnson, W. Evan, Cheng Li, and Ariel Rabinovic. \"Adjusting batch effects in microarray expression data using empirical Bayes methods.\" Biostatistics 8.1 (2007): 118-127.","title":"References"},{"location":"Getting Started/Workspaces.html","text":"What are workspaces? Polly workspaces allow you to reduce clutter and organize your data in a folder system of choice. This can be accessed by clicking on the Workspaces card. Figure 1. Polly Homepage There are 3 panels on the workspace interface. The left panel allows you to create a new workspace and access all your workspaces. The middle panel shows you all the content within the selected workspace. The panel on the right shows you the details around the content within the workspace. Figure 2. Workspace Panels Creating a workspace You can create a new workspace on Polly by clicking on New Workspace on the left panel. Figure 3. Creating New Workspace Enter the Name of the workspace and description (if any) and click on save . The new workspace will be created. You can find the newly created workspace with the list of all your workspaces on the left panel. Figure 4. Workspace Description The information about the contents of the workspace can be seen on the right panel. Figure 5. Workspace Information While the description of workspace and other information can be accessed through the Info option Figure 6. Workspace Info option Content within a workspace A workspace contains all your data, analyses , notebooks , and reports . They can be accessed through the middle panel. Figure 7. Workspace Contents Adding files and notebooks to your workspace Using GUI To add files to your workspaces, click on the cloud upload icon present above the middle panel. You can choose the Upload Files or Upload a Notebook option from within the menu in order to upload the data files and notebooks respectively. Note: If your file size is greater than 100 MB , use the Polly CLI option to upload to the workspace. Figure 8. File Addition Using CLI You can use PollyCLI as well to import your data directly within Polly. This is a more convenient option for uploading large data (more than 100 MB in size). The steps for the same are detailed here . Filtering the content In order to reduce the clutter, workspaces allow you to filter the content you want to display within the middle panel. Click on Filter icon above the middle panel where you can select/deselect the file type you would want to show in the workspace. Once the selection is done click on Apply to confirm your selections. Figure 9. Content Filtering Creating a New folder Polly workspace allows you to organize and manage your data in folders and subfolders. To create a folder, click on the cloud upload icon and select Create a New Folder from the dropdown menu. Figure 10. Create Folder Enter the name of the folder you want to create and click on Create Folder . Figure 11. Add Folder Name You can click on the same icon again to upload files within the created folder. Note: Only the data files can be organized within a folder while the uploaded notebooks and analyses are present separately within the selected workspace. Navigating through a folder Double click on the folder name to look at its contents. Figure 12. Folder Navigation The purple section in the middle panel shows the path of the directory you are in. You can click on the name of any folder within the path to select the folder you want to browse to. Figure 13. Folder Navigation Path Editing your workspace The menu beside the workspace name shows the menu items to edit your workspace details. Figure 14. Workspace Edition Select Edit Details within the menu. Rename the workspace and/or change the description of the workspace and click on Update . Click Cancel to close the tab without saving any changes. Figure 15. Workspace Edit Update Sharing your workspace The menu beside the workspace name shows the menu items to share your workspace. Select Share within the menu to share your workspace. Figure 16. Workspace Sharing The same action can be achieved by clicking on Add Collaborator icon placed at the top right corner. Figure 17. Workspace Sharing You can add the email of the Polly user you want to share the workspace with under Add Collaborator . There are 2 types of workspace permission you can give to your users. Admin : This permission allows the user to rename, add and delete files, analysis, code, and reports within the workspace. They can share the workspace and restore any analysis within it. Writer : This permission allows the user to only add new files, analysis, code, and reports within the workspace. They cannot delete anything from within the shared workspace . They can also restore any analysis within the workspace but does not have the permission to share the workspace with others. On selecting the desired permission, click Add to add the user to the list of collaborators. Once all the users are added, click on Done . You can check all collaborators for a workspace on the top panel at the right. Each collaborator is represented with a user icon. Figure 18. Add Collaborators Changing share permission for a user You can change the access permission for a user at any point in time. Click on the user icons at the top right. A window with the list of collaborators will appear. From the dropdown, select the permission you would want to give to the user and click the tick next to it. Click on Done once completed. Figure 19. Changing share permission Removing a collaborator You can choose to remove a collaborator as well. Click on the cross icon present across the name of the user. Click on Yes to permanently remove the user or No to disregard the changes. Figure 20. Removing a collaborator Deleting a workspace Open the menu beside the workspace name. Select Delete within the menu to delete your workspace. Once a workspace is deleted, all its contents are deleted as well for all the collaborators. Figure 21. Deleting a workspace Deleting items within a workspace Deleting Multiple Files: Select the files that you want to delete through the checkbox. The selection menu will appear on the top right. Click on Delete icon . Figure 22. Deleting Multiple Files Confirm by clicking inside the checkbox and then click on Delete to delete your selected files. Figure 23. Deletion Permission Deleting a single file: If you need to delete a single file, an alternate option can also be used. Click on the kebab menu present at the right end and choose the Delete option. Figure 24. Deleting a single file Renaming a file within workspace Click across the file you want to rename on the kebab menu. Select the Rename option. Figure 25. Renaming Workspace Provide the new name to the file and click on Rename to confirm your changes. Figure 26. Confirm Renaming Accessing and Downloading a file within a workspace Using GUI To access and download files from your workspaces, Click on the file to look at the file details on the right panel. You can look at the versions of the file along with general details like the file size and modifications date. Note: If your file size is greater than 100 MB , use the Polly CLI option to access and download it. You can download the file by choosing the Download option provided at the bottom of the panel. Figure 27. Accessing and Downloading a file Using CLI You can use PollyCLI as well to import your data directly within Polly. This is a more convenient option for accessing large data (more than 100 MB in size). The steps for the same are detailed here . Accessing and Restoring an analysis within a workspace Click on the analysis to look at the file details on the right panel. You can look at your saved states as versions of your analysis along with input details. Figure 28. Accessing and Restoring the saved states Choose the version through the checkbox and click on the Restore icon to restore the saved state. Figure 29. Accessing and Restoring the analysis You can also choose to look at the state history by clicking on the History icon . It gives a detailed description of the parameters and the input files used to save the selected state. Restore after reassuration. Figure 30. Accessing and Restoring the analysis For some applications, you can select the algorithm to look at the parameters used for the selected analysis. Click on the History icon and then Retore the selected algorithm. Figure 31. History icon Figure 32. Restoring algorithm Accessing and Restoring a Pollyglot notebook within a workspace Select the Polly Notebook you want to launch. You can look at the notebook details on the right panel. If you are running the notebook for the first time, the option Edit and Launch would appear as a default selection to launch the selected notebook. You are required to select an environment and a machine to run the given notebook, once these selections are done you can launch the notebook. Figure 33. Accessing and Restoring a Notebook Select the desired Docker Environment from the drop-down menu. Figure 34. Docker Environment Selection as well as the Machine Type required to run the job. Once the selections are done, click on Launch . Figure 35. Machine Type Selection For an older notebook: You have two options, you can either launch the notebook directly by the Launch button or you can choose to edit it first before launching through the Edit and Launch button. Note: Under the edit option, you can only change the machine type. The docker environment would remain the same as the one selected when you run the notebook for the first time. Figure 36. Launch for old notebooks Starting a New Analysis Click on the New Analysis option provided at the top of the right panel. You will be redirected to the Application interface where you can select an application to launch. Figure 37. Starting a New Analysis Accessing Documentation Click on the \u201c?\u201d icon present at the bottom left corner to access documentation. Figure 38. Accessing Documentation Logging out of Polly Click on the user icon at the bottom left corner. Select the Log Out option for logging out of Polly. Figure 39. Logging out of Polly","title":"Workspaces"},{"location":"Getting Started/Workspaces.html#what-are-workspaces","text":"Polly workspaces allow you to reduce clutter and organize your data in a folder system of choice. This can be accessed by clicking on the Workspaces card. Figure 1. Polly Homepage There are 3 panels on the workspace interface. The left panel allows you to create a new workspace and access all your workspaces. The middle panel shows you all the content within the selected workspace. The panel on the right shows you the details around the content within the workspace. Figure 2. Workspace Panels","title":"What are workspaces?"},{"location":"Getting Started/Workspaces.html#creating-a-workspace","text":"You can create a new workspace on Polly by clicking on New Workspace on the left panel. Figure 3. Creating New Workspace Enter the Name of the workspace and description (if any) and click on save . The new workspace will be created. You can find the newly created workspace with the list of all your workspaces on the left panel. Figure 4. Workspace Description The information about the contents of the workspace can be seen on the right panel. Figure 5. Workspace Information While the description of workspace and other information can be accessed through the Info option Figure 6. Workspace Info option","title":"Creating a workspace"},{"location":"Getting Started/Workspaces.html#content-within-a-workspace","text":"A workspace contains all your data, analyses , notebooks , and reports . They can be accessed through the middle panel. Figure 7. Workspace Contents","title":"Content within a workspace"},{"location":"Getting Started/Workspaces.html#adding-files-and-notebooks-to-your-workspace","text":"Using GUI To add files to your workspaces, click on the cloud upload icon present above the middle panel. You can choose the Upload Files or Upload a Notebook option from within the menu in order to upload the data files and notebooks respectively. Note: If your file size is greater than 100 MB , use the Polly CLI option to upload to the workspace. Figure 8. File Addition Using CLI You can use PollyCLI as well to import your data directly within Polly. This is a more convenient option for uploading large data (more than 100 MB in size). The steps for the same are detailed here .","title":"Adding files and notebooks to your workspace"},{"location":"Getting Started/Workspaces.html#filtering-the-content","text":"In order to reduce the clutter, workspaces allow you to filter the content you want to display within the middle panel. Click on Filter icon above the middle panel where you can select/deselect the file type you would want to show in the workspace. Once the selection is done click on Apply to confirm your selections. Figure 9. Content Filtering","title":"Filtering the content"},{"location":"Getting Started/Workspaces.html#creating-a-new-folder","text":"Polly workspace allows you to organize and manage your data in folders and subfolders. To create a folder, click on the cloud upload icon and select Create a New Folder from the dropdown menu. Figure 10. Create Folder Enter the name of the folder you want to create and click on Create Folder . Figure 11. Add Folder Name You can click on the same icon again to upload files within the created folder. Note: Only the data files can be organized within a folder while the uploaded notebooks and analyses are present separately within the selected workspace.","title":"Creating a New folder"},{"location":"Getting Started/Workspaces.html#navigating-through-a-folder","text":"Double click on the folder name to look at its contents. Figure 12. Folder Navigation The purple section in the middle panel shows the path of the directory you are in. You can click on the name of any folder within the path to select the folder you want to browse to. Figure 13. Folder Navigation Path","title":"Navigating through a folder"},{"location":"Getting Started/Workspaces.html#editing-your-workspace","text":"The menu beside the workspace name shows the menu items to edit your workspace details. Figure 14. Workspace Edition Select Edit Details within the menu. Rename the workspace and/or change the description of the workspace and click on Update . Click Cancel to close the tab without saving any changes. Figure 15. Workspace Edit Update","title":"Editing your workspace"},{"location":"Getting Started/Workspaces.html#sharing-your-workspace","text":"The menu beside the workspace name shows the menu items to share your workspace. Select Share within the menu to share your workspace. Figure 16. Workspace Sharing The same action can be achieved by clicking on Add Collaborator icon placed at the top right corner. Figure 17. Workspace Sharing You can add the email of the Polly user you want to share the workspace with under Add Collaborator . There are 2 types of workspace permission you can give to your users. Admin : This permission allows the user to rename, add and delete files, analysis, code, and reports within the workspace. They can share the workspace and restore any analysis within it. Writer : This permission allows the user to only add new files, analysis, code, and reports within the workspace. They cannot delete anything from within the shared workspace . They can also restore any analysis within the workspace but does not have the permission to share the workspace with others. On selecting the desired permission, click Add to add the user to the list of collaborators. Once all the users are added, click on Done . You can check all collaborators for a workspace on the top panel at the right. Each collaborator is represented with a user icon. Figure 18. Add Collaborators","title":"Sharing your workspace"},{"location":"Getting Started/Workspaces.html#changing-share-permission-for-a-user","text":"You can change the access permission for a user at any point in time. Click on the user icons at the top right. A window with the list of collaborators will appear. From the dropdown, select the permission you would want to give to the user and click the tick next to it. Click on Done once completed. Figure 19. Changing share permission","title":"Changing share permission for a user"},{"location":"Getting Started/Workspaces.html#removing-a-collaborator","text":"You can choose to remove a collaborator as well. Click on the cross icon present across the name of the user. Click on Yes to permanently remove the user or No to disregard the changes. Figure 20. Removing a collaborator","title":"Removing a collaborator"},{"location":"Getting Started/Workspaces.html#deleting-a-workspace","text":"Open the menu beside the workspace name. Select Delete within the menu to delete your workspace. Once a workspace is deleted, all its contents are deleted as well for all the collaborators. Figure 21. Deleting a workspace","title":"Deleting a workspace"},{"location":"Getting Started/Workspaces.html#deleting-items-within-a-workspace","text":"Deleting Multiple Files: Select the files that you want to delete through the checkbox. The selection menu will appear on the top right. Click on Delete icon . Figure 22. Deleting Multiple Files Confirm by clicking inside the checkbox and then click on Delete to delete your selected files. Figure 23. Deletion Permission Deleting a single file: If you need to delete a single file, an alternate option can also be used. Click on the kebab menu present at the right end and choose the Delete option. Figure 24. Deleting a single file","title":"Deleting items within a workspace"},{"location":"Getting Started/Workspaces.html#renaming-a-file-within-workspace","text":"Click across the file you want to rename on the kebab menu. Select the Rename option. Figure 25. Renaming Workspace Provide the new name to the file and click on Rename to confirm your changes. Figure 26. Confirm Renaming","title":"Renaming a file within workspace"},{"location":"Getting Started/Workspaces.html#accessing-and-downloading-a-file-within-a-workspace","text":"Using GUI To access and download files from your workspaces, Click on the file to look at the file details on the right panel. You can look at the versions of the file along with general details like the file size and modifications date. Note: If your file size is greater than 100 MB , use the Polly CLI option to access and download it. You can download the file by choosing the Download option provided at the bottom of the panel. Figure 27. Accessing and Downloading a file Using CLI You can use PollyCLI as well to import your data directly within Polly. This is a more convenient option for accessing large data (more than 100 MB in size). The steps for the same are detailed here .","title":"Accessing and Downloading a file within a workspace"},{"location":"Getting Started/Workspaces.html#accessing-and-restoring-an-analysis-within-a-workspace","text":"Click on the analysis to look at the file details on the right panel. You can look at your saved states as versions of your analysis along with input details. Figure 28. Accessing and Restoring the saved states Choose the version through the checkbox and click on the Restore icon to restore the saved state. Figure 29. Accessing and Restoring the analysis You can also choose to look at the state history by clicking on the History icon . It gives a detailed description of the parameters and the input files used to save the selected state. Restore after reassuration. Figure 30. Accessing and Restoring the analysis For some applications, you can select the algorithm to look at the parameters used for the selected analysis. Click on the History icon and then Retore the selected algorithm. Figure 31. History icon Figure 32. Restoring algorithm","title":"Accessing and Restoring an analysis within a workspace"},{"location":"Getting Started/Workspaces.html#accessing-and-restoring-a-pollyglot-notebook-within-a-workspace","text":"Select the Polly Notebook you want to launch. You can look at the notebook details on the right panel. If you are running the notebook for the first time, the option Edit and Launch would appear as a default selection to launch the selected notebook. You are required to select an environment and a machine to run the given notebook, once these selections are done you can launch the notebook. Figure 33. Accessing and Restoring a Notebook Select the desired Docker Environment from the drop-down menu. Figure 34. Docker Environment Selection as well as the Machine Type required to run the job. Once the selections are done, click on Launch . Figure 35. Machine Type Selection For an older notebook: You have two options, you can either launch the notebook directly by the Launch button or you can choose to edit it first before launching through the Edit and Launch button. Note: Under the edit option, you can only change the machine type. The docker environment would remain the same as the one selected when you run the notebook for the first time. Figure 36. Launch for old notebooks","title":"Accessing and Restoring a Pollyglot notebook within a workspace"},{"location":"Getting Started/Workspaces.html#starting-a-new-analysis","text":"Click on the New Analysis option provided at the top of the right panel. You will be redirected to the Application interface where you can select an application to launch. Figure 37. Starting a New Analysis","title":"Starting a New Analysis"},{"location":"Getting Started/Workspaces.html#accessing-documentation","text":"Click on the \u201c?\u201d icon present at the bottom left corner to access documentation. Figure 38. Accessing Documentation","title":"Accessing Documentation"},{"location":"Getting Started/Workspaces.html#logging-out-of-polly","text":"Click on the user icon at the bottom left corner. Select the Log Out option for logging out of Polly. Figure 39. Logging out of Polly","title":"Logging out of Polly"},{"location":"Scaling compute/Polly CLI.html","text":"What is Polly CLI? Polly CLI (Command Line Interface) is an open source tool that enables you to interact with Polly services using commands in your command-line shell. Polly CLI lets you upload data and run jobs on the Polly cloud infrastructure by scaling computation resources as per need. You can start and stop jobs, monitor them and view logs. Required System Configurations Polly CLI can work on any Unix based system (Linux and Mac distributions). It does not work on Windows. We will be releasing a Windows version soon. It can be used on local computers as well as cloud instances and servers. There are no specific machine configurations required for Polly CLI. It can work on a system with as low as 512 MB RAM and 1 CPU. Installation Dependencies Required for Polly CLI The following dependencies are required to be installed before installing Polly CLI: Node and npm : Linux: For installation on Linux, follow the steps mentioned here . Mac: For installation on Mac, follow the steps mentioned here . Commands to install To install Polly CLI, run the following commands on Terminal / Command prompt: Linux: sudo npm install -g @elucidatainc/pollycli Mac: npm install -g @elucidatainc/pollycli Commands to uninstall To uninstall Polly CLI, run the following commands on Terminal / Command prompt: Linux: sudo npm uninstall -g @elucidatainc/pollycli Mac: npm uninstall -g @elucidatainc/pollycli Note: sudo might have to be used before every command while accessing Polly CLI on cloud instance or server. Logging in and out of Polly CLI Log in Open the terminal on the system and execute the following command to log in. polly login Enter the Polly Username and Password when prompted. Figure 1. Polly Login Once logged in, you will stay logged in the system and won\u2019t need to log in again even if a new terminal is opened or the system is restarted. You will only need to log in again if you manually log out from the system. When you are in the Polly environment, you can log in to Polly CLI without the need to input credentials. We auto detect the Polly permissions or the user. The following are the ways to auto log in to Polly in different Polly environments: Notebook: You are automatically logged in to Polly CLI as the same user as your Polly log in. You can directly start using all the Polly CLI commands as you wish. Polly CLI jobs: If you want to access Polly CLI inside a job, you will have to install Polly CLI in the docker of the job and execute the following command to log in. polly login --auto Log out Execute the following command to log out polly logout Note: Logging out will not stop a running job. Create and View Polly Workspaces What are Polly Workspaces? Polly Workspaces are online workspaces that contain data, analyses, code, logs etc for a specific workspace or experiment. Data is stored and Analysis is performed within a user chosen workspace. More details about workspaces is mentioned here . Creating a new Workspace To create a new workspace, use the following command. polly workspaces create You will be asked to name the Workspace and provide a description. Once the workspace is created, the workspace name and ID will be shown on the screen. This workspace ID will be needed while creating a JSON file for running jobs. Figure 2. Create Workspace View Workspaces To view all the existing Workspaces with access, use the following command. polly workspaces list You will be asked to select which Workspaces to list: all: On selecting this, all the Workspaces will be listed. latest or oldest: On selecting this you will be asked to enter the number of workspaces as shown in the image below. Figure 3. List Workspaces Workspace ID will be required for transferring data and running jobs. Data Transfer Polly CLI can be used to transfer large data to and from Polly Workspaces. Upto 5 TBs of data can be transferred in one go. Listing the files and folders in a Polly Workspace Directory Files and folders within any Polly Workspace can be listed using the following command. polly files list --workspace-id --workspace-path Workspace ID can be obtained by using the command polly workspaces list as explained in the previous section. The path to the workspace directory has to start with polly:// . Eg - The following command will list all the files and folders within the folder ABC in the workspace 1234. polly files list --workspace-id 1234 --workspace-path polly://ABC/ Note: This command only shows files and folders just one layer within the directory mentioned (just like the ls command on terminal). If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Manually sync data to and from Polly Polly CLI can be used to sync the data between a Polly Workspace and a local directory. Data can be synced manually both ways using the following command. polly files sync --workspace-id --source --destination Workspace ID of the workspace where the data is being synced has to be mentioned in the --workspace-id option. Source and destination can be Polly workspace path as well as local path. Workspace path should start with polly:// followed by the directory path in the workspace where the data is to be synced. Here polly:// is the root directory for the mentioned workspace. The following command will sync data from Polly workspace to current local directory. polly files sync --workspace-id 1234 --source polly://directory1/ --destination ./ The following command will sync data from current local directory to Polly Workspace directory. polly files sync --workspace-id 1234 --source ./ --destination polly://directory1/ Note: Only files that have been changed or added new will get transferred using the sync command. The files that remained unchanged after the last sync will not get transferred. This command can only be used for folders or directories (not for individual files). To transfer just a single file to or from Polly, use the copy command mentioned in the next section. If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Copy files to and from Polly Files can be copied to and from a Polly Workspace using the following command. polly files copy --workspace-id --source --destination This command will copy an individual file from source to destination. The transfer can be from or to Polly Workspace depending on the source and destination defined. Workspace path should start with polly:// followed by the directory structure within the Workspace. Note: If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Docker Management Polly has its own docker repository where dockers can be managed. The advantage of the Polly Docker Repository is that these dockers can also be stored within the same platform as data, code, and analyses and to access the stored dockers all you need is the Polly login credential. Docker login and logout In order to login/logout of the docker repository you can run the following command: polly dockers login polly dockers logout The output generated from the above command should then be run on the terminal. Note: Add sudo before the command if required according to system settings. Figure 4. Example output generated from above commands Create a Docker Repository Docker repository is a collection of dockers where you can publish and access your docker images. Here you can store one or more versions of your docker image. Every docker can have a tag and if a docker with the same tag is pushed again, it will overwrite the older docker after saving its older version that can still be accessed and used. There can be multiple such repositories for an organization and only the members of your organization with Polly login credentials can assess them. Use the following command to create a docker repository: polly dockers create --name , < docker_repository_name > --description < description of the repository > Figure 5. Docker Repository Generation List docker repositories You can list all the stored docker repositories in your organization by using this command. polly dockers list --all Figure 6. Polly Docker List List docker repositories commits You can list specific repository to view the various commits or tags stored within it through the Polly docker commit list. polly dockers commit-list --name < docker_repository_name > --all Figure 7. Docker Commit List In order to reduce the indecision when there are multiple dockers with the same name and same tag, you can distinguish them by their unique identifier which is assigned to every docker commit. In case the unique identifier is not passed in the command, the latest version of the docker with that tag will be used. You can go back to older commit as well if required by using its tag and unique identity listed in the Polly docker commit list. Polly Dockers Path Typical path of a docker on Polly is docker.polly.elucidata.io/ < organization >/ < dockername >: < tag > Note: You can have two dockers with the same tag as well, in which case, the latest commit for that tag will be called by default if you call that docker. To call the docker with an older commit, commit hash will be required to specify in the path. docker.polly.elucidata.io/ < organization >/ < dockername >@ < commit_hash > The following example contains 2 docker images with the same tag. The latest image has the tag \u201cactive\u201c. The active image can just be called by using the usual path. docker.polly.elucidata.io/elucidata/16June:latest The image with the inactive tag can be called by specifying the commit hash. docker.polly.elucidata.io/elucidata/16June@sha256:5747316366b8cc9e3021cd7286f42b2d6d81e3d743e2ab571f55bcd5df788cc8 Figure 8. Example of Docker with different tag status Pull/push of dockers The commands for docker\u2019s pull/push are the same commands that you use for pulling and pushing from any other docker registry. The only difference is the path of the docker. Example commands to pull a docker would be: docker.polly.elucidata.io/elucidata/16June:latest docker pull docker.polly.elucidata.io/elucidata/16june@sha256:5747316366b8cc9e3021cd7286f42b2d6d81e3d743e2ab571f55bcd5df788cc8 Running Dockerized Jobs Polly CLI can run dockerized jobs on managed Polly infrastructure. Polly infrastructure will scale computational resources with increased usage. All you need to do is submit a job and rest is taken care of by Polly. Create job description JSON file JSON file is needed to describe the job to be run on Polly. This file should contain the information about the computational resources (machine), docker image, the name of the job and specific commands (if required) to be run after the docker has been run, as keys. Text can be copy pasted from the example below to create the JSON file. { \"machineType\" : \"gp\", \"cpu\": 1, \"memory\": \"1Gi\", \"image\": \"docker/whalesay\", \"tag\": \"latest\", \"name\": \"Single Cell RNA\", \"command\": [ \"cowsay\",\"hello world\" ] } machineType Name of the machine required to run the job needs to be mentioned as per the following table. machineType No. of vCPUs Memory (RAM) No. of GPUs gp 4 16 GB - ci2xlarge 16 32 GB - ci3xlarge 36 72 GB - mi2xlarge 4 32 GB - mi3xlarge 8 64 GB - mi4xlarge 16 122 GB - gpusmall 16 61 GB 1 More machines (including some with GPUs) will be added soon. If you need a specific machine to be added to the list, please contact us at polly@elucidata.io . If computational power required is less than 2 vCPUs and 8 GB RAM, use the keys \u201ccpu\u201d and \u201cmemory\u201d in the JSON file instead of the key \u201cmachineType\u201d . If all 3 keys are present, \u201cmachineType\u201d takes priority and the machine will be assigned accordingly. In the example JSON (image) mentioned above, machine selected will be \u201cgp\u201d with 4 vCPUs and 16 GB RAM and NOT 1vCPU and 1 GB RAM. cpu: Mention the number of CPUs needed here. For smaller jobs, just a part of the CPU can also be chosen. For example, if 0.1 vCPUs are required for the job, the number of CPUs can be mentioned as \u201c100m\u201d . If more than 2 CPUs are required for the job, use the key \u201cmachineType\u201d to choose the relevant machine instead of \u201ccpu\u201d and \u201cmemory\u201d . memory: RAM required needs to be mentioned in text (eg - \u201c1Gi\u201d or \u201c500 Mi\u201d) in this key. If memory needed is more than 8 GB, use the key \u201cmachineType\u201d to choose the relevant machine instead of \u201ccpu\u201d and \u201cmemory\u201d . image : The path to the docker image present in DockerHub or ECR needs to be mentioned in this key. tag: Tag of the docker image needs to be mentioned in this key. name: Name you want to provide to the job has to be mentioned in this key. command: Any commands to be executed after the docker has been run can be mentioned in this key. Docker Building Guidelines While creating a docker to be run on Polly, the following must be taken care of. Dockers must be present in either Docker Hub or Amazon ECR. Soon you will be able to have Dockers directly on Polly. Only self contained dockers can be run on Polly. A self contained docker is one which has the code to get input files as well as upload output files back contained in the docker. Public as well as private dockers are supported. In order to run private dockers, \u201csecret\u201d should be passed as a key in the JSON file. If your private dockers are on Polly itself, you don't require to generate this secret. To get the secret key for the private docker, the following steps need to be followed. For MacOS, you need to remove the key value pair \"credsStore\": \"osxkeychain\" from the config.json file present in the directory /Users/< username >/.docker . You need to be logged in to DockerHub or ECR through the terminal. If not, you will need to log in. Run the command sudo polly on the terminal. Select the option miscellaneous followed by create secret for docker. Provide the path to the docker config file (the usual path for docker config is /Users/< username >/.docker/config.json in Mac and /home/< username >/.docker/config.json in Linux). Relative paths are not supported. Select the account in which the docker to be run is present. Copy the long text string (secret key) output to the JSON file in the key \u201csecret\u201d. { \"cpu\": 1, \"memory\": \"1Gi\", \"image\": \"docker/whalesay\", \"tag\": \"latest\", \"Secret\": \"ewoAImF1dVnphR0ZzWjNWd2RHRTZSVkJKUXlOcFlXMGsiCgkw==\", \"name\": \"exampleName\", \"command\": [ \"cowsay\",\"hello world\" ] Passing Environment variables: Two types of Environment variables can be passed in the json file. Normal environment variables are saved in a database for future references. These can be passed in the parameter \"env\u201d in the json file. Private environment variables are not saved in any database. These can be used for passing credentials in the json file. These can be passed in the parameter \u201csecret_env\u201d in the json file. Note: The value of Environment variables should always be string. For example, the correct way to assign Environment variable is {\u201cparallel_threads\u201d : \u201c2\u201d} and NOT {\u201cparallel_threads\u201d : 2} . { \"cpu\": \"100m\", \"memory\": \"64Mi\", \"image\": \"your_docker\", \"tag\": \"latest\", \"env\": { \"ENV1\": \"ENV_VALUE1\", \"ENV2\": \"ENV_VALUE2\" }, \"secret_env\": { \"SECRET_ENV1\": \"SECRET_ENV_VALUE1\", \"SECRET_ENV2\": \"SECRET_ENV_VALUE2\" }, \"name\": \"docker running\" } Here is an example gist showing how input data for a job can be taken from and output stored back to Polly Workspaces. Execute Job To execute the job, execute the following command polly jobs submit On executing this command, you will be asked to enter the id of the workspace where the job should be run and the path to the job description JSON file. With this, the job will be submitted to run and Job ID will be created. This Job ID will be needed to check the status and the logs of the submitted job. Note: You do not need to create a new Workspace for running a job. You can simply list the older Workspaces and run a job in an already created Workspace. Figure 9. Submit Jobs Monitor Job status Get job status The following command can be used to view the status of a particular job. polly jobs status --workspace-id --job-id Figure 10. Single Job Status The following command can be used to view the statuses of all the jobs in a workspace. polly jobs status --workspace-id A prompt to enter job id will appear which when kept blank gets all the job statuses in a workspaces. Figure 11. All Job Statuses in a Workspace If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Get job logs To view the logs of any job, use the following command: polly jobs logs --workspace-id --job-id This will give the logs for the job. In case the job is still running, it will give the logs generated till that instant. Figure 12. Job Logs Note: If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Polly CLI help If help is needed for any command, just type --help at the end of the command and execute. Figure 13. Polly CLI Help Some useful gists Accessing Polly files in and out of a job Running a cluster of jobs with different parameters Bash script to identify when a job finishes","title":"Polly CLI"},{"location":"Scaling compute/Polly CLI.html#what-is-polly-cli","text":"Polly CLI (Command Line Interface) is an open source tool that enables you to interact with Polly services using commands in your command-line shell. Polly CLI lets you upload data and run jobs on the Polly cloud infrastructure by scaling computation resources as per need. You can start and stop jobs, monitor them and view logs.","title":"What is Polly CLI?"},{"location":"Scaling compute/Polly CLI.html#required-system-configurations","text":"Polly CLI can work on any Unix based system (Linux and Mac distributions). It does not work on Windows. We will be releasing a Windows version soon. It can be used on local computers as well as cloud instances and servers. There are no specific machine configurations required for Polly CLI. It can work on a system with as low as 512 MB RAM and 1 CPU.","title":"Required System Configurations"},{"location":"Scaling compute/Polly CLI.html#installation","text":"Dependencies Required for Polly CLI The following dependencies are required to be installed before installing Polly CLI: Node and npm : Linux: For installation on Linux, follow the steps mentioned here . Mac: For installation on Mac, follow the steps mentioned here . Commands to install To install Polly CLI, run the following commands on Terminal / Command prompt: Linux: sudo npm install -g @elucidatainc/pollycli Mac: npm install -g @elucidatainc/pollycli Commands to uninstall To uninstall Polly CLI, run the following commands on Terminal / Command prompt: Linux: sudo npm uninstall -g @elucidatainc/pollycli Mac: npm uninstall -g @elucidatainc/pollycli Note: sudo might have to be used before every command while accessing Polly CLI on cloud instance or server.","title":"Installation"},{"location":"Scaling compute/Polly CLI.html#logging-in-and-out-of-polly-cli","text":"Log in Open the terminal on the system and execute the following command to log in. polly login Enter the Polly Username and Password when prompted. Figure 1. Polly Login Once logged in, you will stay logged in the system and won\u2019t need to log in again even if a new terminal is opened or the system is restarted. You will only need to log in again if you manually log out from the system. When you are in the Polly environment, you can log in to Polly CLI without the need to input credentials. We auto detect the Polly permissions or the user. The following are the ways to auto log in to Polly in different Polly environments: Notebook: You are automatically logged in to Polly CLI as the same user as your Polly log in. You can directly start using all the Polly CLI commands as you wish. Polly CLI jobs: If you want to access Polly CLI inside a job, you will have to install Polly CLI in the docker of the job and execute the following command to log in. polly login --auto Log out Execute the following command to log out polly logout Note: Logging out will not stop a running job.","title":"Logging in and out of Polly CLI"},{"location":"Scaling compute/Polly CLI.html#create-and-view-polly-workspaces","text":"What are Polly Workspaces? Polly Workspaces are online workspaces that contain data, analyses, code, logs etc for a specific workspace or experiment. Data is stored and Analysis is performed within a user chosen workspace. More details about workspaces is mentioned here . Creating a new Workspace To create a new workspace, use the following command. polly workspaces create You will be asked to name the Workspace and provide a description. Once the workspace is created, the workspace name and ID will be shown on the screen. This workspace ID will be needed while creating a JSON file for running jobs. Figure 2. Create Workspace View Workspaces To view all the existing Workspaces with access, use the following command. polly workspaces list You will be asked to select which Workspaces to list: all: On selecting this, all the Workspaces will be listed. latest or oldest: On selecting this you will be asked to enter the number of workspaces as shown in the image below. Figure 3. List Workspaces Workspace ID will be required for transferring data and running jobs.","title":"Create and View Polly Workspaces"},{"location":"Scaling compute/Polly CLI.html#data-transfer","text":"Polly CLI can be used to transfer large data to and from Polly Workspaces. Upto 5 TBs of data can be transferred in one go. Listing the files and folders in a Polly Workspace Directory Files and folders within any Polly Workspace can be listed using the following command. polly files list --workspace-id --workspace-path Workspace ID can be obtained by using the command polly workspaces list as explained in the previous section. The path to the workspace directory has to start with polly:// . Eg - The following command will list all the files and folders within the folder ABC in the workspace 1234. polly files list --workspace-id 1234 --workspace-path polly://ABC/ Note: This command only shows files and folders just one layer within the directory mentioned (just like the ls command on terminal). If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Manually sync data to and from Polly Polly CLI can be used to sync the data between a Polly Workspace and a local directory. Data can be synced manually both ways using the following command. polly files sync --workspace-id --source --destination Workspace ID of the workspace where the data is being synced has to be mentioned in the --workspace-id option. Source and destination can be Polly workspace path as well as local path. Workspace path should start with polly:// followed by the directory path in the workspace where the data is to be synced. Here polly:// is the root directory for the mentioned workspace. The following command will sync data from Polly workspace to current local directory. polly files sync --workspace-id 1234 --source polly://directory1/ --destination ./ The following command will sync data from current local directory to Polly Workspace directory. polly files sync --workspace-id 1234 --source ./ --destination polly://directory1/ Note: Only files that have been changed or added new will get transferred using the sync command. The files that remained unchanged after the last sync will not get transferred. This command can only be used for folders or directories (not for individual files). To transfer just a single file to or from Polly, use the copy command mentioned in the next section. If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Copy files to and from Polly Files can be copied to and from a Polly Workspace using the following command. polly files copy --workspace-id --source --destination This command will copy an individual file from source to destination. The transfer can be from or to Polly Workspace depending on the source and destination defined. Workspace path should start with polly:// followed by the directory structure within the Workspace. Note: If the path contains space in the folder names or file names, provide the entire path within double quotes (\u201c\u201c). If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter.","title":"Data Transfer"},{"location":"Scaling compute/Polly CLI.html#docker-management","text":"Polly has its own docker repository where dockers can be managed. The advantage of the Polly Docker Repository is that these dockers can also be stored within the same platform as data, code, and analyses and to access the stored dockers all you need is the Polly login credential. Docker login and logout In order to login/logout of the docker repository you can run the following command: polly dockers login polly dockers logout The output generated from the above command should then be run on the terminal. Note: Add sudo before the command if required according to system settings. Figure 4. Example output generated from above commands Create a Docker Repository Docker repository is a collection of dockers where you can publish and access your docker images. Here you can store one or more versions of your docker image. Every docker can have a tag and if a docker with the same tag is pushed again, it will overwrite the older docker after saving its older version that can still be accessed and used. There can be multiple such repositories for an organization and only the members of your organization with Polly login credentials can assess them. Use the following command to create a docker repository: polly dockers create --name , < docker_repository_name > --description < description of the repository > Figure 5. Docker Repository Generation List docker repositories You can list all the stored docker repositories in your organization by using this command. polly dockers list --all Figure 6. Polly Docker List List docker repositories commits You can list specific repository to view the various commits or tags stored within it through the Polly docker commit list. polly dockers commit-list --name < docker_repository_name > --all Figure 7. Docker Commit List In order to reduce the indecision when there are multiple dockers with the same name and same tag, you can distinguish them by their unique identifier which is assigned to every docker commit. In case the unique identifier is not passed in the command, the latest version of the docker with that tag will be used. You can go back to older commit as well if required by using its tag and unique identity listed in the Polly docker commit list. Polly Dockers Path Typical path of a docker on Polly is docker.polly.elucidata.io/ < organization >/ < dockername >: < tag > Note: You can have two dockers with the same tag as well, in which case, the latest commit for that tag will be called by default if you call that docker. To call the docker with an older commit, commit hash will be required to specify in the path. docker.polly.elucidata.io/ < organization >/ < dockername >@ < commit_hash > The following example contains 2 docker images with the same tag. The latest image has the tag \u201cactive\u201c. The active image can just be called by using the usual path. docker.polly.elucidata.io/elucidata/16June:latest The image with the inactive tag can be called by specifying the commit hash. docker.polly.elucidata.io/elucidata/16June@sha256:5747316366b8cc9e3021cd7286f42b2d6d81e3d743e2ab571f55bcd5df788cc8 Figure 8. Example of Docker with different tag status Pull/push of dockers The commands for docker\u2019s pull/push are the same commands that you use for pulling and pushing from any other docker registry. The only difference is the path of the docker. Example commands to pull a docker would be: docker.polly.elucidata.io/elucidata/16June:latest docker pull docker.polly.elucidata.io/elucidata/16june@sha256:5747316366b8cc9e3021cd7286f42b2d6d81e3d743e2ab571f55bcd5df788cc8","title":"Docker Management"},{"location":"Scaling compute/Polly CLI.html#running-dockerized-jobs","text":"Polly CLI can run dockerized jobs on managed Polly infrastructure. Polly infrastructure will scale computational resources with increased usage. All you need to do is submit a job and rest is taken care of by Polly.","title":"Running Dockerized Jobs"},{"location":"Scaling compute/Polly CLI.html#create-job-description-json-file","text":"JSON file is needed to describe the job to be run on Polly. This file should contain the information about the computational resources (machine), docker image, the name of the job and specific commands (if required) to be run after the docker has been run, as keys. Text can be copy pasted from the example below to create the JSON file. { \"machineType\" : \"gp\", \"cpu\": 1, \"memory\": \"1Gi\", \"image\": \"docker/whalesay\", \"tag\": \"latest\", \"name\": \"Single Cell RNA\", \"command\": [ \"cowsay\",\"hello world\" ] } machineType Name of the machine required to run the job needs to be mentioned as per the following table. machineType No. of vCPUs Memory (RAM) No. of GPUs gp 4 16 GB - ci2xlarge 16 32 GB - ci3xlarge 36 72 GB - mi2xlarge 4 32 GB - mi3xlarge 8 64 GB - mi4xlarge 16 122 GB - gpusmall 16 61 GB 1 More machines (including some with GPUs) will be added soon. If you need a specific machine to be added to the list, please contact us at polly@elucidata.io . If computational power required is less than 2 vCPUs and 8 GB RAM, use the keys \u201ccpu\u201d and \u201cmemory\u201d in the JSON file instead of the key \u201cmachineType\u201d . If all 3 keys are present, \u201cmachineType\u201d takes priority and the machine will be assigned accordingly. In the example JSON (image) mentioned above, machine selected will be \u201cgp\u201d with 4 vCPUs and 16 GB RAM and NOT 1vCPU and 1 GB RAM. cpu: Mention the number of CPUs needed here. For smaller jobs, just a part of the CPU can also be chosen. For example, if 0.1 vCPUs are required for the job, the number of CPUs can be mentioned as \u201c100m\u201d . If more than 2 CPUs are required for the job, use the key \u201cmachineType\u201d to choose the relevant machine instead of \u201ccpu\u201d and \u201cmemory\u201d . memory: RAM required needs to be mentioned in text (eg - \u201c1Gi\u201d or \u201c500 Mi\u201d) in this key. If memory needed is more than 8 GB, use the key \u201cmachineType\u201d to choose the relevant machine instead of \u201ccpu\u201d and \u201cmemory\u201d . image : The path to the docker image present in DockerHub or ECR needs to be mentioned in this key. tag: Tag of the docker image needs to be mentioned in this key. name: Name you want to provide to the job has to be mentioned in this key. command: Any commands to be executed after the docker has been run can be mentioned in this key.","title":"Create job description JSON file"},{"location":"Scaling compute/Polly CLI.html#docker-building-guidelines","text":"While creating a docker to be run on Polly, the following must be taken care of. Dockers must be present in either Docker Hub or Amazon ECR. Soon you will be able to have Dockers directly on Polly. Only self contained dockers can be run on Polly. A self contained docker is one which has the code to get input files as well as upload output files back contained in the docker. Public as well as private dockers are supported. In order to run private dockers, \u201csecret\u201d should be passed as a key in the JSON file. If your private dockers are on Polly itself, you don't require to generate this secret. To get the secret key for the private docker, the following steps need to be followed. For MacOS, you need to remove the key value pair \"credsStore\": \"osxkeychain\" from the config.json file present in the directory /Users/< username >/.docker . You need to be logged in to DockerHub or ECR through the terminal. If not, you will need to log in. Run the command sudo polly on the terminal. Select the option miscellaneous followed by create secret for docker. Provide the path to the docker config file (the usual path for docker config is /Users/< username >/.docker/config.json in Mac and /home/< username >/.docker/config.json in Linux). Relative paths are not supported. Select the account in which the docker to be run is present. Copy the long text string (secret key) output to the JSON file in the key \u201csecret\u201d. { \"cpu\": 1, \"memory\": \"1Gi\", \"image\": \"docker/whalesay\", \"tag\": \"latest\", \"Secret\": \"ewoAImF1dVnphR0ZzWjNWd2RHRTZSVkJKUXlOcFlXMGsiCgkw==\", \"name\": \"exampleName\", \"command\": [ \"cowsay\",\"hello world\" ] Passing Environment variables: Two types of Environment variables can be passed in the json file. Normal environment variables are saved in a database for future references. These can be passed in the parameter \"env\u201d in the json file. Private environment variables are not saved in any database. These can be used for passing credentials in the json file. These can be passed in the parameter \u201csecret_env\u201d in the json file. Note: The value of Environment variables should always be string. For example, the correct way to assign Environment variable is {\u201cparallel_threads\u201d : \u201c2\u201d} and NOT {\u201cparallel_threads\u201d : 2} . { \"cpu\": \"100m\", \"memory\": \"64Mi\", \"image\": \"your_docker\", \"tag\": \"latest\", \"env\": { \"ENV1\": \"ENV_VALUE1\", \"ENV2\": \"ENV_VALUE2\" }, \"secret_env\": { \"SECRET_ENV1\": \"SECRET_ENV_VALUE1\", \"SECRET_ENV2\": \"SECRET_ENV_VALUE2\" }, \"name\": \"docker running\" } Here is an example gist showing how input data for a job can be taken from and output stored back to Polly Workspaces.","title":"Docker Building Guidelines"},{"location":"Scaling compute/Polly CLI.html#execute-job","text":"To execute the job, execute the following command polly jobs submit On executing this command, you will be asked to enter the id of the workspace where the job should be run and the path to the job description JSON file. With this, the job will be submitted to run and Job ID will be created. This Job ID will be needed to check the status and the logs of the submitted job. Note: You do not need to create a new Workspace for running a job. You can simply list the older Workspaces and run a job in an already created Workspace. Figure 9. Submit Jobs","title":"Execute Job"},{"location":"Scaling compute/Polly CLI.html#monitor-job-status","text":"Get job status The following command can be used to view the status of a particular job. polly jobs status --workspace-id --job-id Figure 10. Single Job Status The following command can be used to view the statuses of all the jobs in a workspace. polly jobs status --workspace-id A prompt to enter job id will appear which when kept blank gets all the job statuses in a workspaces. Figure 11. All Job Statuses in a Workspace If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter. Get job logs To view the logs of any job, use the following command: polly jobs logs --workspace-id --job-id This will give the logs for the job. In case the job is still running, it will give the logs generated till that instant. Figure 12. Job Logs Note: If you are within a Polly environment while accessing Polly CLI (like notebook or CLI job), workspace id can be auto-detected by passing --yes or -y in the command instead of --workspace-id parameter.","title":"Monitor Job status"},{"location":"Scaling compute/Polly CLI.html#polly-cli-help","text":"If help is needed for any command, just type --help at the end of the command and execute. Figure 13. Polly CLI Help","title":"Polly CLI help"},{"location":"Scaling compute/Polly CLI.html#some-useful-gists","text":"Accessing Polly files in and out of a job Running a cluster of jobs with different parameters Bash script to identify when a job finishes","title":"Some useful gists"},{"location":"Scaling compute/Polly Notebooks.html","text":"Introduction Polly Notebook is a scalable analytics platform which allows you to perform data analysis remotely in a Jupyter-like notebook. It provides the flexibility to select the compute capacity, the environment according to your need along with the ability to share the analyses with your peers for seamless team collaboration. Polly Notebook provides a Jupyter-like interface on the cloud. Some of the features of Polly Notebooks over other local hosting options are: Ready-to-code platform: Installing and maintaining environments for every notebook can be a frustrating overhead. We provide custom docker environments that come pre-installed with modules commonly used in bioinformatics. You can also add your own custom docker environments. Cloud storage: With Polly Notebooks, you can store your data files and notebooks in a single place that will be ready to run in less than 5 minutes from anywhere in the world. No need to fetch your code from Bitbucket anymore! Share and collaborate on your Workspaces: Polly allows sharing of workspaces so you can review and refer notebooks within your team. Resource management: Most biological analyses (like RNAseq) are commonly resource-intensive, whether in terms of RAM or processing power. In such cases, you either have to scramble for bigger resources or compromise on the speed by using less processing power. Polly makes it possible to scale up your resources at any time. Accessing Polly Notebooks Navigate to the Polly Workspaces in which the analysis needs to be performed. The notebooks of this workspace can accessed from the middle panel. Figure 1. Polly Workspaces You can access Polly Notebooks in three ways: Create a new notebook: Go to the Applications interface and click on the Polly Notebooks button located on the bottom left side of the navigation bar to create a new notebook. You are required to provide a name to the Notebook and select the workspace along with an environment and a machine to run the given notebook. Figure 2. New Notebook button to create a new notebook Upload a notebook: Click on the Cloud Upload icon prsent at the top of the middle panel and select the Upload a Notebook option. Figure 3. Upload Notebook button to upload a notebook The notebook can be uploaded from the local system as well as from the various cloud storage services (Dropbox, Google Drive and Box). To upload from local system, files can be dragged and dropped. To upload from various cloud storage services, select the relevant option, login to the service and select the files to be uploaded. Figure 4. Window to import notebook from local or other cloud storage services Open an existing notebook: Click on the name of any existing notebook to Edit and Launch it. If you are running the notebook for the first time, the option Edit and Launch would appear as a default selection to launch the selected notebook. You are required to select an environment and a machine to run the given notebook, oly after these selections are done you can launch the notebook. Figure 5. Opening a notebook for the first time For an older notebook: You have two options, you can either launch the notebook directly by the Launch button or you can choose to edit it first before launching through the Edit and Launch button. Note: Under the edit option, you can only change the machine type. The docker environment would remain the same as the one selected when you run the notebook for the first time. Figure 6. Opening an old notebook Pre-Configured Environments Polly supports various notebook environments in the form of dockers to cater to the needs of different users. Each of the dockers is built according to various data analytic needs ranging from basic scripting, processing large data or training and testing of ML models. The menu to select the notebook environments will pop-up whenever you create or upload the notebook and opens it for the first time. Figure 7. Menu to select various available environments The various notebook environments supported are as follows: Environment Usage R libraries Python Modules System R General R scripting askpass 1.1 assertthat 0.2.1 backports 1.1.5 base64enc 0.1-3 BH 1.72.0-3 BiocManager 1.30.10 bitops 1.0-6 brew 1.0-6 callr 3.4.2 cli 2.0.2 clipr 0.7.0 clisymbols 1.2.0 colorspace 1.4-1 commonmark 1.7 covr 3.4.0 crayon 1.3.4 crosstalk 1.0.0 curl 4.3 cyclocomp 1.1.0 desc 1.2.0 devtools 2.2.2 diffobj 0.2.3 digest 0.6.25 DT 0.12 ellipsis 0.3.0 evaluate 0.14 fansi 0.4.1 farver 2.0.3 fastmap 1.0.1 foghorn 1.1.4 fs 1.3.1 gargle 0.4.0 ggplot2 3.2.1 gh 1.1.0 git2r 0.26.1 glue 1.3.1 gmailr 1.0.0 gridExtra 2.3 gtable 0.3.0 highlight 0.5.0 highr 0.8 htmltools 0.4.0 htmlwidgets 1.5.1 httpuv 1.5.2 httr 1.4.1 hunspell 3.0 ini 0.3.1 IRdisplay 0.7.0 IRkernel 1.1 jsonlite 1.6.1 knitr 1.28 labeling 0.3 later 1.0.0 lazyeval 0.2.2 leaflet 2.0.3 leaflet.providers 1.9.0 lifecycle 0.1.0 lintr 2.0.1 magrittr 1.5 markdown 1.1 memoise 1.1.0 mime 0.9 mockery 0.4.2 munsell 0.5.0 openssl 1.4.1 parsedate 1.2.0 pbdZMQ 0.3-3 pillar 1.4.3 pingr 2.0.0 pkgbuild 1.0.6 pkgconfig 2.0.3 pkgdown 1.4.1 pkgload 1.0.2 plyr 1.8.6 png 0.1-7 PollyConnector 0.0.0 praise 1.0.0 prettyunits 1.1.1 processx 3.4.2 promises 1.1.0 ps 1.3.2 purrr 0.3.3 R6 2.4.1 rappdirs 0.3.1 raster 3.0-12 rcmdcheck 1.3.3 RColorBrewer 1.1-2 Rcpp 1.0.3 rematch 1.0.1 rematch2 2.1.0 remotes 2.1.1 repr 1.1.0 reshape2 1.4.3 reticulate 1.14 rex 1.1.2 rhub 1.1.1 RJSONIO 1.3-1.4 rlang 0.4.5 rmarkdown 2.1 roxygen2 7.0.2 rprojroot 1.3-2 rstudioapi 0.11 rversions 2.0.1 rvest 0.3.5 scales 1.1.0 selectr 0.4-2 sessioninfo 1.1.1 shiny 1.4.0 sourcetools 0.1.7 sp 1.4-1 spelling 2.1 stringi 1.4.6 stringr 1.4.0 sys 3.3 testthat 2.3.2 tibble 2.1.3 tinytex 0.20 triebeard 0.3.0 urltools 1.7.3 usethis 1.5.1 utf8 1.1.4 uuid 0.1-4 vctrs 0.2.3 viridis 0.5.1 viridisLite 0.3.0 whisker 0.4 whoami 1.3.0 withr 2.1.2 xfun 0.12 xml2 1.2.2 xmlparsedata 1.0.3 xopen 1.0.0 xtable 1.8-4 yaml 2.2.1 None Python 2 General Python 2 scripting None attrs 19.3.0 backports-abc 0.5 backports.functools-lru-cache 1.6.1 backports.shutil-get-terminal-size 1.0.0 bleach 3.1.0 certifi 2019.11.28 chardet 3.0.4 cmapPy 1.0.5 configparser 4.0.2 contextlib2 0.6.0.post1 cycler 0.10.0 decorator 4.4.1 defusedxml 0.6.0 entrypoints 0.3 enum34 1.1.6 funcsigs 1.0.2 functools32 3.2.3.post2 futures 3.3.0 h5py 2.10.0 idna 2.8 importlib-metadata 1.5.0 ipaddress 1.0.23 ipykernel 4.10.1 ipython 5.9.0 ipython-genutils 0.2.0 ipywidgets 7.4.2 Jinja2 2.11.1 jsonschema 3.2.0 jupyter-client 5.3.4 jupyter-core 4.6.2 kiwisolver 1.1.0 MarkupSafe 1.1.1 matplotlib 2.2.4 mistune 0.8.4 nbconvert 5.6.1 nbformat 4.4.0 notebook 5.7.8 numpy 1.16.6 pandas 0.24.1 pandocfilters 1.4.2 pathlib2 2.3.5 pexpect 4.8.0 pickleshare 0.7.5 plotly 3.7.0 prometheus-client 0.7.1 prompt-toolkit 1.0.18 ptyprocess 0.6.0 PubChemPy 1.0.4 Pygments 2.5.2 pyparsing 2.4.6 pyrsistent 0.15.7 python-dateutil 2.8.1 pytz 2019.3 pyzmq 18.1.1 qgrid 1.1.1 requests 2.21.0 retrying 1.3.3 scandir 1.10.0 scikit-learn 0.20.3 scipy 1.2.3 Send2Trash 1.5.0 simplegeneric 0.8.1 singledispatch 3.4.0.3 six 1.14.0 subprocess32 3.5.4 terminado 0.8.3 testpath 0.4.4 tornado 5.1.1 traitlets 4.3.3 urllib3 1.24.3 wcwidth 0.1.8 webencodings 0.5.1 widgetsnbextension 3.4.2 zipp 1.1.0 Python 3 General Python 3 scripting None alembic 1.4.1 async-generator 1.10 attrs 19.3.0 awscli 1.17.12 backcall 0.1.0 bleach 3.1.1 botocore 1.14.12 certifi 2019.11.28 chardet 3.0.4 colorama 0.4.3 cycler 0.10.0 decorator 4.4.2 defusedxml 0.6.0 docutils 0.15.2 entrypoints 0.3 idna 2.8 importlib-metadata 1.5.0 ipykernel 5.1.4 ipython 7.13.0 ipython-genutils 0.2.0 ipywidgets 7.5.1 jedi 0.16.0 Jinja2 2.11.1 jmespath 0.9.5 jsonschema 3.2.0 jupyter-client 6.0.0 jupyter-core 4.6.3 jupyter-dashboards 0.7.0 jupyterhub 0.9.4 kiwisolver 1.1.0 Mako 1.1.2 MarkupSafe 1.1.1 matplotlib 2.2.3 mistune 0.8.4 nbconvert 5.6.1 nbformat 5.0.4 notebook 5.7.2 numpy 1.18.1 pamela 1.0.0 pandas 1.0.1 pandocfilters 1.4.2 parso 0.6.2 pexpect 4.8.0 pickleshare 0.7.5 prometheus-client 0.7.1 prompt-toolkit 3.0.3 ptyprocess 0.6.0 pyasn1 0.4.8 Pygments 2.5.2 pyparsing 2.4.6 pyrsistent 0.15.7 python-dateutil 2.8.1 python-editor 1.0.4 python-oauth2 1.1.1 pytz 2019.3 PyYAML 5.3 pyzmq 19.0.0 qgrid 1.3.0 requests 2.21.0 rsa 3.4.2 s3transfer 0.3.3 Send2Trash 1.5.0 six 1.14.0 SQLAlchemy 1.3.13 terminado 0.8.3 testpath 0.4.4 tornado 5.1.1 traitlets 4.3.3 urllib3 1.24.3 wcwidth 0.1.8 webencodings 0.5.1 widgetsnbextension 3.5.1 zipp 3.1.0 Pollyglot Multiple kernels (R, python and bash) in same notebook/environment All libraries from base R docker Seurat pagoda2 CellRanger SingleR All libraries from base python docker scanPy velocyto scVI (scVI supports pytorch) louvain Barcoded Bulk RNA-seq Alignment and processing of RNA-seq fastq files with barcodes All libraries from R docker limma affy DESeq2 edgeR cqn sva BioMart mygene amritr Boruta fgsea gsva ReactomePA xCell singleR enrichR org.Hs.eg.db org.Mm.eg.db Annotation dbi clusterProfiler \ufeff STARsubread-1.6.4-source gosaamer Fastqc Multiqc Picard Machine Learning in python Training, testing and validation of ML models None All libraries from base python docker h5py keras lightgbm tensorflow xgboost Single Cell Downstream Single Cell Analysis All libraries from base R docker Seurat pagoda2 CellRanger SingleR ExperimentHub All libraries from base python docker scanPy velocyto scVI (scVI supports pytorch) louvain rpy2 anndata2ri Data Exploration R and python for general data analysis All libraries from base R docker All libraries from base python docker RNA-seq Downstream Transcriptomics Analysis All libraries from R docker limma affy DESeq2 edgeR cqn sva BioMart mygene amritr Boruta fgsea gsva ReactomePA xCell singleR enrichR org.Hs.eg.db org.Mm.eg.db Annotation dbi clusterProfiler All libraries from base python docker Metabolomics Metabolomics Analysis CAMERA XCMS limma matrixStats stats stringr ggplot2 plotly ggsci latex2exp dplyr ggrepel SuperExactTest UpSetR PollyCommonR mapGCT MetaboAnalystR KEGGREST Pathview KEGGgraph Morpheus Pca3d X13CMS Phantasus MSnbase MAIT cmapR xMSannotator All libraries from base python docker Computational Machines Available The size of the data varies from few MBs to hundreds of GBs, and in order to process and analyze this huge data, one would need the computation power from a small machine to a large workstation. Polly Notebook supports configurations having 2 to 72 GB Ram and 1 to 36 CPU cores. The menu to select a machine configuration will pop-up when you creates a new notebook or uploads a notebook and tries to open it for the first time. Figure 8. Menu to select various machine configurations Most of the machine configuration are already specified to cover the wide variety of use cases. More machine configuration can also be made available on request (contact us at polly@elucidata.io ). The general machine configurations are divided into three broad categories: General purpose: Configurations from 1 to 4 CPU cores and 2 to 16 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly small 1 2 GB Polly medium 2 4 GB Polly large 2 8 GB Polly x-large 4 16 GB Compute Intensive: Configurations from 16 to 36 CPU cores and 32 to 72 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 16 32 GB Polly 3x-large 36 72 GB Memory-Optimized: Configurations from 4 to 8 CPU cores and 32 to 64 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 4 32 GB Polly 3x-large 8 64 GB Polly 4x-large 16 120GB Other Useful Features There are few other useful features as well that might come handy when using a Polly Notebook. Click on the kebab menu at the end of the selected notebook. A menu with various options will open. Figure 9. Edit button to change machine configuration Scroll down in the menu and navigate to the desired option. Rename : You can rename the file using this option. Provide the new name to the file and click on Rename to confirm your changes. Edit : Polly gives the flexibility to change the machine configuration to allow the usage of the notebook according to the computing power required at each step. You can change the configuration according to the need at each step. A menu with the different machine configuration will open, with the various options available will be displayed under the Select Machine Type segment. Select the appropriate option to change the configuration. Download : You can select this option to download the selected notebook on your system. Delete : You can use this option to delete the selected notebook. Getting started with Polly Notebook Upon selecting a pre-configured docker environment and a computational machine, a Polly Notebook starts launching on a new tab of the browser. Based upon the type of computational machine chosen while launching a Polly Notebook you will see a progress bar which will tell you that your new notebook is opening. Figure 10. Progress bar upon launching a Polly Notebook Once the server is ready, you will see the new notebook gets opened on the browser. The interface is very similar to that of a Jupyter notebook. Figure 11. Polly Notebook interface On the top left, you can see a pre-defined name given to the notebook if in case a new notebook was created. Towards the top right, you can see the Polly Workspace name and below it, you can see the kernel/docker environment selected for opening the notebook. Menu bar: There are multiple tabs present in the menu bar section which can be used to operate various functions in the notebook. For example, under the File tab, you can select the Rename option to change the name of the current active notebook. Toolbar: It contains multiple icons that allow you to perform various operations that are frequently used. Structure of Polly Notebook The Polly notebook comprises of a sequence of cells. There are three types of cells: markdown cells , raw cells , and code cells . In each of these types, you can input multi-line content and each cell can be executed by pressing Shift+Enter , or by clicking either the Run cells option on Cell tab in the menu bar or the \u201cPlay\u201d button in the toolbar. Figure 12. Structure of a Polly Notebook Markdown cells You can record the computational process in a proficient manner using rich text . The Markdown language allows you to define a structure to the notebook by using markdown headings. It gives a basic method to play out text markup, that is, to determine which parts of the text should be stressed (italics), bold, form lists, etc. Raw cells You can write output directly in the raw cells . A raw cell is not evaluated by a notebook meaning anything written in the raw cell goes to the output when that cell is executed. Code cells A code cell allows you to edit and write a new code. The code cell executes the code written by you based on the kernel selected while launching the notebook. The code cell can include multiple programming languages as well as seen on the bottom right side of the image above. The above example is of a Pollyglot Docker environment which allows you to select multiple programming languages in the same notebook thus, you can select the type of kernel you prefer to code on. Once the code cell is executed, the results which are computed by sending the code to the kernel are displayed as an output below the cell. Again to execute a code cell , you can click on the \u201cRun\u201d button and if you want to stop the computation process of a particular code cell , then the \u201cInterrupt\u201d button needs to be selected in the toolbar. Figure 13. Running a code cell Polly Offerings Polly Offerings tab in the Menu bar contains the following two options, namely Terminal and File Explorer which are described below. Figure 14. Polly Offerings tab Terminal Once the Terminal option is selected, it launches a new tab on the browser and provides access to the command-line interface to execute any sets of commands. You have access to all the file types which are available in the docker environment and those can be managed through the terminal as well. The terminal option also allows you to install Python or R packages (as described later), managing system binaries and system configurations, and helps you working with code repositories hosted on GitHub, Bitbucket, etc. Figure 15. Terminal screen window File Explorer Similar to the above option, if you select the File Explorer option, a new tab opens up in the browser and you can view different file types and directories present in the docker environment. Under the Files tab, the list of all the files and directories is available to you and any modification such as delete, upload or modifying by opening a file type can be done. Figure 16. File Explorer window Additionally, you can also launch a new notebook by selecting the New button present on the top right corner of the page in File Explorer . The new notebook will open in a new tab and would automatically be made available in the Notebook section of the same Polly Workspace of the original notebook. Figure 17. Launching a new notebook using File Explorer File Explorer window also allows you to view, edit or create various file types in an interactive manner. The Text File option in the New button can be used to create a new text file. For viewing or editing a file, you can click on the file and a text editor will open in a new tab of the browser. You can view or edit the file and save the changes made in the file. The text editor also allows you to select a programming language from the Language tab to edit and convert the file format. Figure 18. Opening a file using a Text editor Accessing Workspace files in Notebook Accessing individual files using Python and R functions For carrying on analysis, if you require any input files which are available in Polly Workspaces, those files can be fetched using a set of commands. You can list all the files present in the Workspace and then select the individual file by the following command: ## Lists all the files present in the project list_project_file() ## The file will be downloaded in the current working directory download_project_file('sample_file.csv') After finishing the analysis, you can push back the newly generated output files again to the Workspace using the following command ## Save the file to the project save_file_to_project('sample_file.csv') Figure 19. Accessing individual files in a notebook Note: These functions cannot access files within folders in workspace. To access those files, use CLI commands. Accessing files and directories using CLI commands The contents of any directory within a Workspace can be listed using the following command on a notebook terminal or a bash cell. polly files list --workspace-path \" \" -y Here, the path of the directory has to start with \u201cpolly://\u201d. To view the contents within a folder called \u201cData\u201d in the workspace, the following command will have to be executed on the notebook terminal. polly files list --workspace-path \"polly://Data\" -y To access the directory in the notebook, the following command will have to be executed on the notebook terminal or a bash cell. polly files sync -s \" \" -d \" \" -y Here, -s refers to source and -d refers to destination. If the folder called \u201cData\u201d is to be accessed from Workspace in the notebook folder called \u201cInput\u201d, execute the following command. polly files sync -s \"polly://Data\" -d \"Input\" -y To save notebook directories back to the Workspace, keep the source as notebook directory and destination as Polly Workspace in the same command as mentioned above. polly files sync -s \" \" -d \" \" -y To save the folder called \u201cOutput\u201d back to Polly Workspace, use the following command. polly files sync -s \"Output\" -d \"polly://\" -y Similarly, if an individual file needs to be accessed in a notebook, use the following command polly files copy -s \"\" -d \"\" -y Here, -s refers to source and -d refers to destination. If the file called \u201cInput1.csv\u201d is to be accessed from Workspace folder \u201cData\u201d in the notebook folder called \u201cInput\u201d, execute the following command. polly files copy -s \"polly://Data/Input1\" -d \"Input/Input1.csv\" -y An individual file can be saved back to workspace by interchanging source and destination in the mentioned command. polly files copy -s \"Input/Input1.csv\" -d \"polly://Data/Input1\" -y Installing Packages Although most of the required packages and tools can be made available to you via the customized docker environment, sometimes you might require to install new packages to carry on the analysis. For installing the packages, you can choose two options based on their convenience, you can do it on the Notebook itself or via the terminal. Installing packages and system binaries using the Notebook cell You can install the required packages and system binaries by running the usual installation codes on the code cell of a notebook. For Python packages: You can run the following command in the code cell with Python kernel selected to install the required packages. # for installing packages DON'T forget to use sudo. It will not ask for password. !sudo pip install For R packages: You can run the following command in the code cell with R kernel selected to install the required packages. # for installing packages DON'T forget to use sudo. It will not ask for password. ## Installing CRAN packages !sudo R -e 'install.packages(c(\"package-name\"), repos=\"https://cloud.r-project.org/\")' ## Installing Bioconductor packages !sudo R -e 'BiocManager::install(c(\"package-name\"), update = TRUE, ask = FALSE)' # If error finding BiocManager then install it first using the following command and re-run the above command. !sudo R -e 'install.packages(c(\"BiocManager\"), repos=\"https://cloud.r-project.org/\")' Figure 20. Installing R and Python packages For System binaries: You can also install the system binaries by running the following command in the code cell selecting the bash kernel. # System binaries sudo apt install # If the above command outputs package not found, You can run this command to update the system package indices sudo apt-get update Figure 21. Installing System binaries using the Notebook code cell Installing packages and system binaries via Terminal Another option is also available to install various packages and system binaries using the terminal. You can access the terminal as described in the document above. The commands for installation are almost similar to commands used while installing using a notebook code cell . For Python packages: You can run the following command directly on the terminal to install the required packages. Once the package installation is successful, you can import the package in your notebook. # for installing packages DON'T forget to use sudo. It will not ask for password. > sudo pip install Figure 22. Installing Python packages using the Terminal For R packages: You are required to go to the terminal and open the R Kernel using \u201csudo R\u201d and then install the required R packages. Once the package installation is successful, you can import the library in your notebook R kernel as usual. ## You can install R package by opening R terminal > sudo R ## Install CRAN packages using the following command > install.packages(c('pkg-name'), dependencies=TRUE, repos= ) # For cran mirror link: You can use either of your choice or this one : \"https://cran.cnr.berkeley.edu/\" ## Install Bioconductor packages using the following command > BiocManager::install(c(\"pkg-name\"), update = TRUE, ask = FALSE) # If error finding BiocManager then install it first using the following command and re-run the above command. > install.packages(\"BiocManager\") Figure 23. Installing R packages using the Terminal For System binaries: You can also install the system binaries by running the following command directly on the terminal itself. # System binaries > sudo apt install # If the above command outputs package not found, You can run this command to update the system package indices > sudo apt-get update Figure 24. Installing System libraries using the Terminal Reusable Scripts Polly Notebook also allows you to make use of the reusable scripts which are already made available to you in every notebook. The reusable scripts consist of the snippet codes which are required frequently to perform any analysis. The scripts can include data reading, normalization, visualization generic functions/codes and can be added to the notebook code cell with just a single click and executed as usual. The reusable scripts can be found on the left side as a collapsible dialogue box and you can choose the scripts at any time while performing the analysis. Figure 25. Reusable scripts on Polly Notebook On the right side, another collapsible dialogue box gets opened when you select any reusable script which provides information about the options and usage of that particular reusable script. You can also add your own reusable scripts on the Polly Notebook so as to make use of them in your repeated analysis and save time. Figure 26. Options and Information of Reusable scripts Videos","title":"Notebooks"},{"location":"Scaling compute/Polly Notebooks.html#introduction","text":"Polly Notebook is a scalable analytics platform which allows you to perform data analysis remotely in a Jupyter-like notebook. It provides the flexibility to select the compute capacity, the environment according to your need along with the ability to share the analyses with your peers for seamless team collaboration. Polly Notebook provides a Jupyter-like interface on the cloud. Some of the features of Polly Notebooks over other local hosting options are: Ready-to-code platform: Installing and maintaining environments for every notebook can be a frustrating overhead. We provide custom docker environments that come pre-installed with modules commonly used in bioinformatics. You can also add your own custom docker environments. Cloud storage: With Polly Notebooks, you can store your data files and notebooks in a single place that will be ready to run in less than 5 minutes from anywhere in the world. No need to fetch your code from Bitbucket anymore! Share and collaborate on your Workspaces: Polly allows sharing of workspaces so you can review and refer notebooks within your team. Resource management: Most biological analyses (like RNAseq) are commonly resource-intensive, whether in terms of RAM or processing power. In such cases, you either have to scramble for bigger resources or compromise on the speed by using less processing power. Polly makes it possible to scale up your resources at any time. Accessing Polly Notebooks Navigate to the Polly Workspaces in which the analysis needs to be performed. The notebooks of this workspace can accessed from the middle panel. Figure 1. Polly Workspaces You can access Polly Notebooks in three ways: Create a new notebook: Go to the Applications interface and click on the Polly Notebooks button located on the bottom left side of the navigation bar to create a new notebook. You are required to provide a name to the Notebook and select the workspace along with an environment and a machine to run the given notebook. Figure 2. New Notebook button to create a new notebook Upload a notebook: Click on the Cloud Upload icon prsent at the top of the middle panel and select the Upload a Notebook option. Figure 3. Upload Notebook button to upload a notebook The notebook can be uploaded from the local system as well as from the various cloud storage services (Dropbox, Google Drive and Box). To upload from local system, files can be dragged and dropped. To upload from various cloud storage services, select the relevant option, login to the service and select the files to be uploaded. Figure 4. Window to import notebook from local or other cloud storage services Open an existing notebook: Click on the name of any existing notebook to Edit and Launch it. If you are running the notebook for the first time, the option Edit and Launch would appear as a default selection to launch the selected notebook. You are required to select an environment and a machine to run the given notebook, oly after these selections are done you can launch the notebook. Figure 5. Opening a notebook for the first time For an older notebook: You have two options, you can either launch the notebook directly by the Launch button or you can choose to edit it first before launching through the Edit and Launch button. Note: Under the edit option, you can only change the machine type. The docker environment would remain the same as the one selected when you run the notebook for the first time. Figure 6. Opening an old notebook","title":"Introduction"},{"location":"Scaling compute/Polly Notebooks.html#pre-configured-environments","text":"Polly supports various notebook environments in the form of dockers to cater to the needs of different users. Each of the dockers is built according to various data analytic needs ranging from basic scripting, processing large data or training and testing of ML models. The menu to select the notebook environments will pop-up whenever you create or upload the notebook and opens it for the first time. Figure 7. Menu to select various available environments The various notebook environments supported are as follows: Environment Usage R libraries Python Modules System R General R scripting askpass 1.1 assertthat 0.2.1 backports 1.1.5 base64enc 0.1-3 BH 1.72.0-3 BiocManager 1.30.10 bitops 1.0-6 brew 1.0-6 callr 3.4.2 cli 2.0.2 clipr 0.7.0 clisymbols 1.2.0 colorspace 1.4-1 commonmark 1.7 covr 3.4.0 crayon 1.3.4 crosstalk 1.0.0 curl 4.3 cyclocomp 1.1.0 desc 1.2.0 devtools 2.2.2 diffobj 0.2.3 digest 0.6.25 DT 0.12 ellipsis 0.3.0 evaluate 0.14 fansi 0.4.1 farver 2.0.3 fastmap 1.0.1 foghorn 1.1.4 fs 1.3.1 gargle 0.4.0 ggplot2 3.2.1 gh 1.1.0 git2r 0.26.1 glue 1.3.1 gmailr 1.0.0 gridExtra 2.3 gtable 0.3.0 highlight 0.5.0 highr 0.8 htmltools 0.4.0 htmlwidgets 1.5.1 httpuv 1.5.2 httr 1.4.1 hunspell 3.0 ini 0.3.1 IRdisplay 0.7.0 IRkernel 1.1 jsonlite 1.6.1 knitr 1.28 labeling 0.3 later 1.0.0 lazyeval 0.2.2 leaflet 2.0.3 leaflet.providers 1.9.0 lifecycle 0.1.0 lintr 2.0.1 magrittr 1.5 markdown 1.1 memoise 1.1.0 mime 0.9 mockery 0.4.2 munsell 0.5.0 openssl 1.4.1 parsedate 1.2.0 pbdZMQ 0.3-3 pillar 1.4.3 pingr 2.0.0 pkgbuild 1.0.6 pkgconfig 2.0.3 pkgdown 1.4.1 pkgload 1.0.2 plyr 1.8.6 png 0.1-7 PollyConnector 0.0.0 praise 1.0.0 prettyunits 1.1.1 processx 3.4.2 promises 1.1.0 ps 1.3.2 purrr 0.3.3 R6 2.4.1 rappdirs 0.3.1 raster 3.0-12 rcmdcheck 1.3.3 RColorBrewer 1.1-2 Rcpp 1.0.3 rematch 1.0.1 rematch2 2.1.0 remotes 2.1.1 repr 1.1.0 reshape2 1.4.3 reticulate 1.14 rex 1.1.2 rhub 1.1.1 RJSONIO 1.3-1.4 rlang 0.4.5 rmarkdown 2.1 roxygen2 7.0.2 rprojroot 1.3-2 rstudioapi 0.11 rversions 2.0.1 rvest 0.3.5 scales 1.1.0 selectr 0.4-2 sessioninfo 1.1.1 shiny 1.4.0 sourcetools 0.1.7 sp 1.4-1 spelling 2.1 stringi 1.4.6 stringr 1.4.0 sys 3.3 testthat 2.3.2 tibble 2.1.3 tinytex 0.20 triebeard 0.3.0 urltools 1.7.3 usethis 1.5.1 utf8 1.1.4 uuid 0.1-4 vctrs 0.2.3 viridis 0.5.1 viridisLite 0.3.0 whisker 0.4 whoami 1.3.0 withr 2.1.2 xfun 0.12 xml2 1.2.2 xmlparsedata 1.0.3 xopen 1.0.0 xtable 1.8-4 yaml 2.2.1 None Python 2 General Python 2 scripting None attrs 19.3.0 backports-abc 0.5 backports.functools-lru-cache 1.6.1 backports.shutil-get-terminal-size 1.0.0 bleach 3.1.0 certifi 2019.11.28 chardet 3.0.4 cmapPy 1.0.5 configparser 4.0.2 contextlib2 0.6.0.post1 cycler 0.10.0 decorator 4.4.1 defusedxml 0.6.0 entrypoints 0.3 enum34 1.1.6 funcsigs 1.0.2 functools32 3.2.3.post2 futures 3.3.0 h5py 2.10.0 idna 2.8 importlib-metadata 1.5.0 ipaddress 1.0.23 ipykernel 4.10.1 ipython 5.9.0 ipython-genutils 0.2.0 ipywidgets 7.4.2 Jinja2 2.11.1 jsonschema 3.2.0 jupyter-client 5.3.4 jupyter-core 4.6.2 kiwisolver 1.1.0 MarkupSafe 1.1.1 matplotlib 2.2.4 mistune 0.8.4 nbconvert 5.6.1 nbformat 4.4.0 notebook 5.7.8 numpy 1.16.6 pandas 0.24.1 pandocfilters 1.4.2 pathlib2 2.3.5 pexpect 4.8.0 pickleshare 0.7.5 plotly 3.7.0 prometheus-client 0.7.1 prompt-toolkit 1.0.18 ptyprocess 0.6.0 PubChemPy 1.0.4 Pygments 2.5.2 pyparsing 2.4.6 pyrsistent 0.15.7 python-dateutil 2.8.1 pytz 2019.3 pyzmq 18.1.1 qgrid 1.1.1 requests 2.21.0 retrying 1.3.3 scandir 1.10.0 scikit-learn 0.20.3 scipy 1.2.3 Send2Trash 1.5.0 simplegeneric 0.8.1 singledispatch 3.4.0.3 six 1.14.0 subprocess32 3.5.4 terminado 0.8.3 testpath 0.4.4 tornado 5.1.1 traitlets 4.3.3 urllib3 1.24.3 wcwidth 0.1.8 webencodings 0.5.1 widgetsnbextension 3.4.2 zipp 1.1.0 Python 3 General Python 3 scripting None alembic 1.4.1 async-generator 1.10 attrs 19.3.0 awscli 1.17.12 backcall 0.1.0 bleach 3.1.1 botocore 1.14.12 certifi 2019.11.28 chardet 3.0.4 colorama 0.4.3 cycler 0.10.0 decorator 4.4.2 defusedxml 0.6.0 docutils 0.15.2 entrypoints 0.3 idna 2.8 importlib-metadata 1.5.0 ipykernel 5.1.4 ipython 7.13.0 ipython-genutils 0.2.0 ipywidgets 7.5.1 jedi 0.16.0 Jinja2 2.11.1 jmespath 0.9.5 jsonschema 3.2.0 jupyter-client 6.0.0 jupyter-core 4.6.3 jupyter-dashboards 0.7.0 jupyterhub 0.9.4 kiwisolver 1.1.0 Mako 1.1.2 MarkupSafe 1.1.1 matplotlib 2.2.3 mistune 0.8.4 nbconvert 5.6.1 nbformat 5.0.4 notebook 5.7.2 numpy 1.18.1 pamela 1.0.0 pandas 1.0.1 pandocfilters 1.4.2 parso 0.6.2 pexpect 4.8.0 pickleshare 0.7.5 prometheus-client 0.7.1 prompt-toolkit 3.0.3 ptyprocess 0.6.0 pyasn1 0.4.8 Pygments 2.5.2 pyparsing 2.4.6 pyrsistent 0.15.7 python-dateutil 2.8.1 python-editor 1.0.4 python-oauth2 1.1.1 pytz 2019.3 PyYAML 5.3 pyzmq 19.0.0 qgrid 1.3.0 requests 2.21.0 rsa 3.4.2 s3transfer 0.3.3 Send2Trash 1.5.0 six 1.14.0 SQLAlchemy 1.3.13 terminado 0.8.3 testpath 0.4.4 tornado 5.1.1 traitlets 4.3.3 urllib3 1.24.3 wcwidth 0.1.8 webencodings 0.5.1 widgetsnbextension 3.5.1 zipp 3.1.0 Pollyglot Multiple kernels (R, python and bash) in same notebook/environment All libraries from base R docker Seurat pagoda2 CellRanger SingleR All libraries from base python docker scanPy velocyto scVI (scVI supports pytorch) louvain Barcoded Bulk RNA-seq Alignment and processing of RNA-seq fastq files with barcodes All libraries from R docker limma affy DESeq2 edgeR cqn sva BioMart mygene amritr Boruta fgsea gsva ReactomePA xCell singleR enrichR org.Hs.eg.db org.Mm.eg.db Annotation dbi clusterProfiler \ufeff STARsubread-1.6.4-source gosaamer Fastqc Multiqc Picard Machine Learning in python Training, testing and validation of ML models None All libraries from base python docker h5py keras lightgbm tensorflow xgboost Single Cell Downstream Single Cell Analysis All libraries from base R docker Seurat pagoda2 CellRanger SingleR ExperimentHub All libraries from base python docker scanPy velocyto scVI (scVI supports pytorch) louvain rpy2 anndata2ri Data Exploration R and python for general data analysis All libraries from base R docker All libraries from base python docker RNA-seq Downstream Transcriptomics Analysis All libraries from R docker limma affy DESeq2 edgeR cqn sva BioMart mygene amritr Boruta fgsea gsva ReactomePA xCell singleR enrichR org.Hs.eg.db org.Mm.eg.db Annotation dbi clusterProfiler All libraries from base python docker Metabolomics Metabolomics Analysis CAMERA XCMS limma matrixStats stats stringr ggplot2 plotly ggsci latex2exp dplyr ggrepel SuperExactTest UpSetR PollyCommonR mapGCT MetaboAnalystR KEGGREST Pathview KEGGgraph Morpheus Pca3d X13CMS Phantasus MSnbase MAIT cmapR xMSannotator All libraries from base python docker","title":"Pre-Configured Environments"},{"location":"Scaling compute/Polly Notebooks.html#computational-machines-available","text":"The size of the data varies from few MBs to hundreds of GBs, and in order to process and analyze this huge data, one would need the computation power from a small machine to a large workstation. Polly Notebook supports configurations having 2 to 72 GB Ram and 1 to 36 CPU cores. The menu to select a machine configuration will pop-up when you creates a new notebook or uploads a notebook and tries to open it for the first time. Figure 8. Menu to select various machine configurations Most of the machine configuration are already specified to cover the wide variety of use cases. More machine configuration can also be made available on request (contact us at polly@elucidata.io ). The general machine configurations are divided into three broad categories: General purpose: Configurations from 1 to 4 CPU cores and 2 to 16 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly small 1 2 GB Polly medium 2 4 GB Polly large 2 8 GB Polly x-large 4 16 GB Compute Intensive: Configurations from 16 to 36 CPU cores and 32 to 72 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 16 32 GB Polly 3x-large 36 72 GB Memory-Optimized: Configurations from 4 to 8 CPU cores and 32 to 64 GB RAM fall under this category. The various configurations are: Name CPU/Cores RAM Polly 2x-large 4 32 GB Polly 3x-large 8 64 GB Polly 4x-large 16 120GB","title":"Computational Machines Available"},{"location":"Scaling compute/Polly Notebooks.html#other-useful-features","text":"There are few other useful features as well that might come handy when using a Polly Notebook. Click on the kebab menu at the end of the selected notebook. A menu with various options will open. Figure 9. Edit button to change machine configuration Scroll down in the menu and navigate to the desired option. Rename : You can rename the file using this option. Provide the new name to the file and click on Rename to confirm your changes. Edit : Polly gives the flexibility to change the machine configuration to allow the usage of the notebook according to the computing power required at each step. You can change the configuration according to the need at each step. A menu with the different machine configuration will open, with the various options available will be displayed under the Select Machine Type segment. Select the appropriate option to change the configuration. Download : You can select this option to download the selected notebook on your system. Delete : You can use this option to delete the selected notebook.","title":"Other Useful Features"},{"location":"Scaling compute/Polly Notebooks.html#getting-started-with-polly-notebook","text":"Upon selecting a pre-configured docker environment and a computational machine, a Polly Notebook starts launching on a new tab of the browser. Based upon the type of computational machine chosen while launching a Polly Notebook you will see a progress bar which will tell you that your new notebook is opening. Figure 10. Progress bar upon launching a Polly Notebook Once the server is ready, you will see the new notebook gets opened on the browser. The interface is very similar to that of a Jupyter notebook. Figure 11. Polly Notebook interface On the top left, you can see a pre-defined name given to the notebook if in case a new notebook was created. Towards the top right, you can see the Polly Workspace name and below it, you can see the kernel/docker environment selected for opening the notebook. Menu bar: There are multiple tabs present in the menu bar section which can be used to operate various functions in the notebook. For example, under the File tab, you can select the Rename option to change the name of the current active notebook. Toolbar: It contains multiple icons that allow you to perform various operations that are frequently used.","title":"Getting started with Polly Notebook"},{"location":"Scaling compute/Polly Notebooks.html#structure-of-polly-notebook","text":"The Polly notebook comprises of a sequence of cells. There are three types of cells: markdown cells , raw cells , and code cells . In each of these types, you can input multi-line content and each cell can be executed by pressing Shift+Enter , or by clicking either the Run cells option on Cell tab in the menu bar or the \u201cPlay\u201d button in the toolbar. Figure 12. Structure of a Polly Notebook Markdown cells You can record the computational process in a proficient manner using rich text . The Markdown language allows you to define a structure to the notebook by using markdown headings. It gives a basic method to play out text markup, that is, to determine which parts of the text should be stressed (italics), bold, form lists, etc. Raw cells You can write output directly in the raw cells . A raw cell is not evaluated by a notebook meaning anything written in the raw cell goes to the output when that cell is executed. Code cells A code cell allows you to edit and write a new code. The code cell executes the code written by you based on the kernel selected while launching the notebook. The code cell can include multiple programming languages as well as seen on the bottom right side of the image above. The above example is of a Pollyglot Docker environment which allows you to select multiple programming languages in the same notebook thus, you can select the type of kernel you prefer to code on. Once the code cell is executed, the results which are computed by sending the code to the kernel are displayed as an output below the cell. Again to execute a code cell , you can click on the \u201cRun\u201d button and if you want to stop the computation process of a particular code cell , then the \u201cInterrupt\u201d button needs to be selected in the toolbar. Figure 13. Running a code cell","title":"Structure of Polly Notebook"},{"location":"Scaling compute/Polly Notebooks.html#polly-offerings","text":"Polly Offerings tab in the Menu bar contains the following two options, namely Terminal and File Explorer which are described below. Figure 14. Polly Offerings tab Terminal Once the Terminal option is selected, it launches a new tab on the browser and provides access to the command-line interface to execute any sets of commands. You have access to all the file types which are available in the docker environment and those can be managed through the terminal as well. The terminal option also allows you to install Python or R packages (as described later), managing system binaries and system configurations, and helps you working with code repositories hosted on GitHub, Bitbucket, etc. Figure 15. Terminal screen window File Explorer Similar to the above option, if you select the File Explorer option, a new tab opens up in the browser and you can view different file types and directories present in the docker environment. Under the Files tab, the list of all the files and directories is available to you and any modification such as delete, upload or modifying by opening a file type can be done. Figure 16. File Explorer window Additionally, you can also launch a new notebook by selecting the New button present on the top right corner of the page in File Explorer . The new notebook will open in a new tab and would automatically be made available in the Notebook section of the same Polly Workspace of the original notebook. Figure 17. Launching a new notebook using File Explorer File Explorer window also allows you to view, edit or create various file types in an interactive manner. The Text File option in the New button can be used to create a new text file. For viewing or editing a file, you can click on the file and a text editor will open in a new tab of the browser. You can view or edit the file and save the changes made in the file. The text editor also allows you to select a programming language from the Language tab to edit and convert the file format. Figure 18. Opening a file using a Text editor","title":"Polly Offerings"},{"location":"Scaling compute/Polly Notebooks.html#accessing-workspace-files-in-notebook","text":"Accessing individual files using Python and R functions For carrying on analysis, if you require any input files which are available in Polly Workspaces, those files can be fetched using a set of commands. You can list all the files present in the Workspace and then select the individual file by the following command: ## Lists all the files present in the project list_project_file() ## The file will be downloaded in the current working directory download_project_file('sample_file.csv') After finishing the analysis, you can push back the newly generated output files again to the Workspace using the following command ## Save the file to the project save_file_to_project('sample_file.csv') Figure 19. Accessing individual files in a notebook Note: These functions cannot access files within folders in workspace. To access those files, use CLI commands. Accessing files and directories using CLI commands The contents of any directory within a Workspace can be listed using the following command on a notebook terminal or a bash cell. polly files list --workspace-path \" \" -y Here, the path of the directory has to start with \u201cpolly://\u201d. To view the contents within a folder called \u201cData\u201d in the workspace, the following command will have to be executed on the notebook terminal. polly files list --workspace-path \"polly://Data\" -y To access the directory in the notebook, the following command will have to be executed on the notebook terminal or a bash cell. polly files sync -s \" \" -d \" \" -y Here, -s refers to source and -d refers to destination. If the folder called \u201cData\u201d is to be accessed from Workspace in the notebook folder called \u201cInput\u201d, execute the following command. polly files sync -s \"polly://Data\" -d \"Input\" -y To save notebook directories back to the Workspace, keep the source as notebook directory and destination as Polly Workspace in the same command as mentioned above. polly files sync -s \" \" -d \" \" -y To save the folder called \u201cOutput\u201d back to Polly Workspace, use the following command. polly files sync -s \"Output\" -d \"polly://\" -y Similarly, if an individual file needs to be accessed in a notebook, use the following command polly files copy -s \"\" -d \"\" -y Here, -s refers to source and -d refers to destination. If the file called \u201cInput1.csv\u201d is to be accessed from Workspace folder \u201cData\u201d in the notebook folder called \u201cInput\u201d, execute the following command. polly files copy -s \"polly://Data/Input1\" -d \"Input/Input1.csv\" -y An individual file can be saved back to workspace by interchanging source and destination in the mentioned command. polly files copy -s \"Input/Input1.csv\" -d \"polly://Data/Input1\" -y","title":"Accessing Workspace files in Notebook"},{"location":"Scaling compute/Polly Notebooks.html#installing-packages","text":"Although most of the required packages and tools can be made available to you via the customized docker environment, sometimes you might require to install new packages to carry on the analysis. For installing the packages, you can choose two options based on their convenience, you can do it on the Notebook itself or via the terminal. Installing packages and system binaries using the Notebook cell You can install the required packages and system binaries by running the usual installation codes on the code cell of a notebook. For Python packages: You can run the following command in the code cell with Python kernel selected to install the required packages. # for installing packages DON'T forget to use sudo. It will not ask for password. !sudo pip install For R packages: You can run the following command in the code cell with R kernel selected to install the required packages. # for installing packages DON'T forget to use sudo. It will not ask for password. ## Installing CRAN packages !sudo R -e 'install.packages(c(\"package-name\"), repos=\"https://cloud.r-project.org/\")' ## Installing Bioconductor packages !sudo R -e 'BiocManager::install(c(\"package-name\"), update = TRUE, ask = FALSE)' # If error finding BiocManager then install it first using the following command and re-run the above command. !sudo R -e 'install.packages(c(\"BiocManager\"), repos=\"https://cloud.r-project.org/\")' Figure 20. Installing R and Python packages For System binaries: You can also install the system binaries by running the following command in the code cell selecting the bash kernel. # System binaries sudo apt install # If the above command outputs package not found, You can run this command to update the system package indices sudo apt-get update Figure 21. Installing System binaries using the Notebook code cell Installing packages and system binaries via Terminal Another option is also available to install various packages and system binaries using the terminal. You can access the terminal as described in the document above. The commands for installation are almost similar to commands used while installing using a notebook code cell . For Python packages: You can run the following command directly on the terminal to install the required packages. Once the package installation is successful, you can import the package in your notebook. # for installing packages DON'T forget to use sudo. It will not ask for password. > sudo pip install Figure 22. Installing Python packages using the Terminal For R packages: You are required to go to the terminal and open the R Kernel using \u201csudo R\u201d and then install the required R packages. Once the package installation is successful, you can import the library in your notebook R kernel as usual. ## You can install R package by opening R terminal > sudo R ## Install CRAN packages using the following command > install.packages(c('pkg-name'), dependencies=TRUE, repos= ) # For cran mirror link: You can use either of your choice or this one : \"https://cran.cnr.berkeley.edu/\" ## Install Bioconductor packages using the following command > BiocManager::install(c(\"pkg-name\"), update = TRUE, ask = FALSE) # If error finding BiocManager then install it first using the following command and re-run the above command. > install.packages(\"BiocManager\") Figure 23. Installing R packages using the Terminal For System binaries: You can also install the system binaries by running the following command directly on the terminal itself. # System binaries > sudo apt install # If the above command outputs package not found, You can run this command to update the system package indices > sudo apt-get update Figure 24. Installing System libraries using the Terminal","title":"Installing Packages"},{"location":"Scaling compute/Polly Notebooks.html#reusable-scripts","text":"Polly Notebook also allows you to make use of the reusable scripts which are already made available to you in every notebook. The reusable scripts consist of the snippet codes which are required frequently to perform any analysis. The scripts can include data reading, normalization, visualization generic functions/codes and can be added to the notebook code cell with just a single click and executed as usual. The reusable scripts can be found on the left side as a collapsible dialogue box and you can choose the scripts at any time while performing the analysis. Figure 25. Reusable scripts on Polly Notebook On the right side, another collapsible dialogue box gets opened when you select any reusable script which provides information about the options and usage of that particular reusable script. You can also add your own reusable scripts on the Polly Notebook so as to make use of them in your repeated analysis and save time. Figure 26. Options and Information of Reusable scripts","title":"Reusable Scripts"},{"location":"Scaling compute/Polly Notebooks.html#videos","text":"","title":"Videos"}]}